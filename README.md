# Master's Machine Learning, Harbour.Space University
## Spring 2021

## Prerequisites

We are expecting our students to have a basic knowlege of:
* calculus, especially matrix calculus, differentiation
* linear algebra
* probability theory and statistics
* programming, especially on Python

Although if you don't have any of this, you could substitude it with your diligence because the course provides additional materials to study requirements yourself.

## Materials for self-study

A lot of great materials are available online. See [extra_materials.md](https://github.com/girafe-ai/ml-mipt/blob/master/extra_materials.md) file for the whole list.

Informal "aggregation" of all topics by previous years students: [file](https://github.com/girafe-ai/ml-mipt/blob/master/ML_informal_notes.pdf) (in Russian) - useful for fast and furious exam passing.

Also lectures and seminars contains references to more detailed materials on topics.


### Great books:
1. Deep Learning book (classics, really): https://www.deeplearningbook.org
2. The Hundred-page Machine Learning book: [link](http://themlbook.com) (available online, e.g. on the [github](https://github.com/ZakiaSalod/The-Hundred-Page-Machine-Learning-Book)


Additional materials for self-study:
1. Naive Bayesian classifier explained: [link](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)
2. Stanford notes on linear models: [link](http://cs229.stanford.edu/notes/cs229-notes1.pdf)
3. Detailed description of bootstrap procedure: [link](http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/notes/notebootstrap.pdf)
4. Bias-variance tradeoff in more general case: A Unified Bias-Variance Decomposition and its Applications [link](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)
5. Notes on matrix derivatives from Stanford: [link]( http://cs231n.stanford.edu/handouts/derivatives.pdf)
6.  Great interactive blogpost by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html
7.  And great gradient boosted trees playground by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html
8. Shap values repo and explanation: https://github.com/slundberg/shap
9. Kaggle tutorial on feature importances: https://www.kaggle.com/learn/machine-learning-explainability
