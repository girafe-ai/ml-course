Full Transformer architecture and training pipeline by Harvard NLP:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb)

Understanding the positional encoding:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/harbour_dlia_s21/day05_self_attention_and_transformer/practice_positional_encoding_carriers.ipynb)

Extra practice on using word embeddings:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/harbour_dlia_s21/day05_self_attention_and_transformer/practice_extra_dealing_with_word_embeddings.ipynb)



__Further readings__:
* [en] The Illustrated Transformer [blog post](https://jalammar.github.io/illustrated-transformer/)

* [en] Harvard NLP [full implementation in PyTorch](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

* [en] OpenAI blog post [Better Language Models
and Their Implications (GPT-2)](https://openai.com/blog/better-language-models/)

* [en] Paper describing positional encoding ["Convolutional Sequence to Sequence Learning"](https://arxiv.org/pdf/1705.03122)

* [en] Paper presenting [Layer Normalization](https://arxiv.org/abs/1607.06450)