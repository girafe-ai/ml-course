{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqEpGyyyGE1Z",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "## Задача №3\n",
    "\n",
    "В данном задании вам необходимо реализовать функции ошибки для линейной регрессии и их производные по параметрам, __не используя автоматические дифференцирование.__ Все методы должны быть реализованы только с использованием библиотеки `numpy`. \n",
    "\n",
    "Ваша основная задача: вывести формулы для производных __MSE, MAE, L1 и L2 регуляризационных членов__ в _векторном случае_ (т.е. когда и объект $\\mathbf{x}_i$, и целевое значение $\\mathbf{y}_i$ являются векторами.\n",
    "\n",
    "\n",
    "Для работы обратимся к [Boston housing prices dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). Он был предобработан для вашего удобства и будет загружен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you are using Google Colab, uncomment the next line to download `boston_subset.json`\n",
    "\"\"\"\n",
    "# !wget https://raw.githubusercontent.com/girafe-ai/ml-course/23f_yandex_ml_trainings/homeworks/assignment03_derivatives/boston_subset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lQUR89nGE1f"
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGf3ShTNGE1q"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"boston_subset.json\", \"r\") as iofile:\n",
    "    dataset = json.load(iofile)\n",
    "feature_matrix = np.array(dataset[\"data\"])\n",
    "targets = np.array(dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbBc_5FhGE2B"
   },
   "source": [
    "## Имплементация функций потерь и методов регуляризации.\n",
    "Для того, чтобы решить задание, вам необходимо реализовать все методы в файле `loss_and_derivatives.py`. Для вашего удобства код скопирован внутрь ноутбука в следующую ячейку. После решения ноутбука можете просто перенести его в .py файл.\n",
    "__Внимание, в данном задании не требуется использовать свободный член (bias term)__, т.е. линейная модель примет простой вид\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = XW\n",
    "$$\n",
    "Единичный столбец также не добавляется к матрице $X$.\n",
    "\n",
    "Реализуйте методы для MSE, MAE, L1 и L2 регуляризации, а также вычисления их производных (опциональное задание) по параметрам линейной модели.\n",
    "\n",
    "__Для вашего удобства данные уже предобработаны, и использование линейной модели без свободного члена не является ошибкой. В данном задании он не должен быть использован.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtELlRTOGE2E",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LossAndDerivatives:\n",
    "    @staticmethod\n",
    "    def mse(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : float\n",
    "            single number with MSE value of linear model (X.dot(w)) with no bias term\n",
    "            on the selected dataset.\n",
    "\n",
    "        Comment: If Y is two-dimentional, average the error over both dimentions.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean((X.dot(w) - Y) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return: float\n",
    "            single number with MAE value of linear model (X.dot(w)) with no bias term\n",
    "            on the selected dataset.\n",
    "\n",
    "        Comment: If Y is two-dimentional, average the error over both dimentions.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_reg(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return: float\n",
    "            single number with sum of squared elements of the weight matrix ( \\sum_{ij} w_{ij}^2 )\n",
    "\n",
    "        Computes the L2 regularization term for the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_reg(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`)\n",
    "\n",
    "        Return : float\n",
    "            single number with sum of the absolute values of the weight matrix ( \\sum_{ij} |w_{ij}| )\n",
    "\n",
    "        Computes the L1 regularization term for the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def no_reg(w):\n",
    "        \"\"\"\n",
    "        Simply ignores the regularization\n",
    "        \"\"\"\n",
    "        return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the MSE derivative for linear regression (X.dot(w)) with no bias term\n",
    "        w.r.t. w weight matrix.\n",
    "\n",
    "        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n",
    "        dimension as well, so you need to consider that fact in derivative implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def mae_derivative(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the MAE derivative for linear regression (X.dot(w)) with no bias term\n",
    "        w.r.t. w weight matrix.\n",
    "\n",
    "        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n",
    "        dimension as well, so you need to consider that fact in derivative implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the L2 regularization term derivative w.r.t. the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the L1 regularization term derivative w.r.t. the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def no_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        Simply ignores the derivative\n",
    "        \"\"\"\n",
    "        return np.zeros_like(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем ваше внимание, требуется реализовать решение в векторном виде (т.е. для каждого объекта предсказание $\\hat{\\mathbf{y}}$ является вектором с размерностью $\\geq 1$.\n",
    "\n",
    "__Внимание! При подсчете ошибки она усредняется как по объектам, так и по размерности y. Аналогичное верно и для производных__.\n",
    "\n",
    "Например, для вектора отклонений на одном объекте $[1., 1., 1., 1.]$ значение функции ошибки будет равно $\\frac{1}{4}(1. + 1. + 1. + 1.)$ \n",
    "\n",
    "Для вашего удобства метод `.mse` уже реализован и вы можете обращаться к нему за примером."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMN81aYyGE2T"
   },
   "source": [
    "Для проверки своего кода вам доступно несколько assert'ов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKUYnPWuGE2V"
   },
   "outputs": [],
   "source": [
    "w = np.array([1.0, 1.0])\n",
    "x_n, y_n = feature_matrix, targets\n",
    "\n",
    "# Repeating data to make everything multi-dimentional\n",
    "w = np.vstack(\n",
    "    [w[None, :] + 0.27, w[None, :] + 0.22, w[None, :] + 0.45, w[None, :] + 0.1]\n",
    ").T\n",
    "y_n = np.hstack([y_n[:, None], 2 * y_n[:, None], 3 * y_n[:, None], 4 * y_n[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1344,
     "status": "error",
     "timestamp": 1582397124081,
     "user": {
      "displayName": "Victor Yacovlev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDahDnBQR6_kQQX4xt7llKTI0xt2Z802bvVR4MrqA=s64",
      "userId": "11689260236152306260"
     },
     "user_tz": -180
    },
    "id": "UtkO4hWYGE2c",
    "outputId": "cb0b99a8-2db4-4873-dfd8-741b52db29f3"
   },
   "outputs": [],
   "source": [
    "reference_mse_derivative = np.array(\n",
    "    [\n",
    "        [7.32890068, 12.88731311, 18.82128365, 23.97731238],\n",
    "        [9.55674399, 17.05397661, 24.98807528, 32.01723714],\n",
    "    ]\n",
    ")\n",
    "reference_l2_reg_derivative = np.array([[2.54, 2.44, 2.9, 2.2], [2.54, 2.44, 2.9, 2.2]])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mse_derivative, LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), \"Something wrong with MSE derivative\"\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l2_reg_derivative, LossAndDerivatives.l2_reg_derivative(w), rtol=1e-3\n",
    "), \"Something wrong with L2 reg derivative\"\n",
    "\n",
    "print(\n",
    "    \"MSE derivative:\\n{} \\n\\nL2 reg derivative:\\n{}\".format(\n",
    "        LossAndDerivatives.mse_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l2_reg_derivative(w),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_mae_derivative = np.array(\n",
    "    [\n",
    "        [0.19708867, 0.19621798, 0.19621798, 0.19572906],\n",
    "        [0.25574138, 0.25524507, 0.25524507, 0.25406404],\n",
    "    ]\n",
    ")\n",
    "reference_l1_reg_derivative = np.array([[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mae_derivative, LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), \"Something wrong with MAE derivative\"\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l1_reg_derivative, LossAndDerivatives.l1_reg_derivative(w), rtol=1e-3\n",
    "), \"Something wrong with L1 reg derivative\"\n",
    "\n",
    "print(\n",
    "    \"MAE derivative:\\n{} \\n\\nL1 reg derivative:\\n{}\".format(\n",
    "        LossAndDerivatives.mae_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l1_reg_derivative(w),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJcSPj8UGE20"
   },
   "source": [
    "### Градиентный спуск для решения реальной задачи\n",
    "Следующая функция позволяет найти оптимальные значения параметров с помощью градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On6aSWuIGE21"
   },
   "outputs": [],
   "source": [
    "def get_w_by_grad(\n",
    "    X, Y, w_0, loss_mode=\"mse\", reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05\n",
    "):\n",
    "    if loss_mode == \"mse\":\n",
    "        loss_function = LossAndDerivatives.mse\n",
    "        loss_derivative = LossAndDerivatives.mse_derivative\n",
    "    elif loss_mode == \"mae\":\n",
    "        loss_function = LossAndDerivatives.mae\n",
    "        loss_derivative = LossAndDerivatives.mae_derivative\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown loss function. Available loss functions: `mse`, `mae`\"\n",
    "        )\n",
    "\n",
    "    if reg_mode is None:\n",
    "        reg_function = LossAndDerivatives.no_reg\n",
    "        reg_derivative = (\n",
    "            LossAndDerivatives.no_reg_derivative\n",
    "        )  # lambda w: np.zeros_like(w)\n",
    "    elif reg_mode == \"l2\":\n",
    "        reg_function = LossAndDerivatives.l2_reg\n",
    "        reg_derivative = LossAndDerivatives.l2_reg_derivative\n",
    "    elif reg_mode == \"l1\":\n",
    "        reg_function = LossAndDerivatives.l1_reg\n",
    "        reg_derivative = LossAndDerivatives.l1_reg_derivative\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown regularization mode. Available modes: `l1`, `l2`, None\"\n",
    "        )\n",
    "\n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)\n",
    "        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        if gradient_norm > 5.0:\n",
    "            gradient = gradient / gradient_norm * 5.0\n",
    "        w -= lr * gradient\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(\n",
    "                \"Step={}, loss={},\\ngradient values={}\\n\".format(\n",
    "                    i, empirical_risk, gradient\n",
    "                )\n",
    "            )\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простой пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1pyDIyqGE25"
   },
   "outputs": [],
   "source": [
    "# Initial weight matrix\n",
    "w = np.ones((2, 1), dtype=float)\n",
    "y_n = targets[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erTRQiAFGE29"
   },
   "outputs": [],
   "source": [
    "w_grad = get_w_by_grad(x_n, y_n, w, loss_mode=\"mse\", reg_mode=\"l2\", n_steps=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с `sklearn`\n",
    "Сравним реализованную модель с версией из `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=0.05)\n",
    "lr.fit(x_n, y_n)\n",
    "print(\n",
    "    \"sklearn linear regression implementation delivers MSE = {}\".format(\n",
    "        np.mean((lr.predict(x_n) - y_n) ** 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gse1m4nyGE3C"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_n[:, -1], y_n[:, -1])\n",
    "plt.scatter(\n",
    "    x_n[:, -1],\n",
    "    x_n.dot(w_grad)[:, -1],\n",
    "    color=\"orange\",\n",
    "    label=\"Handwritten linear regression\",\n",
    "    linewidth=5,\n",
    ")\n",
    "plt.scatter(x_n[:, -1], lr.predict(x_n), color=\"cyan\", label=\"sklearn Ridge\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в полученных решениях есть небольшие различия, это не страшно. Модель основанная на вашей реализации не использует свободный член (он равен $0$), в то время версия из `sklearn` настраивает и его."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GgeWdBmGE3H"
   },
   "source": [
    "### Сдача решения\n",
    "Сдайте в чекер реализованный класс `LossAndDerivatives`. Для этого можете скопировать всю ячейку с кодом (в том числе и импортирование `numpy`) в файл `derivatives.py`.\n",
    "\n",
    "На этом задача завершена. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment0_02_linear_regression_and_gradient_descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Py3 Research",
   "language": "python",
   "name": "py3_research_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
