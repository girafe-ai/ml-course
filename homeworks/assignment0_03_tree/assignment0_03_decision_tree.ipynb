{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assignment 04: Decision Tree construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working in colab, uncomment the following line\n",
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_made/homeworks/assignment0_04_tree/tree.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_classification, make_regression, load_digits, load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the `random_state` (a.k.a. random seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your ultimate task for today is to impement the `DecisionTree` class and use it to solve classification and regression problems.__\n",
    "\n",
    "__Specifications:__\n",
    "- The class inherits from `sklearn.BaseEstimator`;\n",
    "- Constructor is implemented for you. It has the following parameters:\n",
    "    * `max_depth` - maximum depth of the tree; `np.inf` by default\n",
    "    * `min_samples_split` - minimal number of samples in the leaf to make a split; `2` by default;\n",
    "    * `criterion` - criterion to select the best split; in classification one of `['gini', 'entropy']`, default `gini`; in regression `variance`;\n",
    "\n",
    "- `fit` method takes `X` (`numpy.array` of type `float` shaped `(n_objects, n_features)`) and `y` (`numpy.array` of type float shaped `(n_objects, 1)` in regression; `numpy.array` of type int shaped `(n_objects, 1)` with class labels in classification). It works inplace and fits the `DecisionTree` class instance to the provided data from scratch.\n",
    "\n",
    "- `predict` method takes `X` (`numpy.array` of type `float` shaped `(n_objects, n_features)`) and returns the predicted $\\hat{y}$ values. In classification it is a class label for every object (the most frequent in the leaf; if several classes meet this requirement select the one with the smallest class index). In regression it is the desired constant (e.g. mean value for `variance` criterion)\n",
    "\n",
    "- `predict_proba` method (works only for classification (`gini` or `entropy` criterion). It takes `X` (`numpy.array` of type `float` shaped `(n_objects, n_features)`) and returns the `numpy.array` of type `float` shaped `(n_objects, n_features)` with class probabilities for every object from `X`. Class $i$ probability equals the ratio of $i$ class objects that got in this node in the training set.\n",
    "\n",
    "    \n",
    "__Small recap:__\n",
    "\n",
    "To find the optimal split the following functional is evaluated:\n",
    "    \n",
    "$$G(j, t) = H(Q) - \\dfrac{|L|}{|Q|} H(L) - \\dfrac{|R|}{|Q|} H(R),$$\n",
    "    where $Q$ is the dataset from the current node, $L$ and $R$ are left and right subsets defined by the split $x^{(j)} < t$.\n",
    "\n",
    "\n",
    "\n",
    "1. Classification. Let $p_i$ be the probability of $i$ class in subset $X$ (ratio of the $i$ class objects in the dataset). The criterions are defined as:\n",
    "    \n",
    "    * `gini`: Gini impurity $$H(R) = 1 -\\sum_{i = 1}^K p_i^2$$\n",
    "    \n",
    "    * `entropy`: Entropy $$H(R) = -\\sum_{i = 1}^K p_i \\log(p_i)$$ (One might use the natural logarithm).\n",
    "    \n",
    "2. Regression. Let $y_l$ be the target value for the $R$, $\\mathbf{y} = (y_1, \\dots, y_N)$ â€“ all targets for the selected dataset $X$.\n",
    "    \n",
    "    * `variance`: $$H(R) = \\dfrac{1}{|R|} \\sum_{y_j \\in R}(y_j - \\text{mean}(\\mathbf{y}))^2$$\n",
    "    \n",
    "    * `mad_median`: $$H(R) = \\dfrac{1}{|R|} \\sum_{y_j \\in R}|y_j - \\text{median}(\\mathbf{y})|$$\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints and comments**:\n",
    "\n",
    "* No need to deal with categorical features, they will not be present.\n",
    "* Siple greedy recursive procedure is enough. However, you can speed it up somehow (e.g. using percentiles).\n",
    "* Please, do not copy implementations available online. You are supposed to build very simple example of the Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File `tree.py` is waiting for you. Implement all the needed methods in that file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree import entropy, gini, variance, mad_median, DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((4, 5), dtype=float) * np.arange(4)[:, None]\n",
    "y = np.arange(4)[:, None] + np.asarray([0.2, -0.3, 0.1, 0.4])[:, None]\n",
    "class_estimator = DecisionTree(max_depth=10, criterion_name='gini')\n",
    "\n",
    "(X_l, y_l), (X_r, y_r) = class_estimator.make_split(1, 1., X, y)\n",
    "\n",
    "assert np.array_equal(X[:1], X_l)\n",
    "assert np.array_equal(X[1:], X_r)\n",
    "assert np.array_equal(y[:1], y_l)\n",
    "assert np.array_equal(y[1:], y_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_data = load_digits().data\n",
    "digits_target = load_digits().target[:, None] # to make the targets consistent with our model interfaces\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits_data, digits_target, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(y_train.shape) == 2 and y_train.shape[0] == len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_estimator = DecisionTree(max_depth=10, criterion_name='gini')\n",
    "class_estimator.fit(X_train, y_train)\n",
    "ans = class_estimator.predict(X_test)\n",
    "accuracy_gini = accuracy_score(y_test, ans)\n",
    "print(accuracy_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([0.09027778, 0.09236111, 0.08333333, 0.09583333, 0.11944444,\n",
    "       0.13888889, 0.09930556, 0.09444444, 0.08055556, 0.10555556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_estimator = DecisionTree(max_depth=10, criterion_name='entropy')\n",
    "class_estimator.fit(X_train, y_train)\n",
    "ans = class_estimator.predict(X_test)\n",
    "accuracy_entropy = accuracy_score(y_test, ans)\n",
    "print(accuracy_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert  0.84 < accuracy_gini < 0.9\n",
    "assert  0.86 < accuracy_entropy < 0.9\n",
    "assert np.sum(np.abs(class_estimator.predict_proba(X_test).mean(axis=0) - reference)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 5-fold cross validation (`GridSearchCV`) to find optimal values for `max_depth` and `criterion` hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': range(3,11), 'criterion_name': ['gini', 'entropy']}\n",
    "gs = GridSearchCV(DecisionTree(), param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gs.best_params_['criterion_name'] == 'entropy'\n",
    "assert 6 < gs.best_params_['max_depth'] < 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"The dependence of quality on the depth of the tree\")\n",
    "plt.plot(np.arange(3,11), gs.cv_results_['mean_test_score'][:8], label='Gini')\n",
    "plt.plot(np.arange(3,11), gs.cv_results_['mean_test_score'][8:], label='Entropy')\n",
    "plt.legend(fontsize=11, loc=1)\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_data = load_boston().data\n",
    "regr_target = load_boston().target[:, None] # to make the targets consistent with our model interfaces\n",
    "RX_train, RX_test, Ry_train, Ry_test = train_test_split(regr_data, regr_target, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTree(max_depth=10, criterion_name='mad_median')\n",
    "regressor.fit(RX_train, Ry_train)\n",
    "predictions_mad = regressor.predict(RX_test)\n",
    "mse_mad = mean_squared_error(Ry_test, predictions_mad)\n",
    "print(mse_mad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTree(max_depth=10, criterion_name='variance')\n",
    "regressor.fit(RX_train, Ry_train)\n",
    "predictions_mad = regressor.predict(RX_test)\n",
    "mse_var = mean_squared_error(Ry_test, predictions_mad)\n",
    "print(mse_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 9 < mse_mad < 20\n",
    "assert 8 < mse_var < 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_R = {'max_depth': range(2,9), 'criterion_name': ['variance', 'mad_median']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_R = GridSearchCV(DecisionTree(), param_grid=param_grid_R, cv=5, scoring='neg_mean_squared_error', n_jobs=-2)\n",
    "gs_R.fit(RX_train, Ry_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_R.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gs_R.best_params_['criterion_name'] == 'mad_median'\n",
    "assert 3 < gs_R.best_params_['max_depth'] < 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_scores = gs_R.cv_results_['mean_test_score'][:7]\n",
    "mad_scores = gs_R.cv_results_['mean_test_score'][7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"The dependence of neg_mse on the depth of the tree\")\n",
    "plt.plot(np.arange(2,9), var_scores, label='variance')\n",
    "plt.plot(np.arange(2,9), mad_scores, label='mad_median')\n",
    "plt.legend(fontsize=11, loc=1)\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel('neg_mse')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
