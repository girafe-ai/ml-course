{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNtLJlW4v5VF"
   },
   "source": [
    "## Домашнее задание №8\n",
    "\n",
    "В данном задании вам предстоит детально рассмотреть механизм Attention (и реализовать несколько его вариантов), а также вернуться к задаче классификации текстов из задания №6 и решить ее с использованием BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг №1. Реализация Attention\n",
    "\n",
    "В данной задаче вам предстоит реализовать механизм Attention, в частности несколько способов подсчета attention scores. Конечно, в популярных фреймворках данный механизм уже реализован, но для лучшего понимания вам предстаит реализовать его с помощью `numpy`.\n",
    "\n",
    "Ваше задание в данной задаче: реализовать `additive` (аддитивный) и `multiplicative` (мультипликативный) варианты Attention. Для вашего удобства (и для примера) `dot product` attention (основанный на скалярном произведении) уже реализован.\n",
    "\n",
    "Детальное описание данных типов Attention доступно в лекционных слайдах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_state = np.array([7, 11, 4]).astype(float)[:, None]\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "plt.pcolormesh(decoder_hidden_state)\n",
    "plt.colorbar()\n",
    "plt.title('Decoder state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot product attention (пример реализации)\n",
    "Рассмотрим единственное состояние энкодера – вектор с размерностью `(n_hidden, 1)`, где `n_hidden = 3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_encoder_hidden_state = np.array([1, 5, 11]).astype(float)[:, None]\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "plt.pcolormesh(single_encoder_hidden_state)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention score между данными состояниями энкодера и декодера вычисляются просто как скалярное произведение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(decoder_hidden_state.T, single_encoder_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае состояний энкодера, конечно, несколько. Attention scores вычисляются с каждым из состояний энкодера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states = np.array([\n",
    "    [1, 5, 11],\n",
    "    [7, 4, 1],\n",
    "    [8, 12, 2],\n",
    "    [-9, 0, 1]\n",
    "    \n",
    "]).astype(float).T\n",
    "\n",
    "encoder_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда для подсчета скалярных произведений между единственным состоянием декодера и всеми состояниями энкодера можно воспользоваться следующей функцией (которая по факту представляет собой просто матричное умножение и приведение типов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention_score(decoder_hidden_state, encoder_hidden_states):\n",
    "    '''\n",
    "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
    "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
    "    \n",
    "    return: np.array of shape (1, n_states)\n",
    "        Array with dot product attention scores\n",
    "    '''\n",
    "    attention_scores = np.dot(decoder_hidden_state.T, encoder_hidden_states)\n",
    "    return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета \"весов\" нам необходим Softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vector):\n",
    "    '''\n",
    "    vector: np.array of shape (n, m)\n",
    "    \n",
    "    return: np.array of shape (n, m)\n",
    "        Matrix where softmax is computed for every row independently\n",
    "    '''\n",
    "    nice_vector = vector - vector.max()\n",
    "    exp_vector = np.exp(nice_vector)\n",
    "    exp_denominator = np.sum(exp_vector, axis=1)[:, np.newaxis]\n",
    "    softmax_ = exp_vector / exp_denominator\n",
    "    return softmax_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_vector = softmax(dot_product_attention_score(decoder_hidden_state, encoder_hidden_states))\n",
    "\n",
    "weights_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, воспользуемся данными весами и вычислим итоговый вектор, как и описано для dot product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_vector = weights_vector.dot(encoder_hidden_states.T).T\n",
    "print(attention_vector)\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "plt.pcolormesh(attention_vector, cmap='spring')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный вектор аккумулирует в себе информацию из всех состояний энкодера, взвешенную на основе близости к заданному состоянию декодера.\n",
    "\n",
    "Реализуем все вышеописанные преобразования в единой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(decoder_hidden_state, encoder_hidden_states):\n",
    "    '''\n",
    "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
    "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
    "    \n",
    "    return: np.array of shape (n_features, 1)\n",
    "        Final attention vector\n",
    "    '''\n",
    "    softmax_vector = softmax(dot_product_attention_score(decoder_hidden_state, encoder_hidden_states))\n",
    "    attention_vector = softmax_vector.dot(encoder_hidden_states.T).T\n",
    "    return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (attention_vector == dot_product_attention(decoder_hidden_state, encoder_hidden_states)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplicative attention\n",
    "Ваша текущая задача: реализовать multiplicative attention.\n",
    "$$ e_i = \\mathbf{s}^TW_{mult}\\mathbf{h}_i $$\n",
    "\n",
    "Матрица весов `W_mult` задана ниже. \n",
    "Стоит заметить, что multiplicative attention позволяет работать с состояниями энкодера и декодера различных размерностей, поэтому состояния энкодера будут обновлены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states_complex = np.array([\n",
    "    [1, 5, 11, 4, -4],\n",
    "    [7, 4, 1, 2, 2],\n",
    "    [8, 12, 2, 11, 5],\n",
    "    [-9, 0, 1, 8, 12]\n",
    "    \n",
    "]).astype(float).T\n",
    "\n",
    "W_mult = np.array([\n",
    "    [-0.78, -0.97, -1.09, -1.79,  0.24],\n",
    "    [ 0.04, -0.27, -0.98, -0.49,  0.52],\n",
    "    [ 1.08,  0.91, -0.99,  2.04, -0.15]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `multiplicative_attention`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplicative_attention(decoder_hidden_state, encoder_hidden_states, W_mult):\n",
    "    '''\n",
    "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
    "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
    "    W_mult: np.array of shape (n_features_dec, n_features_enc)\n",
    "    \n",
    "    return: np.array of shape (n_features_enc, 1)\n",
    "        Final attention vector\n",
    "    '''\n",
    "    # your code here\n",
    "    return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive attention\n",
    "Теперь вам предстоит реализовать additive attention.\n",
    "\n",
    "$$ e_i = \\mathbf{v}^T \\text{tanh} (W_{add-enc} \\mathbf{h}_i + W_{add-dec} \\mathbf{s}) $$\n",
    "\n",
    "Матрицы весов `W_add_enc` и `W_add_dec` доступны ниже, как и вектор весов `v_add`. Для вычисления активации можно воспользоваться `np.tanh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_add = np.array([[-0.35, -0.58,  0.07,  1.39, -0.79, -1.78, -0.35]]).T\n",
    "\n",
    "W_add_enc = np.array([\n",
    "    [-1.34, -0.1 , -0.38,  0.12, -0.34],\n",
    "    [-1.  ,  1.28,  0.49, -0.41, -0.32],\n",
    "    [-0.39, -1.38,  1.26,  1.21,  0.15],\n",
    "    [-0.18,  0.04,  1.36, -1.18, -0.53],\n",
    "    [-0.23,  0.96,  1.02,  0.39, -1.26],\n",
    "    [-1.27,  0.89, -0.85, -0.01, -1.19],\n",
    "    [ 0.46, -0.12, -0.86, -0.93, -0.4 ]\n",
    "])\n",
    "\n",
    "W_add_dec = np.array([\n",
    "    [-1.62, -0.02, -0.39],\n",
    "    [ 0.43,  0.61, -0.23],\n",
    "    [-1.5 , -0.43, -0.91],\n",
    "    [-0.14,  0.03,  0.05],\n",
    "    [ 0.85,  0.51,  0.63],\n",
    "    [ 0.39, -0.42,  1.34],\n",
    "    [-0.47, -0.31, -1.34]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `additive_attention`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_attention(decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec):\n",
    "    '''\n",
    "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
    "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
    "    v_add: np.array of shape (n_features_int, 1)\n",
    "    W_add_enc: np.array of shape (n_features_int, n_features_enc)\n",
    "    W_add_dec: np.array of shape (n_features_int, n_features_dec)\n",
    "    \n",
    "    return: np.array of shape (n_features_enc, 1)\n",
    "        Final attention vector\n",
    "    '''\n",
    "    # your code here\n",
    "    return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сдайте функции `multiplicative_attention` и `additive_attention` в контест.\n",
    "\n",
    "Не забудьте про импорт `numpy`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг №2 (опциональный). Классификация текстов с использованием предобученной языковой модели.\n",
    "\n",
    "Вновь вернемся к набору данных SST-2. Разобьем выборку на train и test аналогично заданию №6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "df = pd.read_csv(\n",
    "    'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
    "    delimiter='\\t',\n",
    "    header=None\n",
    ")\n",
    "texts_train = df[0].values[:5000]\n",
    "y_train = df[1].values[:5000]\n",
    "texts_test = df[0].values[5000:]\n",
    "y_test = df[1].values[5000:]\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь остальной код предстоит написать вам.\n",
    "\n",
    "Для успешной сдачи на максимальный балл необходимо добиться хотя бы __84.5% accuracy на тестовой части выборки__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your beautiful experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сдача взадания в контест\n",
    "Сохраните в словарь `out_dict` вероятности принадлежности к нулевому и первому классу соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {\n",
    "    'train': # np.array of size (5000, 2) with probas\n",
    "    'test': # np.array of size (1920, 2) with probas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько `assert`'ов для проверки вашей посылки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(out_dict['train'], np.ndarray), 'Dict values should be numpy arrays'\n",
    "assert out_dict['train'].shape == (5000, 2), 'The predicted probas shape does not match the train set size'\n",
    "assert np.allclose(out_dict['train'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the objects'\n",
    "\n",
    "assert isinstance(out_dict['test'], np.ndarray), 'Dict values should be numpy arrays'\n",
    "assert out_dict['test'].shape == (1920, 2), 'The predicted probas shape does not match the test set size'\n",
    "assert np.allclose(out_dict['test'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the object'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите код ниже для генерации посылки и сдайте файл `submission_dict_hw08.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "np.save('submission_dict_hw08.npy', out_dict, allow_pickle=True)\n",
    "print('File saved to `submission_dict_hw08.npy`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_hw01_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Py3 Research",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
