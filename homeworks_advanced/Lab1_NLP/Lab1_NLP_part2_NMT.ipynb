{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztcPpl6hPsAU"
      },
      "source": [
        "# Lab 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6raI4E_fPsAW"
      },
      "source": [
        "## Part 2: Neural Machine Translation in the wild\n",
        "In the third homework you are supposed to get the best translation you can for the EN-RU translation task.\n",
        "\n",
        "Basic approach using RNNs as encoder and decoder is implemented for you. \n",
        "\n",
        "Your ultimate task is to use the techniques we've covered, e.g.\n",
        "\n",
        "* Optimization enhancements (e.g. learning rate decay)\n",
        "\n",
        "* CNN encoder (with or without positional encoding)\n",
        "\n",
        "* attention/self-attention mechanism\n",
        "\n",
        "* pretrained language model\n",
        "\n",
        "* [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt)\n",
        "\n",
        "* or just fine-tunning BERT ;)\n",
        "\n",
        "to improve the translation quality. \n",
        "\n",
        "__Please use at least three different approaches/models and compare them (translation quality/complexity/training and evaluation time).__\n",
        "\n",
        "Write down some summary on your experiments and illustrate it with convergence plots/metrics and your thoughts. Just like you would approach a real problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORBsRFA-PsAY",
        "outputId": "c07eb3c0-e298-436c-e623-df8014260fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘data.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Thanks to YSDA NLP course team for the data\n",
        "# (who thanks tilda and deephack teams for the data in their turn)\n",
        "!wget -nc https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/datasets/Machine_translation_EN_RU/data.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ihZ_nVUmsyC"
      },
      "source": [
        "The `data.txt` is a tsv file, each line of which contains a sentence in english and a corresponding translation, separated by `\\t`. We'll load it into memory and create a list of pairs, which would yield the same interface as with the torchtext's datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wzRffwfgizj",
        "outputId": "8929b01f-dfcc-4756-c6de-22169238a639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size 50,000\n",
            "Sample:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Cordelia Hotel is situated in Tbilisi, a 3-minute walk away from Saint Trinity Church.',\n",
              "  'Отель Cordelia расположен в Тбилиси, в 3 минутах ходьбы от Свято-Троицкого собора.'],\n",
              " ['At Tupirmarka Lodge you will find a 24-hour front desk, room service, and a snack bar.',\n",
              "  'В числе удобств лоджа Tupirmarka круглосуточная стойка регистрации и снэк-бар. Гости могут воспользоваться услугой доставки еды и напитков в номер.'],\n",
              " ['Featuring free WiFi in all areas, Naigao Xiaowo offers accommodation in Shanghai.',\n",
              "  'Апартаменты Naigao Xiaowo расположены в городе Шанхай. К услугам гостей бесплатный Wi-Fi во всех зонах.'],\n",
              " ['Each has a TV and a private bathroom with shower.',\n",
              "  'В вашем распоряжении также телевизор и собственная ванная комната с душем.'],\n",
              " ['Your room comes with air conditioning and satellite TV.',\n",
              "  'Номер оснащен кондиционером и спутниковым телевидением.']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "with open(\"data.txt\") as f:\n",
        "    data = [l.rstrip().split(\"\\t\") for l in f]\n",
        "\n",
        "print(f\"Dataset size {len(data):,}\")\n",
        "print(\"Sample:\")\n",
        "data[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjDuhDOWPsAa"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW41CAd1ofXD"
      },
      "source": [
        "First of all, let's split our dataset into train, test and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXYV0vWaolpm",
        "outputId": "216e2c70-c017-483e-fe2d-51c08d4e97ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 40000\n",
            "Test size: 7500\n",
            "Val size: 2500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "data_size = len(data)\n",
        "train_size = int(0.8 * data_size)\n",
        "test_size = int(0.15 * data_size)\n",
        "val_size = data_size - train_size - test_size\n",
        "train_data, test_data, val_data = random_split(\n",
        "    data, [train_size, test_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Test size: {len(test_data)}\")\n",
        "print(f\"Val size: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gGXkxD5odoA"
      },
      "source": [
        "Here comes the preprocessing. If you find pieces, that you don't understand, please, go back to 3rd week's practice notebook. The code is mostly taken from it.\n",
        "\n",
        "Do not hesitate to use BPE or more complex preprocessing pipeline ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6sGsE2moPsAa"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "\n",
        "def tokenize(sent):\n",
        "    return tokenizer.tokenize(sent.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LmshoKhRpQVj"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "from torchtext.vocab import vocab as Vocab\n",
        "\n",
        "\n",
        "src_counter = Counter()\n",
        "trg_counter = Counter()\n",
        "for src, trg in train_data:\n",
        "    src_counter.update(tokenize(src))\n",
        "    trg_counter.update(tokenize(trg))\n",
        "\n",
        "src_vocab = Vocab(src_counter, min_freq=3)\n",
        "trg_vocab = Vocab(trg_counter, min_freq=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxBsel4JpaTP",
        "outputId": "2dba5ff9-4bfc-48ae-b556-2ed38fbe5313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source (en) vocabulary size: 6711\n",
            "Target (ru) vocabulary size: 9310\n"
          ]
        }
      ],
      "source": [
        "unk_token = \"<unk>\"\n",
        "sos_token, eos_token, pad_token = \"<sos>\", \"<eos>\", \"<pad>\"\n",
        "specials = [sos_token, eos_token, pad_token]\n",
        "\n",
        "for vocab in [src_vocab, trg_vocab]:\n",
        "    if unk_token not in vocab:\n",
        "        vocab.insert_token(unk_token, index=0)\n",
        "        vocab.set_default_index(0)\n",
        "\n",
        "    for token in specials:\n",
        "        if token not in vocab:\n",
        "            vocab.append_token(token)\n",
        "\n",
        "print(f\"Source (en) vocabulary size: {len(src_vocab)}\")\n",
        "print(f\"Target (ru) vocabulary size: {len(trg_vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DpFjrwNHpiZq"
      },
      "outputs": [],
      "source": [
        "def encode(sent, vocab):\n",
        "    tokenized = [sos_token] + tokenize(sent) + [eos_token]\n",
        "    return [vocab[tok] for tok in tokenized]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpWUg9RuqGXm",
        "outputId": "1d7be699-c609-434e-abc1-96b8927496b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([45, 256]), torch.Size([46, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_list, trg_list = [], []\n",
        "    for src, trg in batch:\n",
        "        src_encoded = encode(src, src_vocab)\n",
        "        src_list.append(torch.tensor(src_encoded))\n",
        "\n",
        "        trg_encoded = encode(trg, trg_vocab)\n",
        "        trg_list.append(torch.tensor(trg_encoded))\n",
        "\n",
        "    src_padded = pad_sequence(src_list, padding_value=src_vocab[pad_token])\n",
        "    trg_padded = pad_sequence(trg_list, padding_value=trg_vocab[pad_token])\n",
        "    return src_padded, trg_padded\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "train_dataloader = DataLoader(train_data, batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_data, batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_data, batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "src_batch, trg_batch = next(iter(train_dataloader))\n",
        "src_batch.shape, trg_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nvNQhzfPsAe"
      },
      "source": [
        "## Model side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wKXxEXwFtMPP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_tokens, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_tokens = n_tokens\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "        _, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_tokens, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_tokens = n_tokens\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hid_dim, n_tokens)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(dim=0)\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        pred = self.out(output.squeeze(dim=0))\n",
        "        return pred, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"encoder and decoder must have same hidden dim\"\n",
        "        assert (\n",
        "            encoder.n_layers == decoder.n_layers\n",
        "        ), \"encoder and decoder must have equal number of layers\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len, batch_size = trg.shape\n",
        "        preds = []\n",
        "        hidden = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <sos> token.\n",
        "        input = trg[0, :]\n",
        "        for i in range(1, trg_len):\n",
        "            pred, hidden = self.decoder(input, hidden)\n",
        "            preds.append(pred)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            _, top_pred = pred.max(dim=1)\n",
        "            input = trg[i, :] if teacher_force else top_pred\n",
        "\n",
        "        return torch.stack(preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client google-api-python-client #https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "metadata": {
        "id": "pwxIo5w8Idj9",
        "outputId": "fe4ed121-c2fe-41ab-f6e5-e17b1064e301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Requirement 'google-api-python-client#https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\n",
            "Processing ./google-api-python-client#https:/storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/content/google-api-python-client#https:/storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl'\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ed9882dc2035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jQ3w4pdDPsAf",
        "outputId": "cf099e9d-44cb-4195-c559-38a908632766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c262969e14ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creates a random tensor on xla:1 (a Cloud TPU core)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\u001b[0m in \u001b[0;36mxla_device\u001b[0;34m(n, devkind)\u001b[0m\n\u001b[1;32m    230\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     devices = get_xla_supported_devices(\n\u001b[0;32m--> 232\u001b[0;31m         devkind=devkind if devkind is not None else None)\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'No devices of {} kind'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevkind\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'ANY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# This is a utility API mainly called from tests or simple code which wants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\u001b[0m in \u001b[0;36mget_xla_supported_devices\u001b[0;34m(devkind, max_devices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0mxla_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DEVICES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m   \u001b[0mdevkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdevkind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevkind\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'TPU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CPU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevkind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/utils/utils.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyd_queue\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0m_DEVICES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLazyProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_XLAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xla_get_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mREDUCE_SUM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: tensorflow/compiler/xla/xla_client/computation_client.cc:273 : Missing XLA configuration"
          ]
        }
      ],
      "source": [
        "dev = xm.xla_device()\n",
        "enc = Encoder(len(src_vocab), emb_dim=256, hid_dim=512, n_layers=2, dropout=0.5)\n",
        "dec = Decoder(len(trg_vocab), emb_dim=256, hid_dim=512, n_layers=2, dropout=0.5)\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4_-hFxcyPsAf",
        "outputId": "3a93b9bc-9d73-417b-ea83-a055f93b9fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-09e3b0b65211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param, -0.08, 0.08)\n",
        "\n",
        "\n",
        "model.apply(init_weights);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15zD2R7cPsAg"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG45OrPDPsAg"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab[pad_token])\n",
        "loss_history, train_loss_history, val_loss_history = [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFsx5PcCudiG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "# Please don't use tensorboard here.\n",
        "# It doesn't save the training plots in the notebook.\n",
        "n_epochs = 50\n",
        "clip = 1\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for src, trg in train_dataloader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if len(loss_history) % 10 == 0:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            plt.subplot(121)\n",
        "            plt.plot(loss_history)\n",
        "            plt.xlabel(\"step\")\n",
        "\n",
        "            plt.subplot(122)\n",
        "            plt.plot(train_loss_history, label=\"train loss\")\n",
        "            plt.plot(val_loss_history, label=\"val loss\")\n",
        "            plt.xlabel(\"epoch\")\n",
        "            plt.legend()\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_loss_history.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in val_dataloader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg)\n",
        "\n",
        "            output = output.view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_loss_history.append(val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp3-T3dDPsAg"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtYccOGpPsAh"
      },
      "outputs": [],
      "source": [
        "trg_itos = trg_vocab.get_itos()\n",
        "model.eval()\n",
        "max_len = 50\n",
        "with torch.no_grad():\n",
        "    for i, (src, trg) in enumerate(val_data):\n",
        "        encoded = encode(src, src_vocab)[::-1]\n",
        "        encoded = torch.tensor(encoded)[:, None].to(device)\n",
        "        hidden = model.encoder(encoded)\n",
        "\n",
        "        pred_tokens = [trg_vocab[sos_token]]\n",
        "        for _ in range(max_len):\n",
        "            decoder_input = torch.tensor([pred_tokens[-1]]).to(device)\n",
        "            pred, hidden = model.decoder(decoder_input, hidden)\n",
        "            _, pred_token = pred.max(dim=1)\n",
        "            if pred_token == trg_vocab[eos_token]:\n",
        "                # Don't add it to prediction for cleaner output.\n",
        "                break\n",
        "\n",
        "            pred_tokens.append(pred_token.item())\n",
        "\n",
        "        print(f\"src: '{src.rstrip().lower()}'\")\n",
        "        print(f\"trg: '{trg.rstrip().lower()}'\")\n",
        "        print(f\"pred: '{' '.join(trg_itos[i] for i in pred_tokens[1:])}'\")\n",
        "        print()\n",
        "\n",
        "        if i == 10:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJsWrg5Z1phB"
      },
      "source": [
        "The metric often used in NMT is the BLEU. We'll also use it to evaluate our models. In fact, the goal of this homework is to beat the specified baseline BLEU scores.\n",
        "\n",
        "Here is how you can calculate the score for your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hogEKcmOPsAh"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "references, hypotheses = [], []\n",
        "with torch.no_grad():\n",
        "    for src, trg in test_dataloader:\n",
        "        output = model(src.to(device), trg.to(device), teacher_forcing_ratio=0)\n",
        "        output = output.cpu().numpy().argmax(axis=2)\n",
        "\n",
        "        for i in range(trg.shape[1]):\n",
        "            reference = trg[:, i]\n",
        "            reference_tokens = [trg_itos[id_] for id_ in reference]\n",
        "            reference_tokens = [tok for tok in reference_tokens if tok not in specials]\n",
        "            references.append(reference_tokens)\n",
        "\n",
        "            hypothesis = output[:, i]\n",
        "            hypothesis_tokens = [trg_itos[id_] for id_ in hypothesis]\n",
        "            hypothesis_tokens = [tok for tok in hypothesis_tokens if tok not in specials]\n",
        "            hypotheses.append(hypothesis_tokens)\n",
        "\n",
        "# corpus_bleu works with multiple references\n",
        "bleu = corpus_bleu([[ref] for ref in references], hypotheses)\n",
        "print(f\"Your model shows test BLEU of {100 * bleu:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9hHhduLPsAh"
      },
      "source": [
        "Baseline solution BLEU score is quite low. Try to achieve at least __24__ BLEU on the test set. \n",
        "The checkpoints are:\n",
        "\n",
        "* __22__ - minimal score to submit the homework, 30% of points\n",
        "\n",
        "* __27__ - good score, 70% of points\n",
        "\n",
        "* __29__ - excellent score, 100% of points"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Lab1_NLP_part2_NMT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}