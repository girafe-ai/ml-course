{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment â„–2: Gradient boosting and feature importance estimation\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will work with widely known Human Actividy Recognition (HAR) dataset. Data is available at [UCI repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones). Download it and place in `data/` folder in the same directory as this notebook. There are available both raw and preprocessed datasets. This time we will use the preprocessed one.\n",
    "\n",
    "There are several great frameworks (listed below). However, we recommend to stick to `LightGBM` for this task.\n",
    "* LightGBM by Microsoft. [Link to github](https://github.com/Microsoft/LightGBM). It is one of the most popular frameworks these days that shows both great quality and performance.\n",
    "* xgboost by dlmc. [Link to github](https://github.com/dmlc/xgboost). The most famous framework which got very popular on kaggle.\n",
    "* Catboost by Yandex. [Link to github](https://github.com/catboost/catboost). Novel framework by Yandex company tuned to deal well with categorical features.\n",
    "\n",
    "Some simple preprocessing is done for you. \n",
    "\n",
    "Parts 1 and 3 have the same weight equal to $1$. Part 2 has weight $0.5$.\n",
    "\n",
    "### Part 1:\n",
    "Your __ultimate target is to get familiar with one of the frameworks above__ and achieve at least 90% accuracy on test dataset:\n",
    "\n",
    "* $\\geq 90\\%$ accuracy: 0.5 points for this part\n",
    "* $\\geq 92\\%$ accuracy: 0.7 points for this part\n",
    "* $\\geq 94\\%$ accuracy: 1 point for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.genfromtxt('data/train/X_train.txt')\n",
    "y_train = np.genfromtxt('data/train/y_train.txt')\n",
    "\n",
    "X_test = np.genfromtxt('data/test/X_test.txt')\n",
    "y_test = np.genfromtxt('data/test/y_test.txt')\n",
    "\n",
    "with open('data/activity_labels.txt', 'r') as iofile:\n",
    "    activity_labels = iofile.readlines()\n",
    "\n",
    "activity_labels = [x.replace('\\n', '').split(' ') for x in activity_labels]\n",
    "activity_labels = dict([(int(x[0]), x[1]) for x in activity_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "data_mean = X_train.mean(axis=0)\n",
    "data_std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - data_mean)/data_std\n",
    "X_test = (X_test - data_mean)/data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has some duplicating features. File `unique_columns.txt` stores the indices of the unique ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    unique_columns = np.genfromtxt('unique_columns.txt', delimiter=',').astype(int)\n",
    "except FileNotFoundError:\n",
    "    ! wget https://raw.githubusercontent.com/ml-mipt/ml-mipt/basic_s20/homeworks_basic/Lab2_boosting/unique_columns.txt -nc\n",
    "    unique_columns = np.genfromtxt('unique_columns.txt', delimiter=',').astype(int)\n",
    "\n",
    "X_train_unique = X_train[:, unique_columns]\n",
    "X_test_unique = X_test[:, unique_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA could be useful in this case. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.fit_transform(X_train_unique)\n",
    "X_test_pca = pca.transform(X_test_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_pca[:1000, 0], X_train_pca[:1000, 1], c=y_train[:1000])\n",
    "plt.grid()\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_pca[:1000, 3], X_train_pca[:1000, 4], c=y_train[:1000])\n",
    "plt.grid()\n",
    "plt.xlabel('Principal component 4')\n",
    "plt.ylabel('Principal component 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite optimal parameters (e.g. for xgboost) can be found on the web, we still want you to use grid/random search (or any other approach) to approximate them by yourself.\n",
    "\n",
    "Please try at least several models of different structure.\n",
    "\n",
    "Provide the following to describe your path:\n",
    "\n",
    "* Plot describing the model accuracy/precision/recall w.r.t. model complexity.\n",
    "* ROC-AUC plot for the 3 best models you aquired (for multiclass case you might refer to the `scikit-plot` library.\n",
    "* Small report describing your experiments.\n",
    "\n",
    "[DART](https://arxiv.org/abs/1505.01866) might be useful as well in your experiments. It is available in [xgboost](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html) and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html), but seems [missing in CatBoost](https://github.com/catboost/catboost/issues/1006).\n",
    "\n",
    "__Without the report and plots maximum score for this part of the lab is 0.3 of its full weight.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Blending the models\n",
    "\n",
    "Take three (or more) best models and try to build the blending ensemble of them. Compare the quality of the final model using the same quality measures as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Explaining the model and estimating the feature importances.\n",
    "\n",
    "Now your goal to take three best models and estimate feature importances using this models.\n",
    "\n",
    "* First, use the methods that libraries provide by default (e.g. `lightgbm.plot_importance`).\n",
    "* Next, use the [`shap`](https://github.com/slundberg/shap) library to explain the models behaviour and analyse the model performance. Compare the feature importances estimated by `shap` and by methods on the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
