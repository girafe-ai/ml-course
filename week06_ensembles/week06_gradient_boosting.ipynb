{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При разработке семинара использовались <a href=\"https://habr.com/ru/company/ods/blog/327250/\">материалы</a> ODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> План семинара </h3>\n",
    "\n",
    "* **Повторение bagging и RF**\n",
    "\n",
    "* **Boosting**\n",
    " - Постановка задачи\n",
    " - Gradient Boosting\n",
    " - Hand made Adaptive Gradient Boosting\n",
    "\n",
    "* **Выводы**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомним прошлый семинар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи про Bagging\n",
    "\n",
    "* Почему хорошими базовыми алгоритмами для бэггинга являются именно деревья?\n",
    "* Как оценить важность признаков по результатам  построения RF?\n",
    "* Как оценить качество RF с помощью out-of-bag процедура?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи на закрепление Bias\\Variance разложения\n",
    "\n",
    "* Что происходит в bias\\variance при изменении следующих параметров алгоритмов ML:\n",
    " * 1) рост максимально допустимой глубины $h$ дерева DT  (при неизменной обучающей выборке)?\n",
    " * 2) уменьшение минимально допустимого числа элементах в листах DT?\n",
    " * 3) рост числа соседей k в kNN?\n",
    " * 4) рост размера обучающей $\\ell$ выборки для kNN?\n",
    " * 5) рост размера обучающей $\\ell$ для DT?\n",
    "\n",
    "* Что можно сказать склонность алгоритмов к переобучению для при тех же сценариях?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<h1 align=\"center\">Boosting</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "Пусть у нас есть три бинарных классификатора, каждый из которых ошибается с вероятностью $p$.\n",
    "С какой вероятностью будет ошибаться классификатор, построенный с помощью простого голосования?\n",
    "При каких значениях эта вероятность будет меньше $p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача классификации на два класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отступ: $$ M_i = b(x_i)y_i$$\n",
    "Найти:\n",
    "\n",
    "алгоритм $a\\in \\textit{A}$, минимизирующий число ошибок на обучении:\n",
    "$$\n",
    "Q(a, X^\\ell) = \\sum\\limits_{i=1}^\\ell [y_i a(x_i) < 0] \\rightarrow \\min\\limits_{a}\n",
    "$$\n",
    "Имеем дискретный функционал, перейдём к его верхней оценке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции потерь\n",
    "<img src='pics/Boosting_Loss_Functions.png' height=600 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бустинг\n",
    "<img src='pics/Boosting.png' height=600 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный бустинг\n",
    "<img src='pics/GBM.png' height=600 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пошаговый пример работы градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C помощью GBM восстанавливаем зашумленную функцию $y=cos(x) + \\varepsilon, \\varepsilon \\sim N(0,\\dfrac{1}{5}), x \\in [-5, 5]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/gbm1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим GBM и будем рисовать два типа графиков: актуальное приближение $\\hat{f}(x)$ (синий график), а также каждое построенное дерево $\\hat{f}_t(x)$ на своих псевдо-остатках (зеленый график). Номер графика соответствует номеру итерации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/gbm2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/gbm3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ко второй итерации наши деревья повторили основную форму функции. Однако, на первой итерации мы видим, что алгоритм построил только \"левую ветвь\" функции. Почему так происходит?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Интерактивная демка, как GBM приближает функции: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Варианты классификационных функций потерь:\n",
    "\n",
    "***Logistic Loss:*** \n",
    "    $$L(y,f) = log(1 + exp(-2yf))$$\n",
    "***Adaboost Loss:***\n",
    "    $$L(y, f) = exp(-yf)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/gbm7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решаем игрушечную задачу классификации, в качестве функции потерь выберем Logistic Loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/gbm4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/gbm5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/gbm6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На псевдо-остатках видно, что у нас есть достаточно много корректно классифицированных наблюдений, и какое-то количество наблюдений с большими ошибками, которые появились из-за шума в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного теоретических задач про GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача 1***\n",
    "\n",
    "Что будет если обучать алгоритмы не на антиградиент, а на градиент в задаче регресии? Бинарной классификации? Насколько полезны будут получаться результаты?\n",
    "\n",
    "***Задача 2***\n",
    "\n",
    "Вы обучали градиентный бустинг, причём y-ки были в обучающей выборке были только положительные. Могут ли у вас получится отрицательные предсказания на тесте? А в бэггинге?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand made Adaptive Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Необходимо реализовать класс `Adaboost`** (для решения задачи классификации)\n",
    "\n",
    "**Спецификация:**\n",
    "- класс наследуется от `sklearn.BaseEstimator`;\n",
    "- конструктор содержит следующие параметры: \n",
    "    - `n_estimators` - количество базовых моделей (решающих пней) в ансамбле;\n",
    "    - `max_depth` - максимальная глубина дерева (по умолчанию - `None`);\n",
    "    \n",
    "- класс имеет методы `fit` и `predict`;\n",
    "- метод `fit` принимает матрицу объектов `X` и вектор ответов `y` (объекты `numpy.ndarray`) и возвращает экземпляр класса\n",
    "    `Adaboost`, представляющий собой ансамбль базовых моделей, обученный по выборке `(X, y)` с учётом заданных в конструкторе параметров; \n",
    "- метод `predict` принимает матрицу объектов и возвращает вектор предсказанных ответов;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "class Adaboost(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Boosting method that uses a number of weak classifiers in \n",
    "    ensemble to make a strong classifier. This implementation uses DecisionTreeClassifier. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clf: int\n",
    "        The number of weak classifiers that will be used. \n",
    "    max_depth: int\n",
    "        Максимальная глубина решающего дерева в составе ансамбля. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=5, max_depth=1, random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.alphas = []\n",
    "        self.clfs = []\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Инициализируйте веса объектов как 1/N\n",
    "        w = <ваш код>\n",
    "        \n",
    "        # Последовательное обучите self.n_estimators слабых моделей (решающих деревьев)\n",
    "        for _ in range(self.n_estimators):\n",
    "            clf = <ваш код>\n",
    "            \n",
    "            # Рассчитайте ошибку данной модели через веса объектов и предсказания данной модели\n",
    "            min_error = <ваш код>\n",
    "            \n",
    "            # Рассчитайте параметр alpha, с помощью которого происходит обновление весов объектов обучающей выборки\n",
    "            # Alpha можно считать аппроксимацией \"эффективности\" алгоритма\n",
    "            alpha = <ваш код>\n",
    "            \n",
    "            # Обновите веса объектов обучающей выбрки в соответствии с алгоритмом\n",
    "            # Ошибочно классифицированные объекты получат б`ольшие веса, чем корректно классифицированные объекты\n",
    "            w *= <ваш код>\n",
    "            \n",
    "            # Нормализуем обновлённые веса\n",
    "            w /= np.sum(w)\n",
    "            \n",
    "            # Сохраним слабую модель и её параметр alpha в список\n",
    "            self.clfs.append(clf)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "        \n",
    "        # От каждой слабой модели нужно получить предсказания\n",
    "        for alpha, clf in zip(self.alphas, self.clfs):\n",
    "            # Просуммируем взвешенные с параметром alpha значения предсказаний \n",
    "            y_pred += <ваш код>\n",
    "\n",
    "        # Результирующий класс определяется знаком взвешенной суммы предсказаний слабых моделей\n",
    "        y_pred = <ваш код>\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "# Сформируем целевые переменные для бинарной классификации\n",
    "digit1 = 1\n",
    "digit2 = 8\n",
    "idx = np.append(np.where(y == digit1)[0], np.where(y == digit2)[0])\n",
    "y = data.target[idx]\n",
    "\n",
    "# Сменим метки классов на {-1, 1}\n",
    "y[y == digit1] = -1\n",
    "y[y == digit2] = 1\n",
    "X = data.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Stump accuracy: 0.77528\n",
      "AdaBoost with 12 estimators accuracy: 0.91011\n"
     ]
    }
   ],
   "source": [
    "# Сформируем обучающую и тестовую выборки и сравним результат бустинга с результатом одной модели\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=SEED)\n",
    "\n",
    "n_estimators = 12\n",
    "clf = Adaboost(n_estimators=n_estimators, max_depth=1,random_state=SEED)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "base_clf = DecisionTreeClassifier(random_state=SEED, max_depth=1)\n",
    "base_clf.fit(X_train, y_train)\n",
    "y_pred_base = base_clf.predict(X_test)\n",
    "\n",
    "print (\"Decision Stump accuracy: {:.5f}\".format(accuracy_score(y_test, y_pred_base)))\n",
    "print (\"AdaBoost with {} estimators accuracy: {:.5f}\".format(n_estimators, accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn' AdaBoost with 12 estimators accuracy: 0.91011\n"
     ]
    }
   ],
   "source": [
    "# Сравним результаты с реализацией в scikit-learn\n",
    "ada = AdaBoostClassifier(base_estimator=base_clf,\n",
    "                        n_estimators=n_estimators, random_state=SEED,\n",
    "                        algorithm='SAMME')\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "print (\"Scikit-learn' AdaBoost with {} estimators accuracy: {:.5f}\".format(n_estimators, accuracy_score(y_test, y_pred_ada)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук составлен по мотивам:\n",
    "1. <a href=\"https://habrahabr.ru/company/ods/blog/327250/#postanovka-ml-zadachi\"> Open Data Science, открытый курс машинного обучения. Тема 10 </a>\n",
    "2. <a href=\"https://github.com/esokolov/ml-course-msu/tree/master/ML15-spring/lecture-notes\"> Лекции Евгения Соколова </a>\n",
    "3. <a href=\"https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending/\"> Блог Александра Дьяконова </a>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
