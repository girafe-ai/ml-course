{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5: Дополнительные материалы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<h1 align=\"center\">Useful libs</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/xgboost.png' width=100>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности:\n",
    "\n",
    "1. Базовый алгоритм приближает направление, посчитанное с учетом вторых производных функции потерь.\n",
    "\n",
    "2. Отклонение направления, построенного базовым алгоритмом, измеряется с помощью модифицированного функционала — из него удалено деление на вторую производную, за счет чего избегаются численные проблемы.\n",
    "\n",
    "3. Функционал регуляризуется -- добавляются штрафы за количество листьев и за норму коэффициентов.\n",
    "\n",
    "4. При построении дерева используется критерий информативности, зависящий от оптимального вектора сдвига.\n",
    "\n",
    "5. Критерий останова при обучении дерева также зависит от оптимального сдвига.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Installation\n",
    "\n",
    "http://xgboost.readthedocs.io/en/latest/build.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building on Ubuntu/Debian\n",
    "\n",
    "!git clone --recursive https://github.com/dmlc/xgboost\n",
    "!cd xgboost\n",
    "!make -j4\n",
    "\n",
    "###### Building on MacOS\n",
    "\n",
    "!brew install gcc5\n",
    "!pip install xgboost\n",
    "\n",
    "###### Building on Windows\n",
    "\n",
    "git submodule init\n",
    "git submodule update\n",
    "\n",
    "alias make='mingw32-make'\n",
    "\n",
    "cp make/mingw64.mk config.mk; make -j4\n",
    "\n",
    "mkdir build\n",
    "cd build\n",
    "cmake .. -G\"Visual Studio 12 2013 Win64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 3,  # максимальная глубина дерева\n",
    "    'eta': 0.3,  # шаг \n",
    "    'silent': 1,  # лог\n",
    "    'objective': 'multi:softprob',  # как оценивать ошибку для мультиклассовой классификации\n",
    "    'num_class': 3}  # число классов\n",
    "num_round = 20  # число итераций\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "preds = bst.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print('F1-score: {:.4f}'.format(metrics.f1_score(y_test, best_preds, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/catboost.png' width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/catboost/catboost\n",
    "\n",
    "https://tech.yandex.com/catboost/doc/dg/concepts/about-docpage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example\n",
    "\n",
    "dataset = np.array([[1,4,5,6],[4,5,6,7],[30,40,50,60],[20,15,85,60]])\n",
    "train_labels = [1.2,3.4,9.5,24.5]\n",
    "model = CatBoostRegressor(learning_rate=1, depth=6, loss_function='RMSE')\n",
    "fit_model = model.fit(dataset, train_labels)\n",
    "\n",
    "print(fit_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CatBoost Classifier\n",
    "\n",
    "# initialize data\n",
    "train_data = np.random.randint(0, 100, size=(100, 10))\n",
    "train_label = np.random.randint(0, 2, size=(100))\n",
    "test_data = np.random.randint(0, 100, size=(50, 10))\n",
    "\n",
    "# specify the training parameters \n",
    "model = CatBoostClassifier(iterations=2, depth=2, learning_rate=1, loss_function='Logloss', logging_level='Verbose')\n",
    "\n",
    "#train the model\n",
    "model.fit(train_data, train_label, cat_features=[0,2,5])\n",
    "\n",
    "# make the prediction using the resulting model\n",
    "preds_class = model.predict(test_data)\n",
    "preds_proba = model.predict_proba(test_data)\n",
    "print(\"class = \", preds_class)\n",
    "print(\"proba = \", preds_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CatBoost Regressor\n",
    "\n",
    "# initialize data\n",
    "train_data = np.random.randint(0, 100, size=(100, 10))\n",
    "train_label = np.random.randint(0, 1000, size=(100))\n",
    "test_data = np.random.randint(0, 100, size=(50, 10))\n",
    "\n",
    "# initialize Pool\n",
    "train_pool = Pool(train_data, train_label, cat_features=[0,2,5])\n",
    "test_pool = Pool(test_data, cat_features=[0,2,5]) \n",
    "\n",
    "# specify the training parameters \n",
    "model = CatBoostRegressor(iterations=2, depth=2, learning_rate=1, loss_function='RMSE')\n",
    "\n",
    "#train the model\n",
    "model.fit(train_pool)\n",
    "\n",
    "# make the prediction using the resulting model\n",
    "preds = model.predict(test_pool)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<h1 align=\"center\">Stacking, Blending etc.</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking & Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборку разбивают на части (фолды), затем последовательно перебирая фолды обучают базовые алгоритмы на всех фолдах, кроме одного, а на оставшемся получают ответы базовых алгоритмов и трактуют их как значения соответствующих признаков на этом фолде. Для получения метапризнаков объектов тестовой выборки базовые алгоритмы обучают на всей обучающей выборке и берут их ответы на тестовой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/stacking.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейшая схема стекинга — блендинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Делим обучающую выборку на два непересекающихся подмножества\n",
    "2. Обучаем несколько базовых моделей на первой части данных.\n",
    "3. Тестируем базовые модели на второй части.\n",
    "4. Используя предсказания из пункта три как признаки, а правильные ответы как таргеты обучаем классификатор \"второго уровня\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/blending.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные тут: https://www.kaggle.com/mubashir44/simple-ensemble-model-stacking/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = pd.read_csv(\"properties_2016.csv\")\n",
    "train_df = pd.read_csv(\"train_2016_v2.csv\")\n",
    "test_df = pd.read_csv(\"sample_submission.csv\")\n",
    "test_df = test_df.rename(columns={'ParcelId': 'parcelid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.merge(properties, how = 'left', on = 'parcelid')\n",
    "test = test_df.merge(properties, on='parcelid', how='left')\n",
    "y_train = train['logerror'].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обработка категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90275, 57) (2985217, 57)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder  \n",
    "\n",
    "lbl = LabelEncoder()\n",
    "\n",
    "for c in train.columns:\n",
    "    train[c]=train[c].fillna(0)\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl.fit(list(train[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "\n",
    "for c in test.columns:\n",
    "    test[c]=test[c].fillna(0)\n",
    "    if test[c].dtype == 'object':\n",
    "        lbl.fit(list(test[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "train_X = train.drop([\"parcelid\", \"transactiondate\", \"logerror\"], axis=1)\n",
    "test_X = test[train_X.columns]\n",
    "print(train_X.shape, test_X.shape)\n",
    "x_train = np.array(train_X)\n",
    "x_test = np.array(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обертка для удобной работы с базовыми моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полезности\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 10 # для воспроизводимости результатов\n",
    "NFOLDS = 5 # число фолдов для out-of-fold предсказаний\n",
    "kf = KFold(n_splits=NFOLDS, random_state=SEED)\n",
    "\n",
    "# Расширим Sklearn Regressor\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Строим простые модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры для регрессоров\n",
    "# Random Forest \n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 50,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees \n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':50,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost \n",
    "ada_params = {\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting \n",
    "gb_params = {\n",
    "    'n_estimators': 50,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 объекта для представления наших 4 моделей\n",
    "rf = SklearnHelper(clf=RandomForestRegressor, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesRegressor, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostRegressor, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingRegressor, seed=SEED, params=gb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagarinsname\\Miniconda3\\envs\\py3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning:\n",
      "\n",
      "Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "\n",
      "C:\\Users\\gagarinsname\\Miniconda3\\envs\\py3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning:\n",
      "\n",
      "Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "\n",
      "C:\\Users\\gagarinsname\\Miniconda3\\envs\\py3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning:\n",
      "\n",
      "Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "\n",
      "C:\\Users\\gagarinsname\\Miniconda3\\envs\\py3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning:\n",
      "\n",
      "Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
    "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.59250183e-03 0.00000000e+00 0.00000000e+00 2.34103618e-02\n",
      " 1.41760367e-02 2.32283597e-03 5.25650999e-03 2.37606327e-02\n",
      " 3.37316572e-03 1.09364402e-02 6.47641812e-02 5.26738585e-02\n",
      " 6.39517320e-04 8.74961609e-03 1.45880424e-02 5.02243960e-04\n",
      " 1.22890914e-04 4.26813999e-03 1.82941328e-02 7.10259273e-03\n",
      " 1.93509446e-02 3.04611439e-03 1.41268730e-03 8.27984088e-02\n",
      " 4.83835812e-02 4.21285031e-02 2.67682679e-03 2.57365699e-03\n",
      " 2.04261261e-04 1.93119450e-04 1.77025440e-03 1.64582824e-02\n",
      " 1.46680337e-02 4.43016098e-02 3.12424667e-02 2.51366536e-02\n",
      " 3.53491137e-03 1.18090022e-02 5.56877200e-02 1.61811361e-02\n",
      " 8.78288162e-04 6.51000725e-03 5.01893934e-04 1.01137071e-02\n",
      " 8.59955051e-03 4.85056132e-05 2.08874998e-02 2.70307554e-03\n",
      " 2.60709893e-04 7.22915276e-02 4.53514294e-02 0.00000000e+00\n",
      " 4.15754978e-02 7.93792947e-02 2.41496741e-03 2.29774205e-03\n",
      " 1.90944282e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagarinsname\\Miniconda3\\envs\\py3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning:\n",
      "\n",
      "Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.12589444e-02 6.18210132e-04 2.08917962e-03 2.44562548e-02\n",
      " 2.93484602e-02 5.72219829e-03 1.73728433e-02 1.67929407e-02\n",
      " 1.48383932e-03 5.57849654e-03 4.16359657e-02 4.87151037e-02\n",
      " 0.00000000e+00 7.76134608e-03 4.79168429e-03 3.09044027e-03\n",
      " 2.83179384e-03 7.34720562e-03 2.40234566e-02 4.55704381e-03\n",
      " 3.86450557e-03 3.73141672e-03 9.21582971e-03 3.78166481e-02\n",
      " 2.50734597e-02 2.51368969e-02 2.02880029e-02 9.31087064e-06\n",
      " 3.63885195e-04 2.35339003e-03 9.04909089e-03 9.96125455e-03\n",
      " 1.09031619e-02 2.83182211e-02 7.79754032e-03 7.43713887e-02\n",
      " 2.16150411e-03 1.12089969e-02 5.02686946e-02 5.79299527e-03\n",
      " 1.90486615e-04 9.89884859e-03 1.22422903e-04 6.40685323e-03\n",
      " 1.88423152e-03 3.24193296e-04 3.68324375e-02 2.87097285e-03\n",
      " 0.00000000e+00 8.75375590e-02 4.22585827e-02 0.00000000e+00\n",
      " 4.43745183e-02 7.98533472e-02 1.34979225e-02 7.39376988e-03\n",
      " 5.93922528e-02]\n",
      "[7.41245803e-04 0.00000000e+00 0.00000000e+00 1.87688296e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.40323055e-03\n",
      " 0.00000000e+00 4.40708314e-05 5.52421217e-02 2.18285922e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 5.88252650e-04 0.00000000e+00 2.26688448e-02 8.00370512e-03\n",
      " 3.89792147e-02 3.40885003e-02 3.12069794e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.83629753e-02 2.04205969e-02\n",
      " 0.00000000e+00 1.62392569e-02 8.00046545e-02 1.03284852e-01\n",
      " 0.00000000e+00 1.92828076e-02 6.19504660e-02 0.00000000e+00\n",
      " 0.00000000e+00 7.80717524e-04 0.00000000e+00 0.00000000e+00\n",
      " 7.52267967e-03 0.00000000e+00 5.71920320e-02 2.95933699e-03\n",
      " 0.00000000e+00 1.90627663e-01 9.62866249e-02 0.00000000e+00\n",
      " 8.54999882e-02 3.49971497e-02 0.00000000e+00 0.00000000e+00\n",
      " 1.50028383e-02]\n",
      "[4.49356535e-03 0.00000000e+00 1.01478990e-03 2.53740683e-03\n",
      " 1.30606089e-02 2.22982654e-03 9.90059914e-03 1.02406023e-02\n",
      " 0.00000000e+00 7.65684983e-03 4.42799731e-02 5.55777558e-02\n",
      " 8.54170674e-04 1.32127936e-02 3.44770355e-05 4.87783114e-03\n",
      " 0.00000000e+00 0.00000000e+00 4.48422789e-03 1.47351052e-04\n",
      " 2.57242141e-03 6.61961245e-04 2.50425816e-03 8.86291707e-02\n",
      " 4.59117330e-02 7.85422351e-02 7.56618945e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.62719272e-04 1.56002153e-02\n",
      " 4.45893050e-03 2.07029851e-02 4.45510086e-02 2.61620010e-02\n",
      " 0.00000000e+00 9.29369502e-03 4.61368268e-02 4.02256252e-03\n",
      " 0.00000000e+00 9.22144397e-06 0.00000000e+00 2.79741108e-03\n",
      " 2.35331915e-03 1.37485354e-03 3.95515590e-02 4.97058499e-03\n",
      " 0.00000000e+00 1.01164807e-01 8.29369461e-02 0.00000000e+00\n",
      " 4.85849665e-02 1.18980453e-01 4.97877031e-04 8.83353429e-03\n",
      " 1.55627241e-02]\n"
     ]
    }
   ],
   "source": [
    "rf_feature = rf.feature_importances(x_train,y_train)\n",
    "et_feature = et.feature_importances(x_train, y_train)\n",
    "ada_feature = ada.feature_importances(x_train, y_train)\n",
    "gb_feature = gb.feature_importances(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>ExtraTrees</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>GradientBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007972</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>0.006582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012576</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>0.187420</td>\n",
       "      <td>0.008531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008243</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>-0.009579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006819</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0.008508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008599</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>-0.302565</td>\n",
       "      <td>0.003702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RandomForest  ExtraTrees  AdaBoost  GradientBoost\n",
       "0      0.007972    0.007753  0.014950       0.006582\n",
       "1      0.012576    0.015212  0.187420       0.008531\n",
       "2      0.008243    0.012145  0.016856      -0.009579\n",
       "3      0.006819    0.006538  0.016856       0.008508\n",
       "4      0.008599    0.005762 -0.302565       0.003702"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n",
    "     'ExtraTrees': et_oof_train.ravel(),\n",
    "     'AdaBoost': ada_oof_train.ravel(),\n",
    "      'GradientBoost': gb_oof_train.ravel()\n",
    "    })\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Portland",
         "reversescale": true,
         "showscale": true,
         "type": "heatmap",
         "uid": "b0734386-1d76-401a-ae64-42809687f5b5",
         "x": [
          "RandomForest",
          "ExtraTrees",
          "AdaBoost",
          "GradientBoost"
         ],
         "y": [
          "RandomForest",
          "ExtraTrees",
          "AdaBoost",
          "GradientBoost"
         ],
         "z": [
          [
           1,
           0.441146650694706,
           0.12471279222145655,
           0.44298939578280166
          ],
          [
           0.441146650694706,
           1,
           0.17741431757230966,
           0.5356472458739513
          ],
          [
           0.12471279222145655,
           0.17741431757230966,
           1,
           0.10447846545942799
          ],
          [
           0.44298939578280166,
           0.5356472458739513,
           0.10447846545942799,
           1
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"516309e4-4259-49fc-b8b4-069c9bb78428\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"516309e4-4259-49fc-b8b4-069c9bb78428\")) {\n",
       "    Plotly.newPlot(\"516309e4-4259-49fc-b8b4-069c9bb78428\", [{\"colorscale\": \"Portland\", \"reversescale\": true, \"showscale\": true, \"x\": [\"RandomForest\", \"ExtraTrees\", \"AdaBoost\", \"GradientBoost\"], \"y\": [\"RandomForest\", \"ExtraTrees\", \"AdaBoost\", \"GradientBoost\"], \"z\": [[1.0, 0.441146650694706, 0.12471279222145655, 0.44298939578280166], [0.441146650694706, 1.0, 0.17741431757230966, 0.5356472458739513], [0.12471279222145655, 0.17741431757230966, 1.0, 0.10447846545942799], [0.44298939578280166, 0.5356472458739513, 0.10447846545942799, 1.0]], \"type\": \"heatmap\", \"uid\": \"bf87d43f-676b-4108-99f3-5606ad544341\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"516309e4-4259-49fc-b8b4-069c9bb78428\")) {window._Plotly.Plots.resize(document.getElementById(\"516309e4-4259-49fc-b8b4-069c9bb78428\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"516309e4-4259-49fc-b8b4-069c9bb78428\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"516309e4-4259-49fc-b8b4-069c9bb78428\")) {\n",
       "    Plotly.newPlot(\"516309e4-4259-49fc-b8b4-069c9bb78428\", [{\"colorscale\": \"Portland\", \"reversescale\": true, \"showscale\": true, \"x\": [\"RandomForest\", \"ExtraTrees\", \"AdaBoost\", \"GradientBoost\"], \"y\": [\"RandomForest\", \"ExtraTrees\", \"AdaBoost\", \"GradientBoost\"], \"z\": [[1.0, 0.441146650694706, 0.12471279222145655, 0.44298939578280166], [0.441146650694706, 1.0, 0.17741431757230966, 0.5356472458739513], [0.12471279222145655, 0.17741431757230966, 1.0, 0.10447846545942799], [0.44298939578280166, 0.5356472458739513, 0.10447846545942799, 1.0]], \"type\": \"heatmap\", \"uid\": \"bf87d43f-676b-4108-99f3-5606ad544341\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"516309e4-4259-49fc-b8b4-069c9bb78428\")) {window._Plotly.Plots.resize(document.getElementById(\"516309e4-4259-49fc-b8b4-069c9bb78428\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x=base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Portland',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    "    )\n",
    "]\n",
    "py.iplot(data, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Тренируем XGBoost на новых признаках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1)\n",
    "x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train\n",
    "y = y_train\n",
    "y_mean = np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.2, random_state=2000)\n",
    "\n",
    "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "dvalid = xgb.DMatrix(Xv, label=yv)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# Try different parameters! My favorite is random search :)\n",
    "xgb_params = {\n",
    "    'eta': 0.025,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.068521\tvalid-mae:0.067813\n",
      "Multiple eval metrics have been passed: 'valid-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mae hasn't improved in 300 rounds.\n",
      "[15]\ttrain-mae:0.067744\tvalid-mae:0.067366\n",
      "[30]\ttrain-mae:0.067471\tvalid-mae:0.067441\n",
      "[45]\ttrain-mae:0.067349\tvalid-mae:0.0676\n",
      "[60]\ttrain-mae:0.067286\tvalid-mae:0.067797\n",
      "[75]\ttrain-mae:0.067226\tvalid-mae:0.067986\n",
      "[90]\ttrain-mae:0.06717\tvalid-mae:0.068163\n",
      "[105]\ttrain-mae:0.067101\tvalid-mae:0.068291\n",
      "[120]\ttrain-mae:0.067043\tvalid-mae:0.068391\n",
      "[135]\ttrain-mae:0.066976\tvalid-mae:0.068463\n",
      "[150]\ttrain-mae:0.066911\tvalid-mae:0.068516\n",
      "[165]\ttrain-mae:0.066843\tvalid-mae:0.068563\n",
      "[180]\ttrain-mae:0.066761\tvalid-mae:0.068587\n",
      "[195]\ttrain-mae:0.066671\tvalid-mae:0.068655\n",
      "[210]\ttrain-mae:0.066593\tvalid-mae:0.068687\n",
      "[225]\ttrain-mae:0.066494\tvalid-mae:0.068702\n",
      "[240]\ttrain-mae:0.06642\tvalid-mae:0.068734\n",
      "[255]\ttrain-mae:0.066313\tvalid-mae:0.068768\n",
      "[270]\ttrain-mae:0.066225\tvalid-mae:0.068795\n",
      "[285]\ttrain-mae:0.066124\tvalid-mae:0.068811\n",
      "[300]\ttrain-mae:0.06603\tvalid-mae:0.068848\n",
      "[315]\ttrain-mae:0.065945\tvalid-mae:0.068881\n",
      "Stopping. Best iteration:\n",
      "[16]\ttrain-mae:0.067715\tvalid-mae:0.067358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.train(xgb_params, dtrain, 2000, watchlist, early_stopping_rounds=300,\n",
    "                  maximize=False, verbose_eval=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(x_test)\n",
    "predicted_test_xgb = model_xgb.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing csv ...\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = predicted_test_xgb\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('xgb_stacked.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук составлен по мотивам:\n",
    "1. <a href=\"https://habrahabr.ru/company/ods/blog/327250/#postanovka-ml-zadachi\"> Open Data Science, открытый курс машинного обучения. Тема 10 </a>\n",
    "2. <a href=\"https://github.com/esokolov/ml-course-msu/tree/master/ML15-spring/lecture-notes\"> Лекции Евгения Соколова </a>\n",
    "3. <a href=\"https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending/\"> Блог Александра Дьяконова </a>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
