{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ml-mipt course\n",
    "\n",
    "## Seminar 2: Linear Regression & other stuff\n",
    "\n",
    "Based on [Evgeny Sokolov](https://github.com/esokolov) open materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at Linear Regression and its implementations in this notebook.\n",
    "\n",
    "__Contents__:\n",
    "* Linear Regression analytical solution\n",
    "* Gradient descent approach\n",
    "* Stochastic gradient\n",
    "* Linear Regression out of the box (sklearn, vw, etc.)\n",
    "* MSE and MAE in Linear Regression\n",
    "\n",
    "See `week02_extra*` notebooks for extra (more complex or just additional) materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Google Colab, uncomment the next line to download `utils.py`\n",
    "\n",
    "# !wget https://https://raw.githubusercontent.com/girafe-ai/ml-mipt/784d834b01893b984cbad8c43f3d053bd5341573/week0_02_linear_reg/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({\"font.size\": 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to generate features matrix $X$ and correct weights vector $w_{true}$. Targer vector $Y$ is computed as  $Xw_{true}$ with gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "RANDOM_SEED = 45\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Let it be the *true* weights vector\n",
    "w_true = np.random.normal(size=(n_features,))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "# For different scales of features\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n",
    "\n",
    "# Here comes the *true* target vector\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recap:*\n",
    "Analytical solution formula is simple:\n",
    "\n",
    "$$\n",
    "w = (X^TX)^{-1}X^Ty.\n",
    "$$\n",
    "\n",
    "Let's check how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the analytical solution is quite close to the original one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "\n",
    "Здесь присутствует обращение матрицы $X^TX$ — очень трудоёмкая операция при большом количестве признаков. Нетрудно подсчитать, что сложность вычислений $O(d^3 + d^2l)$. При решении задач такая трудоёмкость часто оказывается непозволительной, поэтому параметры ищут итерационными методами, стоимость которых меньше. Один из них — градиентный спуск.\n",
    "\n",
    "Напомним, что в градиентном спуске значения параметров на следующем шаге получаются из значений параметров на текущем шаге смещением в сторону антиградиента функционала: \n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta_t \\nabla Q(w^{(t)}),$$\n",
    "где $\\eta_t$ — длина шага градиентного спуска.\n",
    "\n",
    "Формула градиента функции ошибки в случае MSE выглядит следующим образом:\n",
    "\n",
    "$$\\nabla Q(w) = -2X^Ty + 2X^TXw = 2X^T(Xw - y).$$\n",
    " \n",
    "Сложность вычислений в данном случае $O(dl)$. Стохастический градиентный спуск отличается от обычного заменой градиента на несмещённую оценку по одному или нескольким объектам. В этом случае сложность становится $O(kd)$, где $k$ — количество объектов, по которым оценивается градиент, $k << l$. Это отчасти объясняет популярность стохастических методов оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация траекторий GD и SGD\n",
    "На простом примере разберём основные тонкости, связанные со стохастической оптимизацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим на сгенерированных данных линейную регрессию для MSE при помощи полного градиентного спуска — тем самым получим вектор параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажем последовательность оценок параметров $w^{(t)}$, получаемых в ходе итераций. Красная точка — $w_{true}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = np.random.uniform(-2, 2, n_features) - 0.5\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "step_size = 1e-2\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w -= 2 * step_size * np.dot(X.T, X @ w - Y) / Y.size\n",
    "    w_list.append(w.copy())\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title(\"GD trajectory\")\n",
    "plt.xlabel(\"$w_1$\")\n",
    "plt.ylabel(\"$w_2$\")\n",
    "plt.xlim((w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=15), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c=\"r\")\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент перпендикулярен линиям уровня. Это объясняет такие зигзагообразные траектории градиентного спуска. Для большей наглядности в каждой точке пространства посчитаем градиент функционала и покажем его направление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "A_mini, B_mini = np.meshgrid(np.linspace(-2, 2, 20), np.linspace(-2, 2, 27))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "# visualize the level set\n",
    "plt.figure(figsize=(13, 9))\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(-1, 1.5, num=30), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize the gradients\n",
    "gradients = np.empty_like(A_mini)\n",
    "for i in range(A_mini.shape[0]):\n",
    "    for j in range(A_mini.shape[1]):\n",
    "        w_tmp = np.array([A_mini[i, j], B_mini[i, j]])\n",
    "        antigrad = -2 * 1e-3 * np.dot(X.T, np.dot(X, w_tmp) - Y) / Y.shape[0]\n",
    "        plt.arrow(A_mini[i, j], B_mini[i, j], antigrad[0], antigrad[1], head_width=0.02)\n",
    "\n",
    "plt.title(\"Antigradients demonstration\")\n",
    "plt.xlabel(r\"$w_1$\")\n",
    "plt.ylabel(r\"$w_2$\")\n",
    "plt.xlim((w_true[0] - 1.5, w_true[0] + 1.5))\n",
    "plt.ylim((w_true[1] - 0.5, w_true[1] + 0.7))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем теперь траектории стохастического градиентного спуска, повторив те же самые действия, оценивая при этом градиент по подвыборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "batch_size = 10\n",
    "w = w_0.copy()\n",
    "w_history_list = [w.copy()]\n",
    "lr = 1e-2\n",
    "\n",
    "for i in range(num_steps):\n",
    "    sample_indices = np.random.randint(0, n_objects, size=batch_size)\n",
    "    X_sample = X[sample_indices, :]\n",
    "    Y_sample = Y[sample_indices]\n",
    "    w -= 2 * step_size * np.dot(X_sample.T, X_sample @ w - Y_sample) / batch_size\n",
    "    w_history_list.append(w.copy())\n",
    "w_history_list = np.array(w_history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title(\"SGD trajectory\")\n",
    "plt.xlabel(r\"$w_1$\")\n",
    "plt.ylabel(r\"$w_2$\")\n",
    "plt.xlim((w_history_list[:, 0].min() - 0.1, w_history_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_history_list[:, 1].min() - 0.1, w_history_list[:, 1].max() + 0.1))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=30), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c=\"r\")\n",
    "plt.scatter(w_history_list[:, 0], w_history_list[:, 1])\n",
    "plt.plot(w_history_list[:, 0], w_history_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, метод стохастического градиента «бродит» вокруг оптимума. Это объясняется подбором шага градиентного спуска $\\eta_k$. Дело в том, что для сходимости стохастического градиентного спуска для последовательности шагов $\\eta_k$ должны выполняться [условия Роббинса-Монро](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586):\n",
    "$$\n",
    "\\sum_{k = 1}^\\infty \\eta_k = \\infty, \\qquad \\sum_{k = 1}^\\infty \\eta_k^2 < \\infty.\n",
    "$$\n",
    "Интуитивно это означает следующее: \n",
    "1. последовательность должна расходиться, чтобы метод оптимизации мог добраться до любой точки пространства, \n",
    "2. но расходиться не слишком быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем посмотреть на траектории SGD, последовательность шагов которой удовлетворяет условиям Роббинса-Монро:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "lr_0 = 0.01\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lr = lr_0 / ((i + 1) ** 0.505)  # What should the power be? )\n",
    "    sample_indices = np.random.randint(0, n_objects, size=batch_size)\n",
    "    X_sample = X[sample_indices, :]\n",
    "    Y_sample = Y[sample_indices]\n",
    "    w -= 2 * lr * np.dot(X_sample.T, X_sample @ w - Y_sample) / batch_size\n",
    "    w_list.append(w.copy())\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title(\"SGD trajectory\")\n",
    "plt.xlabel(r\"$w_1$\")\n",
    "plt.ylabel(r\"$w_2$\")\n",
    "plt.xlim((w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=40), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c=\"r\")\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение скоростей сходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последнее, что хотелось бы продемонстрировать — сравнение, насколько быстро достигают оптимума метод полного и стохастического градиентного спуска. Сгенерируем выборку и построим график зависимости функционала от итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation\n",
    "n_features = 50\n",
    "n_objects = 1000\n",
    "num_steps = 200\n",
    "batch_size = 10\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, n_features)\n",
    "\n",
    "X = np.random.uniform(-10, 10, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 5, n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sgd = 1e-3\n",
    "lr_gd = 1e-3\n",
    "w_sgd = np.random.uniform(-4, 4, n_features)\n",
    "w_gd = w_sgd.copy()\n",
    "residuals_sgd = [np.mean(np.power(np.dot(X, w_sgd) - Y, 2))]\n",
    "residuals_gd = [np.mean(np.power(np.dot(X, w_gd) - Y, 2))]\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lr = lr_sgd / ((i + 1) ** 0.51)\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    w_sgd -= 2 * lr * np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample]) / batch_size\n",
    "    residuals_sgd.append(np.mean(np.power(np.dot(X, w_sgd) - Y, 2)))\n",
    "\n",
    "    w_gd -= 2 * lr_gd * np.dot(X.T, np.dot(X, w_gd) - Y) / Y.shape[0]\n",
    "    residuals_gd.append(np.mean(np.power(np.dot(X, w_gd) - Y, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "plt.plot(range(num_steps + 1), residuals_gd, label=\"Full GD\")\n",
    "plt.plot(range(num_steps + 1), residuals_sgd, label=\"SGD\")\n",
    "plt.title(\"Empirial risk over iterations\")\n",
    "plt.xlim((-1, num_steps + 1))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iter num\")\n",
    "plt.ylabel(r\"Q($w$)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, GD буквально за несколько итераций оказывается вблизи оптимума, в то время как поведение SGD может быть весьма нестабильным. Как правило, для более сложных моделей наблюдаются ещё большие флуктуации в зависимости качества функционала от итерации при использовании стохастических градиентных методов. Путём подбора величины шага можно добиться лучшей скорости сходимости, и существуют методы, адаптивно подбирающие величину шага (AdaGrad, Adam, RMSProp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression out of the box\n",
    "\n",
    "Finally, let's take a brief look at implemented versions of Linear Regression from sklearn. The main classes are:\n",
    "\n",
    "* [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) — classical linear regression (*actially, it is just `scipy.linalg.lstsq` wrapped with sklearn `Predictor` class) __analytical__ solver.\n",
    "* [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) — Linear regression with L1 regularization.\n",
    "* [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) — Linear regression with L2 regularization.\n",
    "\n",
    "To minimize any other error function you are free to use [SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) (or wait a few weeks and we will get the great *PyTorch* automatic differentiation engine).\n",
    "\n",
    "Let's compare the speed of analytical and gradient solutions from the sklearn realizations.\n",
    "\n",
    "IPython magic `%%time` wrapper will be used.\n",
    "\n",
    "To measure the quality $R^2$ score will be used. It compares our model (`a`) with one always predicting mean `y`:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - a(x_i))^2}{\\sum_i (y_i - \\overline{y}_i)^2}$$\n",
    "\n",
    "`LinearRegression` vs. `Ridge`: __Fight!__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/mortal_combat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 700\n",
    "n_objects = 100000\n",
    "num_steps = 150\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, (n_features, 1))\n",
    "\n",
    "X = np.random.uniform(-100, 100, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 10, (n_objects, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "print(f\"R2: {lr.score(X, Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = Ridge(alpha=0.0, solver=\"sparse_cg\")\n",
    "lr.fit(X, Y)\n",
    "print(f\"R2: {lr.score(X, Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = Lasso(alpha=1.0)\n",
    "lr.fit(X, Y)\n",
    "print(f\"R2: {lr.score(X, Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции потерь в регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функционал качества в задачах обучения с учителем обычно задается в виде суммы по объектам выборки:\n",
    "$$Q(a) = \\frac 1 \\ell \\sum_{i=1}^\\ell L(y_i, a(x_i)),$$\n",
    "где $L(\\cdot, \\cdot)$ - функция потерь, задающая штраф за разницу между предсказанием и истинным значением целевого признака. Свойства функции потерь:\n",
    "* $L(y_i, a(x_i)) \\geqslant 0$;\n",
    "* $L(y_i, y_i) = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как отмечалось на первой лекции, функционал качества должен в первую очередь отвечать требованиям заказчика, при этом математические свойства функции потерь могут быть неудобны для оптимизации. \n",
    "\n",
    "__Пример:__ если мы не различаем маленькие ошибки (между 0.01 и 0.1 нет особой разницы), но зато не хотим получать большие ошибки, можно использовать следующую функцию потерь:\n",
    "\n",
    "$$L(y_i, a(x_i)) = [| y_i - a(x_i) | < \\varepsilon],$$ $\\varepsilon$ - допустимая разница между предсказанием и фактом.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среднеквадратичная и средняя абсолютная ошибка\n",
    "Кроме требований заказчика, функционал качества должен учитывать математические особенности модели, например устойчивость к шумовым объектам. В линейной регрессии Mean Squared Error: $L(y_i, a(x_i)) = (a(x_i) - y_i)^2$ не обладает этим свойством, потому что задает очень большие штрафы за большие отклонения от фактического значения. \n",
    "\n",
    "Рассмотрим это явление на примере. Выберем один признак, от которого целевой признак (имеющий индекс 15 в матрице X) зависит практически линейно. Добавим к выборке два объекта-выброса и посмотрим, как изменится оптимизированная на MSE прямая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/data_preprocessed.json\") as file:\n",
    "    X = pd.read_json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subset = X[[7, 15]].values\n",
    "# add two outliers\n",
    "X_subset_modified = np.vstack((X_subset, [[1, 90], [2, 50]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_points_and_plot_line_MSE(X_subset):\n",
    "    plt.scatter(X_subset[:, 0], X_subset[:, 1])\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_subset[:, 0][:, None], X_subset[:, 1])\n",
    "    grid = np.linspace(0, 2, 100)\n",
    "    line = lr.predict(grid[:, None])\n",
    "    plt.plot(grid, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter_points_and_plot_line_MSE(X_subset)\n",
    "plt.ylim(-20, 100)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter_points_and_plot_line_MSE(X_subset_modified)\n",
    "plt.ylim(-20, 100)\n",
    "plt.xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за шумовых объектов прямая достаточно сильно изменила наклон. Поэтому вместо MSE часто используют Mean Absoulte Error:\n",
    "$$L(y_i, a(x_i)) = |y_i - a(x_i)|$$\n",
    "\n",
    "Теперь обучим регрессию, оптимизируя MAE. В sklearn такая регрессия не реализована, но можно использовать модуль statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_points_and_plot_line_MAE(X_subset):\n",
    "    mod = smf.quantreg(\n",
    "        \"f15 ~ f7\", pd.DataFrame(data=X_subset, columns=[\"f7\", \"f15\"])\n",
    "    )  # задаеем зависимость и передаем данные\n",
    "    res = mod.fit(q=0.5)\n",
    "    plt.scatter(X_subset[:, 0], X_subset[:, 1])  # визуализируем точки\n",
    "    grid = np.linspace(0, 2, 100)\n",
    "    plt.plot(grid, grid * res.params[\"f7\"] + res.params[\"Intercept\"])  # визуализируем прямую\n",
    "    return mod, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "model, result = scatter_points_and_plot_line_MAE(X_subset)\n",
    "plt.ylim(-20, 100)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.subplot(1, 2, 2)\n",
    "model, result = scatter_points_and_plot_line_MAE(X_subset_modified)\n",
    "plt.ylim(-20, 100)\n",
    "plt.xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямая не изменила направление из-за выбросов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем добавить больше шумовых объектов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_subset_modified_twice = np.vstack(\n",
    "    (\n",
    "        X_subset_modified,\n",
    "        np.random.randint(5, size=60).reshape(-1, 2) * [1, 30],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "model, result = scatter_points_and_plot_line_MAE(X_subset)\n",
    "plt.ylim(-20, 100)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.subplot(1, 2, 2)\n",
    "model, result = scatter_points_and_plot_line_MAE(X_subset_modified_twice)\n",
    "plt.ylim(-20, 100)\n",
    "plt.xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямая изменила наклон, когда мы добавили 30 (почти 15%) шумовых точек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мультиколлинеарность и регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решение задачи МНК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_linear_regression(X_train, Y_train):\n",
    "    return np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habrahabr.ru/post/206306/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/energy_efficiency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посмотрим на скоррелированность данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = data.drop([\"Y1\", \"Y2\"], axis=1).corr()\n",
    "sns.heatmap(corr, square=True, ax=ax, cmap=sns.diverging_palette(220, 10, as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr, square=True, ax=ax, cmap=sns.diverging_palette(220, 10, as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что x1 скоррелирован с x2, а x4 с x5. Из-за этого матрица $X^{T}*X$ необратима."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посмотрим как на таких данных отработает наша линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем выборку на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Y1\", \"Y2\"], axis=1)\n",
    "y = data[\"Y1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим регрессию и посмотрим на качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = my_linear_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict(X_train, w)\n",
    "print(\"Train MSE: \", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Train R2: \", r2_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(X_test, w)\n",
    "print(\"Test MSE: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Test R2: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то не очень"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Попробуем убрать скоррелированные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"X1\", \"X4\", \"Y1\", \"Y2\"], axis=1)\n",
    "y = data[\"Y1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим регрессию и посмотрим на качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = my_linear_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict(X_train, w)\n",
    "print(\"Train MSE: \", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Train R2: \", r2_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(X_test, w)\n",
    "print(\"Test MSE: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Test R2: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Юху! Получили алгоритм с хорошим качеством"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализуем линейную регрессию с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_linear_regression(X_train, Y_train, l2=0):\n",
    "    return (\n",
    "        np.linalg.inv(X_train.T.dot(X_train) + l2 * np.eye(X_train.shape[1]))\n",
    "        .dot(X_train.T)\n",
    "        .dot(y_train)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим регрессию с регуляризацией и посмотрим на качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Y1\", \"Y2\"], axis=1)\n",
    "y = data[\"Y1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = my_linear_regression(X_train, y_train, l2=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict(X_train, w)\n",
    "print(\"Train MSE: \", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Train R2: \", r2_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(X_test, w)\n",
    "print(\"Test MSE: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Test R2: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этого же эффекта (отсутствие переобучения) добились добавив регуляризацию"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
