{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ml-mipt course\n",
    "## Seminar 2\n",
    "## Linear Regression & other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at Linear Regression and its implementations in this notebook.\n",
    "\n",
    "__Contents__:\n",
    "* Linear Regression analytical solution\n",
    "* Unstability of the solution in case of multicollinear features\n",
    "* Gradient descent approach\n",
    "* Stochastic gradient\n",
    "* Instability analysis\n",
    "* Linear Regression out of the box (sklearn, vw, etc.)\n",
    "\n",
    "See `week0_02_extra*` notebooks for extra (more complex or just additional) materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, uncomment the next line to download `utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:50.117610Z",
     "start_time": "2021-03-28T09:48:50.115531Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://https://raw.githubusercontent.com/girafe-ai/ml-mipt/784d834/week0_02_linear_reg/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:50.544504Z",
     "start_time": "2021-03-28T09:48:50.304030Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T11:32:04.853241Z",
     "start_time": "2021-09-25T11:32:04.757230Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 45\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to generate features matrix $X$ and correct weights vector $w_{true}$. Targer vector (or matrix in general case) $Y$ is computed as  $X\\mathbf{w}_{\\text{true}}$ with gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:50.964189Z",
     "start_time": "2021-03-28T09:48:50.960392Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "# Let it be the *true* weights vector\n",
    "w_true = np.random.normal(size=(n_features,))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "# For different scales of features. In case of 3 features the code is equal to the commented line below\n",
    "# X *= np.arange([1, 3, 5])[None, :]\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n",
    "\n",
    "# Here comes the *true* target vector\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recap:*\n",
    "In case of linear model\n",
    "$$\n",
    "\\hat{Y} = X\\mathbf{w}\n",
    "$$\n",
    "and __MSE__ loss function\n",
    "$$\n",
    "Q(Y, X, \\mathbf{w}) = MSE(Y, X\\mathbf{w}) =  \\|Y - X\\mathbf{w}\\|^2_2 = \\sum_i (y_i - \\mathbf{x}^T_i \\mathbf{w})^2\n",
    "$$\n",
    "analytical solution takes simple form:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = (X^T X)^{-1}X^T Y.\n",
    "$$\n",
    "\n",
    "_To do: derive it on the practice session._\n",
    "\n",
    "Let's check how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:51.675147Z",
     "start_time": "2021-03-28T09:48:51.672221Z"
    }
   },
   "outputs": [],
   "source": [
    "w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:51.910831Z",
     "start_time": "2021-03-28T09:48:51.887736Z"
    }
   },
   "outputs": [],
   "source": [
    "w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:52.290612Z",
     "start_time": "2021-03-28T09:48:52.287976Z"
    }
   },
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the analytical solution is quite close to the original one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the dataset with correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:53.245115Z",
     "start_time": "2021-03-28T09:48:53.241762Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "eps = 1e-3\n",
    "\n",
    "# Let it be the *true* weights vector\n",
    "w_true = np.random.normal(size=(n_features,))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "# Now we duplicate the second feature with some small noise, so featues 2 and 3 are collinear\n",
    "X[:, -1] = X[:, -2] + np.random.uniform(-eps, eps, X[:, -2].shape)\n",
    "\n",
    "# Here comes the *true* target vector\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:53.547161Z",
     "start_time": "2021-03-28T09:48:53.543640Z"
    }
   },
   "outputs": [],
   "source": [
    "w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:53.818874Z",
     "start_time": "2021-03-28T09:48:53.816086Z"
    }
   },
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the second and third coefficents are opposite. This makes our model highly *unstable*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could one actually fix it? Here comes the __regularization__.\n",
    "\n",
    "Let's use the L2 norm of weigths vector as a regularization term to constrain the desired solution.\n",
    "\n",
    "$$\n",
    "Q_{\\text{reg}}(Y, X, \\mathbf{w}) = MSE(Y, X\\mathbf{w}) + \\lambda\\|\\mathbf{w}\\|_2^2=  \\|Y - X\\mathbf{w}\\|^2_2 + \\lambda\\|\\mathbf{w}\\|^2_2= \\sum_i (y_i - \\mathbf{x}^T_i \\mathbf{w})^2 + \\sum_p w^2_p\n",
    "$$\n",
    "\n",
    "Analytical solution is available in this case as well:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^*_{\\text{reg}} = (X^T X + \\lambda I_p)^{-1}X^T Y,\n",
    "$$\n",
    "where $I_p$ is diagonal matrix consisting of 1s (with size p).\n",
    "\n",
    "__Be careful with the regularization term if you have included the column of 1s into X matrix! We do not want regularize the bias (free) term in our linear model.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:54.635974Z",
     "start_time": "2021-03-28T09:48:54.632373Z"
    }
   },
   "outputs": [],
   "source": [
    "w_star_reg = np.linalg.inv(X.T.dot(X) + 0.05 * np.eye(n_features)).dot(X.T).dot(Y)\n",
    "w_star_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:54.858654Z",
     "start_time": "2021-03-28T09:48:54.855854Z"
    }
   },
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "The analytical solution described above includes invertion of the matrix $X^T X$ (or $X^T X + \\lambda I$), which is quite expensive in terms of computation resourses. The complexity of matrix inversion can be estimated as $O(p^3 + p^2 N)$. This leads us to the iterative optimization methods, which are more efficient and are de-facto the main approach to optimization in Machine Learning.\n",
    "\n",
    "Gradient descent is one of the most popular optimization methods. Worth to mention the fact that the minimization (maximization) target (e.g loss function value) should be differentiable w.r.t model parameters. Using the gradient descent, the weights vector $\\mathbf{w}^{(t+1)}$ on step $t+1$ can be expressed in the following form:\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta_t \\nabla Q(\\mathbf{w}^{(t)}),\n",
    "$$\n",
    "where $\\eta_t$ stays for the gradient step (usually referred as _learning rate_).\n",
    "\n",
    "The gradient in case of MSE loss function takes the following form:\n",
    "\n",
    "$$\n",
    "\\nabla Q(\\mathbf{w}) = -2X^TY + 2X^TX\\mathbf{w} = 2X^T(X\\mathbf{w} - Y).\n",
    "$$\n",
    "\n",
    "In this case the complexity is only $O(pN)$. To make it even more effective (and using the hypothesis of homogeneous data in the dataset) one could use _stochastic gradient descent_, which computes the gradient only over some random subset of data K points, so the final complexity decreases to $O(pK)$, where $K << N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuailizing the gradient descent trajectory\n",
    "This part is deeply based on [Evgeny Sokolov](https://github.com/esokolov) open materials.\n",
    "\n",
    "Let's take a close look on the optimization path in simple two-dimentional space (where features are in different scales). We will use MSE loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below show $\\mathbf{w}^{(t)}$ values on every step $t$. The red dot in the center stays for $\\mathbf{w}_{\\text{true}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:56.339490Z",
     "start_time": "2021-03-28T09:48:56.336206Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "# Let it be the *true* weights vector\n",
    "w_true = np.random.normal(size=(n_features,))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "\n",
    "# For different scales of features. In case of 3 features the code is equal to the commented line below\n",
    "# X *= np.arange([1, 3, 5])[None, :]\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n",
    "\n",
    "# Here comes the *true* target vector\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:56.845726Z",
     "start_time": "2021-03-28T09:48:56.711739Z"
    }
   },
   "outputs": [],
   "source": [
    "w_0 = np.random.uniform(-2, 2, n_features) - 0.5\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "step_size = 1e-2\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w -= None  # YOUR CODE HERE\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:48:57.608099Z",
     "start_time": "2021-03-28T09:48:57.065085Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title(\"GD trajectory\")\n",
    "plt.xlabel(\"$w_1$\")\n",
    "plt.ylabel(\"$w_2$\")\n",
    "plt.xlim(w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1)\n",
    "plt.ylim(w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=15), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c=\"r\")\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient vector is orthogonal to the equipotential surface . That's the reason why the optimization path is not so smooth. Let's visualize the gradient directions to make it more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "A_mini, B_mini = np.meshgrid(np.linspace(-3, 3, 40), np.linspace(-3, 3, 40))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "# visualize the level set\n",
    "plt.figure(figsize=(13, 9))\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(-1, 1.5, num=40), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize the gradients\n",
    "gradients = np.empty_like(A_mini)\n",
    "for i in range(A_mini.shape[0]):\n",
    "    for j in range(A_mini.shape[1]):\n",
    "        w_tmp = np.array([A_mini[i, j], B_mini[i, j]])\n",
    "        antigrad = -2 * 1e-3 * np.dot(X.T, np.dot(X, w_tmp) - Y) / Y.shape[0]\n",
    "        plt.arrow(A_mini[i, j], B_mini[i, j], antigrad[0], antigrad[1], head_width=0.02)\n",
    "\n",
    "plt.title(\"Antigradients demonstration\")\n",
    "plt.xlabel(r\"$w_1$\")\n",
    "plt.ylabel(r\"$w_2$\")\n",
    "plt.xlim((w_true[0] - 1.5, w_true[0] + 1.5))\n",
    "plt.ylim((w_true[1] - 0.5, w_true[1] + 0.5))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the _stochastic gradient descent_. Let the number of elements the loss function computed on each state (`batch_size`) be equal to $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "w = w_0.copy()\n",
    "w_history_list = [w.copy()]\n",
    "lr = 1e-2\n",
    "\n",
    "for i in range(num_steps):\n",
    "    sample_indices = None  # YOUR CODE HERE\n",
    "    w -= None  # YOUR CODE HERE\n",
    "    w_history_list.append(w.copy())\n",
    "\n",
    "w_history_list = np.array(w_history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title(\"SGD trajectory\")\n",
    "plt.xlabel(r\"$w_1$\")\n",
    "plt.ylabel(r\"$w_2$\")\n",
    "plt.xlim((w_history_list[:, 0].min() - 0.1, w_history_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_history_list[:, 1].min() - 0.1, w_history_list[:, 1].max() + 0.1))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=30), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c=\"r\")\n",
    "plt.scatter(w_history_list[:, 0], w_history_list[:, 1])\n",
    "plt.plot(w_history_list[:, 0], w_history_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plot, SGD \"wanders\" around the optima. It is controlled by the SGD step size $\\eta_k$ and the convergence is not guaranteed in general case. For SGD method convergence given the sequence of steps $\\{\\eta_k\\}$ it is necessary that [Robbins-Monroe Conditions](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586) are satisfied:\n",
    "$$\n",
    "\\sum_{k = 1}^\\infty \\eta_k = \\infty, \\qquad \\sum_{k = 1}^\\infty \\eta_k^2 < \\infty.\n",
    "$$\n",
    "More intuitively, those conditions may be explained as follows:\n",
    "1. A sequence of steps $\\{\\eta_k\\}$ should diverge, so optimization method is capable or reaching any point in the given parameter space,\n",
    "2. At the same time it should diverge \"not so fast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze SGD trajectories, which are generated by a sequence of steps, satisfying the Robbins-Monroe Conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "lr_0 = 0.01\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lr = lr_0 / ((i + 1) ** 0.505)  # What should the power be? )\n",
    "    sample_indices = None  # YOUR CODE HERE\n",
    "    w -= None  # YOUR CODE HERE\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title(\"SGD trajectory\")\n",
    "plt.xlabel(r\"$w_1$\")\n",
    "plt.ylabel(r\"$w_2$\")\n",
    "plt.xlim((w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=40), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8)\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c=\"r\")\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the convergence speed\n",
    "Finally, it is important to compare the convergence speed for full and stochastic GD. Let's generate a random dataset and plot the loss function value w.r.t. iteration number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation\n",
    "n_features = 50\n",
    "n_objects = 1000\n",
    "num_steps = 500\n",
    "batch_size = 10\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, n_features)\n",
    "\n",
    "X = np.random.uniform(-10, 10, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 5, n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sgd = 1e-3\n",
    "lr_gd = 1e-3\n",
    "w_sgd = np.random.uniform(-4, 4, n_features)\n",
    "w_gd = w_sgd.copy()\n",
    "residuals_sgd = [np.mean(np.power(np.dot(X, w_sgd) - Y, 2))]\n",
    "residuals_gd = [np.mean(np.power(np.dot(X, w_gd) - Y, 2))]\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lr = lr_sgd / ((i + 1) ** 0.51)\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    w_sgd -= 2 * lr * np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample]) / batch_size\n",
    "    residuals_sgd.append(np.mean(np.power(np.dot(X, w_sgd) - Y, 2)))\n",
    "\n",
    "    w_gd -= 2 * lr_gd * np.dot(X.T, np.dot(X, w_gd) - Y) / Y.shape[0]\n",
    "    residuals_gd.append(np.mean(np.power(np.dot(X, w_gd) - Y, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "plt.plot(range(num_steps + 1), residuals_gd, label=\"Full GD\")\n",
    "plt.plot(range(num_steps + 1), residuals_sgd, label=\"SGD\")\n",
    "plt.title(\"Empirial risk over iterations\")\n",
    "plt.xlim((-1, num_steps + 1))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iter num\")\n",
    "plt.ylabel(r\"Q($w$)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated, GD reaches optima within several iterations, while SGD may appear less stable and requires more time to converge. Usually larger models demonstrate larger fluctuations for loss function values during the convergence process of stochastic gradient-based methods. In practice, SGD step size may be adjusted to achieve better convergence speed and there are several methods which implement adaptive gradient descent step size: AdaGrad, Adam, RMSProp etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Instability analysis\n",
    "Using the new technique, let's analyse the linear regression behavior in case of multicollinear features.\n",
    "\n",
    "In case of (multi-)collinear features the solution is *unstable*. Let's take a look at the *condition number* of our matrix:\n",
    "$$\\kappa(a) = \\frac{\\sigma_\\max(A)}{\\sigma_\\min(A)}$$\n",
    "where $\\sigma _{\\max }(A)$ and $\\sigma _{\\min }(A)$ are maximal and minimal singular values of matrix $A$ respectively. Hence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:49:35.655147Z",
     "start_time": "2021-03-28T09:49:35.650813Z"
    }
   },
   "outputs": [],
   "source": [
    "def w_by_grad(\n",
    "    X: np.ndarray, Y: np.ndarray, num_steps: int, w_0: np.ndarray, lr: float\n",
    ") -> np.ndarray:\n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        w -= 2 * lr * np.dot(X.T, np.dot(X, w) - Y) / Y.shape[0]\n",
    "    return w\n",
    "\n",
    "\n",
    "def w_by_stoch_grad(\n",
    "    X: np.ndarray, Y: np.ndarray, num_steps: int, w_0: np.ndarray, lr: float, n_objects: int\n",
    ") -> np.ndarray:\n",
    "    w = w_0.copy()\n",
    "    lr_0 = 0.45\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        lr = lr_0 / ((i + 1) ** 0.51)\n",
    "        sample = np.random.randint(n_objects, size=batch_size)\n",
    "        w -= 2 * lr * np.dot(X[sample].T, np.dot(X[sample], w) - Y[sample]) / Y.shape[0]\n",
    "    return w\n",
    "\n",
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> np.float64:\n",
    "    return np.linalg.norm(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:49:36.123079Z",
     "start_time": "2021-03-28T09:49:36.120631Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "sgd_lr = 0.1\n",
    "num_steps = 250\n",
    "noise_eps_seq = np.logspace(-2, -6, 20)\n",
    "\n",
    "w_0 = np.random.uniform(-2, 2, (n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:49:44.894895Z",
     "start_time": "2021-03-28T09:49:37.033591Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_numbers = []\n",
    "vector_norms_list = []\n",
    "rmse_list = []\n",
    "results_list = []\n",
    "for eps in noise_eps_seq:\n",
    "    local_condition_numbers = []\n",
    "    local_vector_norms_list = []\n",
    "    local_rmse_list = []\n",
    "    for i in range(50):\n",
    "        X[:, -1] = 2 * (X[:, -2] + np.random.uniform(-eps, eps, X[:, -2].shape))\n",
    "\n",
    "        a = np.linalg.eigvals(X.T.dot(X))\n",
    "        local_condition_numbers.append(a.max() / a.min())\n",
    "\n",
    "        w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "        w_star_grad = w_by_grad(X, Y, num_steps, w_0, lr)\n",
    "        w_star_sgd = w_by_stoch_grad(X, Y, num_steps, w_0, sgd_lr, n_objects)\n",
    "        local_vector_norms_list.append(\n",
    "            [\n",
    "                np.linalg.norm(w_star),\n",
    "                np.linalg.norm(w_star_grad),\n",
    "                np.linalg.norm(w_star_sgd),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        analytical_predict = X.dot(w_star)\n",
    "        grad_predict = X.dot(w_star_grad)\n",
    "        sgd_predict = X.dot(w_star_sgd)\n",
    "\n",
    "        local_rmse_list.append(\n",
    "            [\n",
    "                rmse(Y, analytical_predict),\n",
    "                rmse(Y, grad_predict),\n",
    "                rmse(Y, sgd_predict),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        results_list.append([w_star, w_star_grad, w_star_sgd])\n",
    "\n",
    "    condition_numbers.append([np.mean(local_condition_numbers), np.std(local_condition_numbers)])\n",
    "    vector_norms_list.append(\n",
    "        [\n",
    "            np.mean(np.array(local_vector_norms_list), axis=0),\n",
    "            np.std(np.array(local_vector_norms_list), axis=0),\n",
    "        ]\n",
    "    )\n",
    "    rmse_list.append(np.mean(np.array(local_rmse_list), axis=0))\n",
    "\n",
    "condition_numbers = np.array(condition_numbers)\n",
    "vector_norms_list = np.array(vector_norms_list)\n",
    "rmse_list = np.array(rmse_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a close look on the collected vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:50:43.594061Z",
     "start_time": "2021-03-28T09:50:43.589340Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:50:45.520370Z",
     "start_time": "2021-03-28T09:50:45.005121Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualise(\n",
    "    np.log(condition_numbers[:, 0]),\n",
    "    np.log(condition_numbers[:, 1]),\n",
    "    noise_eps_seq,\n",
    "    title=\"Condition number in log scale by noise level\",\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:50:48.282664Z",
     "start_time": "2021-03-28T09:50:47.789010Z"
    }
   },
   "outputs": [],
   "source": [
    "visualise(\n",
    "    np.log(vector_norms_list[:, 0, 0]),\n",
    "    np.log(vector_norms_list[:, 1, 0]),\n",
    "    noise_eps_seq,\n",
    "    title=\"Vector norm in log scale for analytical solution by noise level\",\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:50:49.473085Z",
     "start_time": "2021-03-28T09:50:48.948731Z"
    }
   },
   "outputs": [],
   "source": [
    "visualise(\n",
    "    vector_norms_list[:, 0, 1],\n",
    "    vector_norms_list[:, 1, 1],\n",
    "    noise_eps_seq,\n",
    "    title=\"Vector norm in original scale for gradient solution by noise level\",\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:50:50.526003Z",
     "start_time": "2021-03-28T09:50:50.018794Z"
    }
   },
   "outputs": [],
   "source": [
    "visualise(\n",
    "    vector_norms_list[:, 0, 2],\n",
    "    vector_norms_list[:, 1, 2],\n",
    "    noise_eps_seq,\n",
    "    title=\"Vector norm in original scale for sgd solution by noise level\",\n",
    "    greater_than_zero=True,\n",
    "    log_scale=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression out of the box\n",
    "\n",
    "Finally, let's take a brief look at implemented versions of Linear Regression from sklearn. The main classes are:\n",
    "\n",
    "* [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) — classical linear regression (*actially, it is just `scipy.linalg.lstsq` wrapped with sklearn `Predictor` class) __analytical__ solver.\n",
    "* [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) — Linear regression with L1 regularization.\n",
    "* [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) — Linear regression with L2 regularization.\n",
    "\n",
    "To minimize any other error function you are free to use [SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) (or wait for a week and we will get the great *PyTorch* automatic differentiation engine).\n",
    "\n",
    "Let's compare the speed of analytical and gradient solutions from the sklearn realizations.\n",
    "\n",
    "IPython magic `%%time` wrapper will be used.\n",
    "\n",
    "To measure the quality $R^2$ score will be used. It compares our model (`a`) with one always predicting mean `y`:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - a(x_i))^2}{\\sum_i (y_i - \\overline{y}_i)^2}$$\n",
    "\n",
    "`LinearRegression` vs. `Ridge`: __Fight!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mortal Combat picutre](https://raw.githubusercontent.com/girafe-ai/ml-mipt/784d834b01893b984cbad8c43f3d053bd5341573/week0_02_linear_reg/img/mortal_combat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:50:55.089728Z",
     "start_time": "2021-03-28T09:50:54.270170Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:50:56.298288Z",
     "start_time": "2021-03-28T09:50:55.555271Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 700\n",
    "n_objects = 100000\n",
    "num_steps = 150\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, (n_features, 1))\n",
    "\n",
    "X = np.random.uniform(-100, 100, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 10, (n_objects, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:51:01.223802Z",
     "start_time": "2021-03-28T09:50:56.770640Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "print(f\"R2: {lr.score(X, Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:51:36.034611Z",
     "start_time": "2021-03-28T09:51:35.142207Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = Ridge(alpha=1.0, solver=\"sparse_cg\")\n",
    "lr.fit(X, Y)\n",
    "print(f\"R2: {lr.score(X, Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T09:51:44.432937Z",
     "start_time": "2021-03-28T09:51:42.707731Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = Lasso(alpha=1.0)\n",
    "lr.fit(X, Y)\n",
    "print(f\"R2: {lr.score(X, Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own neat version of Linear Regression\n",
    "\n",
    "Let's use `sklearn`'s standard interfaces to implement sealed version of our version of Linear Regression using SGD\n",
    "\n",
    "First we need to inherit base classes, then implement 3 main stages of regressor life as methods:\n",
    "* hyperparameter initialization - constructor\n",
    "* parameters training on known objects - fit method\n",
    "* target estimation for unknown objects - predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRergessionSGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"LinearRergession with L2 regularization and SGD optimizer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        C: float = 1.0,\n",
    "        batch_size: int = 25,\n",
    "        lr: float = 1e-2,\n",
    "        num_steps: int = 200,\n",
    "    ) -> None:\n",
    "        self.C = C\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        w = np.random.randn(X.shape[1])[:, None]\n",
    "        n_objects = len(X)\n",
    "\n",
    "        # this is just copied from above\n",
    "        for i in range(self.num_steps):\n",
    "            sample_indices = np.random.randint(n_objects, size=self.batch_size)\n",
    "            w -= (\n",
    "                2\n",
    "                * self.lr\n",
    "                * np.dot(X[sample_indices].T, np.dot(X[sample_indices], w) - Y[sample_indices])\n",
    "                / self.batch_size\n",
    "            )\n",
    "\n",
    "        self.w = w\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate dataset with differently scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 700\n",
    "n_objects = 100000\n",
    "num_steps = 150\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, (n_features, 1))\n",
    "\n",
    "X = np.random.uniform(-100, 100, (n_objects, n_features)) * np.arange(n_features)\n",
    "Y = X.dot(w_true) + np.random.normal(0, 10, (n_objects, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and split it to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_lr = LinearRergessionSGD().fit(x_train, y_train)\n",
    "print(f\"R2: {own_lr.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOOOOOOOOPS!!!\n",
    "\n",
    "Something went wrong. What could it be?\n",
    "\n",
    "During our SGD we've encountered too big values to store in float.\n",
    "\n",
    "That leads us to feature normalization.\n",
    "Lest's scale features: just subtract mean from each feature and divide by sample variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_scaled = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_lr = LinearRergessionSGD().fit(x_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for test we need to scale test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled = scaler.transform(x_test)\n",
    "print(f\"R2: {own_lr.score(x_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! we didn't implement no `score` method. But `sklearn`'s base class provide us it aleready implemented.\n",
    "\n",
    "You note that scaling data before prediction is not a big pleasure. So we could get rid of this bulkiness with pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LinearRergessionSGD())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(x_train, y_train)\n",
    "print(f\"R2: {pipe.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As if we don't have any complex assembly behind pipeline interface!\n",
    "\n",
    "And no data leak guaranteed as a gift!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
