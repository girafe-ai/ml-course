{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ml-mipt course](https://github.com/girafe-ai/ml-mipt) by girafe-ai <a class=\"tocSkip\">\n",
    "\n",
    "# Seminar 3: Intro to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes today:\n",
    "\n",
    "- Introduction to PyTorch\n",
    "- Automatic gradient computation\n",
    "- Logistic regression (it's a neural network, actually ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://pytorch.org/tutorials/_static/pytorch-logo-dark.svg)\n",
    "\n",
    "[Official website](http://pytorch.org/)\n",
    "\n",
    "This notebook will teach you to use pytorch low-level core.\n",
    "\n",
    "Pytorch feels differently than other frameworks (like tensorflow/theano) on almost every level. TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual minibatches of data.  In pytorch, __there's only one world__: all tensors have a numeric value.\n",
    "\n",
    "You compute outputs on the fly without pre-declaring anything. The code looks exactly as in pure numpy with one exception: pytorch computes gradients for you. And can run stuff on GPU. And has a number of pre-implemented building blocks for your neural nets. [And a few more things.](https://medium.com/towards-data-science/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:03:32.228111Z",
     "start_time": "2022-02-01T22:03:32.225185Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:03:36.216389Z",
     "start_time": "2022-02-01T22:03:32.441915Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy vs Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:03:38.633241Z",
     "start_time": "2022-02-01T22:03:38.627308Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "print(\"X :\\n%s\\n\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print(\"add 5 :\\n%s\\n\" % (x + 5))\n",
    "print(\"X*X^T  :\\n%s\\n\" % np.dot(x, x.T))\n",
    "print(\"mean over cols :\\n%s\\n\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\\n\" % (np.cumsum(x, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:03:39.428178Z",
     "start_time": "2022-02-01T22:03:39.412312Z"
    }
   },
   "outputs": [],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32)  # or torch.arange(0,16).view(4,4)\n",
    "\n",
    "print(\"X :\\n%s\" % x)\n",
    "print(\"X.shape : %s\\n\" % (x.shape,))\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n%s\" % torch.matmul(x, x.transpose(1, 0)))  # short: x.mm(x.t())\n",
    "print(\"mean over cols :\\n%s\" % torch.mean(x, dim=-1))\n",
    "print(\"cumsum of cols :\\n%s\" % torch.cumsum(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy and Pytorch: comparison\n",
    "\n",
    "As you can notice, pytorch allows you to hack stuff much the same way you did with numpy. This means that you can _see the numeric value of any tensor at any moment of time_. Debugging such code can be done with by printing tensors or using any debug tool you want (e.g. [gdb](https://wiki.python.org/moin/DebuggingWithGdb)).\n",
    "\n",
    "You could also notice the a few new method names and a different API. So no, there's no compatibility with numpy [yet](https://github.com/pytorch/pytorch/issues/2228) and yes, you'll have to memorize all the names again. Get excited!\n",
    "\n",
    "![img](https://i.redd.it/6xzsaw1lrd211.jpg)\n",
    "\n",
    "For example,\n",
    "\n",
    "- If something takes a list/tuple of axes in numpy, you can expect it to take \\*args in pytorch\n",
    "- `x.reshape([1,2,8]) -> x.view(1,2,8)`\n",
    "- You should swap _axis_ for _dim_ in operations like mean or cumsum\n",
    "- `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
    "- most mathematical operations are the same, but types an shaping is different\n",
    "- `x.astype('int64') -> x.type(torch.LongTensor)`\n",
    "\n",
    "To help you acclimatize, there's a [table](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users) covering most new things. There's also a neat [documentation page](http://pytorch.org/docs/master/).\n",
    "\n",
    "Finally, if you're stuck with a technical problem, we recommend searching [pytorch forumns](https://discuss.pytorch.org/). Or just googling, which usually works just as efficiently.\n",
    "\n",
    "If you feel like you almost give up, remember two things: __GPU__ and __free gradients__. Besides you can always jump back to numpy with x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup: trigonometric knotwork\n",
    "\n",
    "_inspired by [this post](https://www.quora.com/What-are-the-most-interesting-equation-plots)_\n",
    "\n",
    "There are some simple mathematical functions with cool plots. For one, consider this:\n",
    "\n",
    "$$ x(t) = t - 1.5 * cos( 15 t) $$\n",
    "\n",
    "$$ y(t) = t - 1.5 * sin( 16 t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:03:43.034184Z",
     "start_time": "2022-02-01T22:03:42.313908Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:04:13.818622Z",
     "start_time": "2022-02-01T22:04:13.708299Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.linspace(-10, 10, steps=10000)\n",
    "\n",
    "# compute x(t) and y(t) as defined above\n",
    "x = t - 1.5 * torch.cos(4.7 * t)\n",
    "y = t - 2.5 * torch.sin(1.6 * t)\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you're done early, try adjusting the formula and seing how  it affects the function\n",
    "\n",
    "## Automatic gradients\n",
    "\n",
    "Any self-respecting DL framework must do your backprop for you. Torch handles this with the `autograd` module.\n",
    "\n",
    "The general pipeline looks like this:\n",
    "\n",
    "- When creating a tensor, you mark it as `requires_grad`:\n",
    "  - __`torch.zeros(5, requires_grad=True)`__\n",
    "  - torch.tensor(np.arange(5), dtype=torch.float32, requires_grad=True)\n",
    "- Define some differentiable `loss = arbitrary_function(a)`\n",
    "- Call `loss.backward()`\n",
    "- Gradients are now available as `a.grads`\n",
    "\n",
    "__Here's an example:__ let's fit a linear regression on Boston house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:05:11.803726Z",
     "start_time": "2022-02-01T22:05:11.692984Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "plt.scatter(boston.data[:, -1], boston.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:05:12.592462Z",
     "start_time": "2022-02-01T22:05:12.588842Z"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:, -1] / 10, dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:05:13.536005Z",
     "start_time": "2022-02-01T22:05:13.533110Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"dL/dw = {}\\n\".format(w.grad))\n",
    "print(\"dL/db = {}\\n\".format(b.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:05:14.351813Z",
     "start_time": "2022-02-01T22:05:14.348310Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = w * x + b\n",
    "loss = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "# propagete gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are now stored in `.grad` of those variables that require them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:05:16.368466Z",
     "start_time": "2022-02-01T22:05:16.364080Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"dL/dw = {}\\n\".format(w.grad))\n",
    "print(\"dL/db = {}\\n\".format(b.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute gradient from multiple losses, the gradients will add up at variables, therefore it's useful to __zero the gradients__ between iteratons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:05:50.217492Z",
     "start_time": "2022-02-01T22:05:50.214932Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:06:03.043989Z",
     "start_time": "2022-02-01T22:05:52.805429Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(500):\n",
    "    # forward\n",
    "    y_pred = w * x + b\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    w.data -= 0.05 * w.grad.data\n",
    "    b.data -= 0.05 * b.grad.data\n",
    "\n",
    "    # zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.scatter(x.data.numpy(), y_pred.data.numpy(), color=\"orange\", linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy())\n",
    "        if loss.data.numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:06:34.507727Z",
     "start_time": "2022-02-01T22:06:34.413730Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quest__: try implementing and writing some nonlinear regression. You can try quadratic features or some trigonometry, or a simple neural network. The only difference is that now you have more variables and a more complicated `y_pred`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember!**\n",
    "![img](https://media.giphy.com/media/3o751UMCYtSrRAFRFC/giphy.gif)\n",
    "\n",
    "When dealing with more complex stuff like neural network, it's best if you use tensors the way samurai uses his sword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level pytorch\n",
    "\n",
    "So far we've been dealing with low-level torch API. While it's absolutely vital for any custom losses or layers, building large neura nets in it is a bit clumsy.\n",
    "\n",
    "Luckily, there's also a high-level torch interface with a pre-defined layers, activations and training algorithms.\n",
    "\n",
    "We'll cover them as we go through a simple image recognition problem: classifying letters into __\"A\"__ vs __\"B\"__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:11:20.818280Z",
     "start_time": "2022-02-01T22:11:20.678931Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/girafe-ai/ml-mipt/397dd24b1a4bfb0cf4353cb3d77cd4cf53d3e46b/week0_03_linear_classification/notmnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:11:33.089384Z",
     "start_time": "2022-02-01T22:11:27.072718Z"
    }
   },
   "outputs": [],
   "source": [
    "from notmnist import load_notmnist\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_notmnist(letters=\"AB\")\n",
    "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])\n",
    "\n",
    "print(f\"Train size = {len(X_train)}\\nTest_size = {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:12:38.464433Z",
     "start_time": "2022-02-01T22:12:38.254596Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.imshow(X_train[i].reshape([28, 28]))\n",
    "    plt.title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with layers. The main abstraction here is __`torch.nn.Module`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T18:00:30.822587Z",
     "start_time": "2021-11-01T18:00:30.820325Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "print(nn.Module.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a vast library of popular layers and architectures already built for ya'.\n",
    "\n",
    "This is a binary classification problem, so we'll train a __Logistic Regression with sigmoid__.\n",
    "\n",
    "$$P(y_i | X_i) = \\sigma(W \\cdot X_i + b) ={ 1 \\over {1+e^{- [W \\cdot X_i + b]}} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T18:03:09.306788Z",
     "start_time": "2021-11-01T18:03:09.304118Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a network that stacks layers on top of each other\n",
    "model = nn.Sequential()\n",
    "\n",
    "# add first \"dense\" layer with 784 input units and 1 output unit.\n",
    "model.add_module(\"l1\", nn.Linear(784, 1))\n",
    "\n",
    "# add softmax activation for probabilities. Normalize over axis 1\n",
    "# note: layer names must be unique\n",
    "model.add_module(\"l2\", nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `add_module` is not recommended way to use Sequential (better to pass Modules to constructor, see usage below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T18:03:10.303558Z",
     "start_time": "2021-11-01T18:03:10.300006Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Weight shapes:\", [w.shape for w in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T18:03:35.295298Z",
     "start_time": "2021-11-01T18:03:35.292624Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Weight shapes:\", [(name, w.shape) for name, w in model.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:29:11.260623Z",
     "start_time": "2021-07-19T17:29:11.256376Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dummy data with 3 samples and 784 features\n",
    "x = torch.tensor(X_train[:3], dtype=torch.float32)\n",
    "y = torch.tensor(y_train[:3], dtype=torch.float32)\n",
    "\n",
    "# compute outputs given inputs, both are variables\n",
    "y_predicted = model(x)  # __call__ = forward\n",
    "\n",
    "y_predicted  # display what we've got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a loss function for our model.\n",
    "\n",
    "The natural choice is to use binary crossentropy (aka logloss, negative llh):\n",
    "\n",
    "$$L = {1 \\over N} \\underset{X_i,y_i} \\sum - [  y_i \\cdot log P(y_i | X_i) \\ + (1-y_i) \\cdot log (1-P(y_i | X_i)) ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:29:12.033665Z",
     "start_time": "2021-07-19T17:29:12.030965Z"
    }
   },
   "outputs": [],
   "source": [
    "y.shape, y_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:29:48.916645Z",
     "start_time": "2021-07-19T17:29:48.913029Z"
    }
   },
   "outputs": [],
   "source": [
    "crossentropy = F.binary_cross_entropy(y_predicted[:, 0], y, reduction=\"none\")\n",
    "\n",
    "loss = torch.mean(crossentropy, dim=0, keepdims=True)\n",
    "\n",
    "\n",
    "assert tuple(crossentropy.size()) == (3,), \"Crossentropy must be a vector with element per sample\"\n",
    "assert tuple(loss.size()) == (1,), \"Loss must be scalar. Did you forget the mean/sum?\"\n",
    "assert loss.data.numpy()[0] > 0, \"Crossentropy must non-negative, zero only for perfect prediction\"\n",
    "assert loss.data.numpy()[0] <= np.log(\n",
    "    3\n",
    "), \"Loss is too large even for untrained model. Please double-check it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ you can also find many such functions in `torch.nn.functional`, just type __`F.<tab>`__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Torch optimizers__\n",
    "\n",
    "When we trained Linear Regression above, we had to manually .zero\\_() gradients on both our variables. Imagine that code for a 50-layer network.\n",
    "\n",
    "Again, to keep it from getting dirty, there's `torch.optim` module with pre-implemented algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:32:25.990711Z",
     "start_time": "2021-07-19T17:32:25.987449Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# here's how it's used:\n",
    "loss.backward()  # add new gradients\n",
    "opt.step()  # change weights\n",
    "opt.zero_grad()  # clear gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:32:28.604058Z",
     "start_time": "2021-07-19T17:32:28.602258Z"
    }
   },
   "outputs": [],
   "source": [
    "# dispose of old variables to avoid bugs later\n",
    "del x, y, y_predicted, loss, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T18:10:25.222728Z",
     "start_time": "2021-11-01T18:10:25.219268Z"
    }
   },
   "outputs": [],
   "source": [
    "# create network again just in case\n",
    "model = nn.Sequential(nn.Linear(784, 1), nn.Sigmoid())\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:39:20.274076Z",
     "start_time": "2021-07-19T17:39:20.195995Z"
    }
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "for i in range(100):\n",
    "    # sample 256 random images\n",
    "    ix = np.random.randint(0, len(X_train), 256)\n",
    "    x_batch = torch.tensor(X_train[ix], dtype=torch.float32)\n",
    "    y_batch = torch.tensor(y_train[ix], dtype=torch.float32)\n",
    "\n",
    "    # predict probabilities\n",
    "    y_predicted = model(x_batch)[:, 0]\n",
    "\n",
    "    assert y_predicted.dim() == 1, \"did you forget to select first column with [:, 0]\"\n",
    "\n",
    "    # compute loss, just like before\n",
    "    crossentropy = F.binary_cross_entropy(y_predicted, y_batch, reduction=\"none\")\n",
    "\n",
    "    loss = torch.mean(crossentropy, dim=0, keepdims=True)\n",
    "\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # SGD step\n",
    "    opt.step()\n",
    "\n",
    "    # clear gradients\n",
    "    opt.zero_grad()\n",
    "\n",
    "    history.append(loss.item())\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"step #%i | mean loss = %.3f\" % (i, np.mean(history[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:39:35.603554Z",
     "start_time": "2021-07-19T17:39:35.485016Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Debugging tips:__\n",
    "\n",
    "- make sure your model predicts probabilities correctly. Just print them and see what's inside.\n",
    "- don't forget _minus_ sign in the loss function! It's a mistake 99% ppl do at some point.\n",
    "- make sure you zero-out gradients after each step. Srsly:)\n",
    "- In general, pytorch's error messages are quite helpful, read 'em before you google 'em.\n",
    "- if you see nan/inf, print what happens at each iteration to find our where exactly it occurs.\n",
    "  - If loss goes down and then turns nan midway through, try smaller learning rate. (Our current loss formula is unstable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's see how our model performs on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T17:39:45.776432Z",
     "start_time": "2021-07-19T17:39:45.773204Z"
    }
   },
   "outputs": [],
   "source": [
    "# use your model to predict classes (0 or 1) for all test samples\n",
    "predicted_y_test = model(torch.tensor(X_test))[:, 0]\n",
    "predicted_y_test = np.array(predicted_y_test > 0.5)\n",
    "\n",
    "accuracy = np.mean(predicted_y_test == y_test)\n",
    "\n",
    "print(\"Test accuracy: %.5f\" % accuracy)\n",
    "assert accuracy > 0.95, \"try training longer\"\n",
    "\n",
    "print(\"Great job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus area:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The game of life (0.4 points)\n",
    "\n",
    "Now it's time for you to make something more challenging. We'll implement Conway's [Game of Life](http://web.stanford.edu/~cdebs/GameOfLife/) in _pure pytorch_.\n",
    "\n",
    "While this is still a toy task, implementing game of life this way has one cool benefit: \\_\\_you'll be able to run it on GPU! \\_\\_ Indeed, what could be a better use of your gpu than simulating game of life on 1M/1M grids?\n",
    "\n",
    "![img](https://cdn.tutsplus.com/gamedev/authors/legacy/Stephane%20Beniak/2012/09/11/Preview_Image.png)\n",
    "If you've skipped the url above out of sloth, here's the game of life:\n",
    "\n",
    "- You have a 2D grid of cells, where each cell is \"alive\"(1) or \"dead\"(0)\n",
    "- Any living cell that has 2 or 3 neighbors survives, else it dies \\[0,1 or 4+ neighbors\\]\n",
    "- Any cell with exactly 3 neighbors becomes alive (if it was dead)\n",
    "\n",
    "For this task, you are given a reference numpy implementation that you must convert to pytorch.\n",
    "_\\[numpy code inspired by: https://github.com/rougier/numpy-100\\]_\n",
    "\n",
    "__Note:__ You can find convolution in `torch.nn.functional.conv2d(Z,filters)`. Note that it has a different input format.\n",
    "\n",
    "__Note 2:__ From the mathematical standpoint, pytorch convolution is actually cross-correlation. Those two are very similar operations. More info: [video tutorial](https://www.youtube.com/watch?v=C3EEy8adxvc), [scipy functions review](http://programmerz.ru/questions/26903/2d-convolution-in-python-similar-to-matlabs-conv2-question), [stack overflow source](https://stackoverflow.com/questions/31139977/comparing-matlabs-conv2-with-scipys-convolve2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate2d\n",
    "\n",
    "\n",
    "def np_update(Z):\n",
    "    # Count neighbours with convolution\n",
    "    filters = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "\n",
    "    N = correlate2d(Z, filters, mode=\"same\")\n",
    "\n",
    "    # Apply rules\n",
    "    birth = (N == 3) & (Z == 0)\n",
    "    survive = ((N == 2) | (N == 3)) & (Z == 1)\n",
    "\n",
    "    Z[:] = birth | survive\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_update(Z):\n",
    "    \"\"\"\n",
    "    Implement an update function that does to Z exactly the same as np_update.\n",
    "    :param Z: torch.FloatTensor of shape [height,width] containing 0s(dead) an 1s(alive)\n",
    "    :returns: torch.FloatTensor Z after updates.\n",
    "\n",
    "    You can opt to create new tensor or change Z inplace.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial frame\n",
    "Z_numpy = np.random.choice([0, 1], p=(0.5, 0.5), size=(100, 100))\n",
    "Z = torch.from_numpy(Z_numpy).type(torch.FloatTensor)\n",
    "\n",
    "# your debug polygon :)\n",
    "Z_new = torch_update(Z.clone())\n",
    "\n",
    "# tests\n",
    "Z_reference = np_update(Z_numpy.copy())\n",
    "assert np.all(\n",
    "    Z_new.numpy() == Z_reference\n",
    "), \"your pytorch implementation doesn't match np_update. Look into Z and np_update(ZZ) to investigate.\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "\n",
    "# initialize game field\n",
    "Z = np.random.choice([0, 1], size=(100, 100))\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    # update\n",
    "    Z = torch_update(Z)\n",
    "\n",
    "    # re-draw image\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(), cmap=\"gray\")\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some fun setups for your amusement\n",
    "\n",
    "# parallel stripes\n",
    "Z = np.arange(100) % 2 + np.zeros([100, 100])\n",
    "# with a small imperfection\n",
    "Z[48:52, 50] = 1\n",
    "\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "    Z = torch_update(Z)\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(), cmap=\"gray\")\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More fun with Game of Life: [video](https://www.youtube.com/watch?v=C2vgICfQawE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about pytorch:\n",
    "\n",
    "- Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
    "- More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
    "- Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
    "- And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cridentials\n",
    "\n",
    "special thanks to YSDA team for provided materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-mipt]",
   "language": "python",
   "name": "conda-env-ml-mipt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
