{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: this notebook origin (shared under MIT license) belongs to [ML course at ICL](https://github.com/yandexdataschool/MLatImperial2020) held by Yandex School of Data Analysis. Special thanks to the course team for making it available online.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecjpqpMok8eh"
   },
   "source": [
    "## week0_05: Bias-Variance decomposition example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPtlnBa24uv8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPl1NddU45kV"
   },
   "outputs": [],
   "source": [
    "def true_dep(x):\n",
    "    return np.cos((x - 0.2)**2) + 0.2 / (1 + 50 * (x - 0.3)**2)\n",
    "\n",
    "x_true = np.linspace(0, 1, 100)\n",
    "y_true = true_dep(x_true)\n",
    "\n",
    "def generate_n_datasets(num_datasets, dataset_length, noise_power=0.02):\n",
    "    shape = (num_datasets, dataset_length, 1)\n",
    "    x = np.random.uniform(size=shape)\n",
    "    y = true_dep(x) + np.random.normal(scale=noise_power, size=shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMq2dk0b7KAN"
   },
   "outputs": [],
   "source": [
    "x, y = generate_n_datasets(1, 30)\n",
    "plt.scatter(x.squeeze(), y.squeeze(), s=20, c='orange')\n",
    "plt.plot(x_true, y_true, c='c', linewidth=1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmhwKmCb9IGM"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rWsmqAu8NDm"
   },
   "outputs": [],
   "source": [
    "def calc_bias2_variance(model, datasets_X, datasets_y):\n",
    "    preds = []\n",
    "    for X, y in tqdm(zip(datasets_X, datasets_y), total=len(datasets_X)):\n",
    "        m = deepcopy(model)\n",
    "        m.fit(X, y)\n",
    "        preds.append(m.predict(x_true[:,np.newaxis]).squeeze())\n",
    "    preds = np.array(preds)\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "    bias2 = (y_true - mean_pred)**2\n",
    "    variance = ((preds - mean_pred[np.newaxis,...])**2).mean(axis=0)\n",
    "\n",
    "    return bias2, variance, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mO6xg7VB-fg-"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are using the `Pipeline` once again to both preprocess the feature space and fit the model at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zerleLK-pvJ"
   },
   "outputs": [],
   "source": [
    "MAX_POWER = 6\n",
    "powers = np.arange(1, MAX_POWER+1)\n",
    "\n",
    "bias2, variance, preds = [], [], []\n",
    "for p in powers:\n",
    "    model = Pipeline([\n",
    "      ('poly', PolynomialFeatures(degree=p)),\n",
    "      ('linear', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    b2, v, p = calc_bias2_variance(model, *generate_n_datasets(1000, 20))\n",
    "    bias2.append(b2)\n",
    "    variance.append(v)\n",
    "    preds.append(p)\n",
    "\n",
    "bias2 = np.array(bias2)\n",
    "variance = np.array(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvTGdDn6hJ5n"
   },
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = int(np.ceil(len(powers) / ncols))\n",
    "\n",
    "plt.figure(figsize=(18, 3.5 * nrows))\n",
    "\n",
    "yrange = y_true.max() - y_true.min()\n",
    "\n",
    "for i, (pred, pow) in tqdm(enumerate(zip(preds, powers), 1)):\n",
    "    plt.subplot(nrows, ncols, i)\n",
    "    for p in pred[np.random.choice(len(pred), size=200, replace=False)]:\n",
    "        plt.plot(x_true, p, linewidth=0.05, c='b');\n",
    "    plt.plot(x_true, y_true, linewidth=3, label='Truth', c='r')\n",
    "    plt.ylim(y_true.min() - 0.5 * yrange, y_true.max() + 0.5 * yrange)\n",
    "    plt.title('power = {}'.format(pow))\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCYG8jE6fxf2"
   },
   "outputs": [],
   "source": [
    "plt.plot(powers, bias2.mean(axis=1), label='bias^2')\n",
    "plt.plot(powers, variance.mean(axis=1), label='variance')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('power');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Runge's phenomenon\n",
    "Speaking about polinomial features, going to higher degrees does not always improve accuracy. This effect was discovered by Carl David Tolm√© Runge (1901) when exploring the behavior of errors when using polynomial interpolation to approximate certain functions. Refer to the [wikipedia page](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe this phenomenon, let's run the exact same code as above, but with increased maximum power of the polinome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POWER = 8\n",
    "powers = np.arange(1, MAX_POWER+1)\n",
    "\n",
    "bias2, variance, preds = [], [], []\n",
    "for p in powers:\n",
    "    model = Pipeline([\n",
    "      ('poly', PolynomialFeatures(degree=p)),\n",
    "      ('linear', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    b2, v, p = calc_bias2_variance(model, *generate_n_datasets(1000, 20))\n",
    "    bias2.append(b2)\n",
    "    variance.append(v)\n",
    "    preds.append(p)\n",
    "\n",
    "bias2 = np.array(bias2)\n",
    "variance = np.array(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = int(np.ceil(len(powers) / ncols))\n",
    "\n",
    "plt.figure(figsize=(18, 3.5 * nrows))\n",
    "\n",
    "yrange = y_true.max() - y_true.min()\n",
    "\n",
    "for i, (pred, pow) in tqdm(enumerate(zip(preds, powers), 1)):\n",
    "    plt.subplot(nrows, ncols, i)\n",
    "    for p in pred[np.random.choice(len(pred), size=200, replace=False)]:\n",
    "        plt.plot(x_true, p, linewidth=0.05, c='b');\n",
    "    plt.plot(x_true, y_true, linewidth=3, label='Truth', c='r')\n",
    "    plt.ylim(y_true.min() - 0.5 * yrange, y_true.max() + 0.5 * yrange)\n",
    "    plt.title('power = {}'.format(pow))\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(powers, bias2.mean(axis=1), label='bias^2')\n",
    "plt.plot(powers, variance.mean(axis=1), label='variance')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('power');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the bias is increasing after the power equal to 6 as well, but is is caused by the oscillations on the edges of the original function (we are trying to interpolate). Refer to the [wiki page](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) for more info."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "BiasVariance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
