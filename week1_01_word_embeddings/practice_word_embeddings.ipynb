{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02ZfltAhzOgl"
   },
   "source": [
    "# Practice: Dealing with Word Embeddings\n",
    "_Reference: embeddings visualization is based on the notebook by [YSDA NLP course](https://github.com/yandexdataschool/nlp_course)_\n",
    "\n",
    "Today we gonna play with word embeddings: train our own little embedding, load one from `gensim` model zoo and use it to visualize text corpora.\n",
    "\n",
    "This whole thing is gonna happen on top of Quora questions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_8oogMdjxG-"
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EB3OIatxi3-"
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gn2OmjnWzOgs"
   },
   "outputs": [],
   "source": [
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw\n",
    "!wget -nc -q \"https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\" -O quora.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQpRIS02zOgu"
   },
   "outputs": [],
   "source": [
    "data = list(open(\"quora.txt\"))\n",
    "data[287191]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDkQ1gpTxuZM"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "A typical first step for an NLP task is to split raw data into words.\n",
    "The text we're working with is in raw format: with all the punctuation and smiles attached to some words, so a simple `str.split` won't do.\n",
    "\n",
    "Let's use `nltk` - a library that handles many NLP tasks like tokenization, stemming or part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyYXKgoPzOgv"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(data[287191]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDJpVwiRzOgw"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Lowercase everything and extract tokens with tokenizer.\n",
    "# Hint: data_tok should be a list of lists of tokens for each line in data.\n",
    "# data_tok = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy3cDzCHzOgx"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "assert all(\n",
    "    isinstance(row, (list, tuple)) for row in data_tok\n",
    "), \"each line should be converted into a list of tokens\"\n",
    "assert all(\n",
    "    all(isinstance(tok, str) for tok in row) for row in data_tok\n",
    "), \"each token should be a string\"\n",
    "\n",
    "is_latin = lambda tok: all(symbol in string.ascii_letters for symbol in tok)\n",
    "assert all(\n",
    "    all(tok.islower() or not is_latin(tok) for tok in row) for row in data_tok\n",
    "), \"make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1uSu2W47RTn"
   },
   "source": [
    "## Training word vectors\n",
    "\n",
    "There's more than one way to train word embeddings. There's Word2Vec and GloVe with different objective functions. Then there's fasttext that uses character-level models to train word embeddings. \n",
    "\n",
    "The choice is huge, so let's start someplace small: `gensim` is another NLP library that features many vector-based models incuding word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8I2WYCsHzOgy"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "model = Word2Vec(\n",
    "    data_tok,\n",
    "    size=32,  # embedding vector size\n",
    "    min_count=5,  # consider words that occured at least 5 times\n",
    "    window=5,  # define context as a 5-word window around the target word\n",
    ").wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkGlLkWU8YcF"
   },
   "source": [
    "Now we can get word vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYP3GodTzOgz"
   },
   "outputs": [],
   "source": [
    "model.get_vector(\"anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31bmyNBv8inB"
   },
   "source": [
    "Or even query similar words! Go play with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UdUs-pSzOgz"
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMEp5k4Z829p"
   },
   "source": [
    "## Using pretrained model\n",
    "\n",
    "Took it a while, huh? Now imagine training life-sized (100~300) word embeddings on gigabytes of text: wikipedia articles or twitter posts. \n",
    "\n",
    "Thankfully, nowadays you can get a pre-trained word embedding model in 2 lines of code (no sms required, promise).\n",
    "\n",
    "We will download pretrained [GloVe](https://nlp.stanford.edu/projects/glove/) model. The `gensim` library provides several different models (difference is mostly in training datasets and embedding sizes). For a full list of available models please refer to [Gensim-data readme](https://github.com/RaRe-Technologies/gensim-data#models). By default we use a relatively small model for the purposes of loading time, however feel free to experiment with models and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZAzPP7JzOg0"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xaWqCUMWu3H"
   },
   "source": [
    "Now that we've loaded our model, we, once again, can use it to extract word embeddings and query most similar words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDfDSpLYyr7c"
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwEadlujXcR2"
   },
   "source": [
    "When quering for the most similar words, we can provide our model not only with positive but also with negative examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHIbFTYuTC-s"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=\"date\", negative=\"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBTFKwJiX6mx"
   },
   "source": [
    "Better yet, positive and negative examples aren't restricted to a single word! Any (or both) of parameters can accept lists of words! This being said, as we use a *small-ish* model, query process can be somewhat fragile. This is especially true when feeding it with multiple words at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__4vq1gFXVmH"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"date\", \"meeting\"], negative=\"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiEG3aEVzOg1"
   },
   "source": [
    "## Visualizing word vectors\n",
    "\n",
    "One way to see if our vectors are any good is to plot them. Thing is, those vectors are in 25-dimensional space and we humans are more used to 2-3d.\n",
    "\n",
    "Luckily, we machine learners know about __dimensionality reduction__ methods.\n",
    "\n",
    "Let's use that to plot 1000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccvEwkWjzOg1"
   },
   "outputs": [],
   "source": [
    "words = sorted(model.vocab.keys(), key=lambda word: model.vocab[word].count, reverse=True)[:1000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeVQ3EjOzOg2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Compute embeddings for each word and store them in a numpy array.\n",
    "# word_vectors = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-JvM_GHzOg3"
   },
   "outputs": [],
   "source": [
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (\n",
    "    len(words),\n",
    "    model.vector_size,\n",
    "), \"word_vectors should have shape (n_words, embedding_size)\"\n",
    "assert np.isfinite(word_vectors).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-xFaWYMzOg3"
   },
   "source": [
    "### Linear projection: PCA\n",
    "\n",
    "The simplest linear dimensionality reduction method is __P__rincipial __C__omponent __A__nalysis.\n",
    "\n",
    "In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">\n",
    "\n",
    "\n",
    "Under the hood, it attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
    "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwD4gbivzOg4"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Map word vectors onto 2d plane using PCA.\n",
    "# word_vectors_pca = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRKiP16NzOg4"
   },
   "outputs": [],
   "source": [
    "assert word_vectors_pca.shape == (len(words), 2), \"there must be a 2d vector for each word\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gEIsDA_zOg5"
   },
   "source": [
    "### Let's draw it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ_MGcR2dLc5"
   },
   "source": [
    "Hover a mouse over there and see if you can identify the clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAExDS10cVqH"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.scatter(x=word_vectors_pca[:, 0], y=word_vectors_pca[:, 1], hover_name=words)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM_dhTILzOg6"
   },
   "source": [
    "### Visualizing neighbors with UMAP\n",
    "PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n",
    "\n",
    "If we instead want to focus on keeping neighboring points near, we could use UMAP, which is itself an embedding method. Here you can read __[more on UMAP (ru)](https://habr.com/ru/company/newprolab/blog/350584/)__ and on __[t-SNE](https://distill.pub/2016/misread-tsne/)__, which is also an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMY4OExQzOg7"
   },
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "\n",
    "umap = UMAP(n_neighbors=5)\n",
    "word_vectors_umap = umap.fit_transform(word_vectors)\n",
    "\n",
    "fig = px.scatter(x=word_vectors_umap[:, 0], y=word_vectors_umap[:, 1], hover_name=words)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS3DNyVwzOg9"
   },
   "source": [
    "## Visualizing phrases\n",
    "\n",
    "Word embeddings can also be used to represent short phrases. The simplest way is to take __an average__ of vectors for all tokens in the phrase with some weights.\n",
    "\n",
    "This trick is useful to identify what data are you working with: find if there are any outliers, clusters or other artefacts.\n",
    "\n",
    "Let's try this new hammer on our data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5O9jJPpzOg-"
   },
   "outputs": [],
   "source": [
    "def get_phrase_embedding(phrase):\n",
    "    # YOUR CODE HERE\n",
    "    # Tokenize phrase, take embedding vectors for each word and return their mean.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xZP3ei8zOhA"
   },
   "outputs": [],
   "source": [
    "vector = get_phrase_embedding(\"I'm pretty sure, this never happened to me before...\")\n",
    "assert np.allclose(\n",
    "    vector[:10],\n",
    "    [\n",
    "        -0.06321539,\n",
    "        0.22295703,\n",
    "        0.5145831,\n",
    "        -0.31779623,\n",
    "        -0.4945792,\n",
    "        0.13577954,\n",
    "        0.03977086,\n",
    "        0.2254847,\n",
    "        0.02975685,\n",
    "        -0.17784223,\n",
    "    ],\n",
    "), \"your embedding should match the reference (don't mind this if you switched models)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfleSp0TzOhB"
   },
   "outputs": [],
   "source": [
    "# Let's only consider ~5k phrases for a first run.\n",
    "chosen_phrases = data[:: len(data) // 1000]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Compute vectors for chosen phrases and turn them to numpy array.\n",
    "# phrase_vectors = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5E-m97RzOhC"
   },
   "outputs": [],
   "source": [
    "assert isinstance(phrase_vectors, np.ndarray) and np.isfinite(phrase_vectors).all()\n",
    "assert phrase_vectors.shape == (len(chosen_phrases), model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SWTa61DzOhC"
   },
   "outputs": [],
   "source": [
    "# Map vectors into 2d space with PCA, UMAP, t-SNE or your other methods.\n",
    "phrase_vectors_2d = UMAP(n_neighbors=3).fit_transform(phrase_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dewz5Ppthj_S"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=phrase_vectors_2d[:, 0], y=phrase_vectors_2d[:, 1], hover_name=chosen_phrases)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZPmrDBxzOhD"
   },
   "source": [
    "Finally, let's build a simple \"similar question\" engine with phrase embeddings we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_eOw_A4zOhE"
   },
   "outputs": [],
   "source": [
    "# compute vector embedding for all lines in data\n",
    "data_vectors = np.array([get_phrase_embedding(phrase) for phrase in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD2S7HF6zOhF"
   },
   "outputs": [],
   "source": [
    "data_norms = np.linalg.norm(data_vectors, axis=1)\n",
    "\n",
    "\n",
    "def find_nearest(query, k=10):\n",
    "    # YOUR CODE HERE\n",
    "    # Given query line, return k most similar phrases from data in soted order.\n",
    "    # Similarity should be measured as cosine between query and phrase embeddings.\n",
    "    # Hint: it's okay to use global variables like data_vectors etc.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sFkjt8UzOhG"
   },
   "outputs": [],
   "source": [
    "results = find_nearest(query=\"How do I enter the matrix?\", k=10)\n",
    "\n",
    "print(\"\".join(results))\n",
    "\n",
    "assert len(results) == 10 and isinstance(results[0], str)\n",
    "assert results[0] == \"How do I do a matrix transpose in Go?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-o1a547zOhG"
   },
   "outputs": [],
   "source": [
    "results = find_nearest(query=\"Why don't i ask a question myself?\", k=10)\n",
    "print(\"\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtiym_38zOhI"
   },
   "source": [
    "## Now what?\n",
    "* Try running TSNE instead of UMAP (it takes a long time)\n",
    "* Try running UMAP or TSNE on all data, not just 5000 phrases\n",
    "* See what other embeddings are there in the model zoo: `gensim.downloader.info()`\n",
    "* Take a look at [FastText](https://github.com/facebookresearch/fastText) embeddings\n",
    "* Optimize find_nearest with locality-sensitive hashing: use [nearpy](https://github.com/pixelogik/NearPy) or `sklearn.neighbors`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeFVovmZzOhI"
   },
   "source": [
    "## Extra: your own word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1jqGcjpzOhI"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "vocabulary = set(chain.from_iterable(data_tok))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# word_to_index = ...\n",
    "# index_to_word = ...\n",
    "word_counter = {word: 0 for word in word_to_index.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YZSGkVHzOhJ"
   },
   "source": [
    "Generating context pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJnIfcoVzOhJ"
   },
   "outputs": [],
   "source": [
    "context_pairs = []\n",
    "window = 4\n",
    "\n",
    "for text in data_tok:\n",
    "    for i, word in enumerate(text):\n",
    "        context_indices = range(max(0, i - window), min(i + window, len(text)))\n",
    "        for j in context_indices:\n",
    "            if j == i:\n",
    "                continue\n",
    "\n",
    "            context_pairs.append((word_to_index[word], word_to_index[text[j]]))\n",
    "            word_counter[word] += 1.0\n",
    "\n",
    "print(f\"Generated {len(context_pairs)} pairs of target and context words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAFjTN6BzOhJ"
   },
   "source": [
    "Casting everything to `torch.LongTensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fF5DsPeMzOhJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "data_torch = torch.tensor(context_pairs, dtype=torch.long)\n",
    "X_torch = data_torch[:, 0]\n",
    "y_torch = data_torch[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t595BmSzOhJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, word):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycVuOP1XzOhJ"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Word2VecModel(25, len(word_to_index)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# To reduce learning rate on plateau of the loss functions\n",
    "lr_scheduler = ReduceLROnPlateau(opt, patience=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXYR683izOhK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "n_steps = 1000\n",
    "loss_history = []\n",
    "for i in range(n_steps):\n",
    "    ix = np.random.randint(0, len(context_pairs), batch_size)\n",
    "    x_batch = X_torch[ix].to(device)\n",
    "    y_batch = y_torch[ix].to(device)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # predict logits\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # compute loss\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # clear gradients\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # compute gradients\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # optimizer step\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "    lr_scheduler.step(loss_history[-1])\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px0urkxSzOhK"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = next(model.word2emb.parameters()).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwXCNQqDzOhK"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def find_nearest(word, k=10):\n",
    "    word_vector = embedding_matrix[word_to_index[word]][None, :]\n",
    "    dists = F.cosine_similarity(embedding_matrix, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-k:]\n",
    "    return [index_to_word[x] for x in top_k.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "938838nczOhL"
   },
   "outputs": [],
   "source": [
    "find_nearest(\"apple\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyMj1hDDzOhL"
   },
   "source": [
    "It might look not so promising. Remember about the upgrades to word2vec: subsampling and negative sampling."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "binpord_practice_word_embeddings_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
