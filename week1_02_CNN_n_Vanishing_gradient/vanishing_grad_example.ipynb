{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook, we will demonstrate the difference between using sigmoid and ReLU nonlinearities in a simple neural network with two hidden layers. This notebook is built off of a minimal net demo done by Andrej Karpathy for CS 231n, which you can check out here: http://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"legend.fontsize\"] = 16\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data -- not linearly separable\n",
    "np.random.seed(0)\n",
    "N = 100  # number of points per class\n",
    "D = 2  # dimensionality\n",
    "K = 3  # number of classes\n",
    "X = np.zeros((N * K, D))\n",
    "num_train_examples = X.shape[0]\n",
    "y = np.zeros(N * K, dtype=\"uint8\")\n",
    "for j in range(K):\n",
    "    ix = range(N * j, N * (j + 1))\n",
    "    r = np.linspace(0.0, 1, N)  # radius\n",
    "    t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2  # theta\n",
    "    X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n",
    "    y[ix] = j\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function \"squashes\" inputs to lie between 0 and 1. Unfortunately, this means that for inputs with sigmoid output close to 0 or 1, the gradient with respect to those inputs are close to zero. This leads to the phenomenon of vanishing gradients, where gradients drop close to zero, and the net does not learn well.\n",
    "\n",
    "On the other hand, the relu function (max(0, x)) does not saturate with input size. Plot these functions to gain intution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (x) * (1 - x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and see now how the two kinds of nonlinearities change deep neural net training in practice. Below, we build a very simple neural net with three layers (two hidden layers), for which you can swap out ReLU/ sigmoid nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a three layer neural net with either RELU or sigmoid nonlinearity via vanilla grad descent\n",
    "\n",
    "\n",
    "def three_layer_net(NONLINEARITY, X, y, model, step_size, reg):\n",
    "    # parameter initialization\n",
    "\n",
    "    W1 = model[\"W1\"]\n",
    "    W2 = model[\"W2\"]\n",
    "    W3 = model[\"W3\"]\n",
    "    b1 = model[\"b1\"]\n",
    "    b2 = model[\"b2\"]\n",
    "    b3 = model[\"b3\"]\n",
    "\n",
    "    # some hyperparameters\n",
    "\n",
    "    # gradient descent loop\n",
    "    num_examples = X.shape[0]\n",
    "    plot_array_1 = []\n",
    "    plot_array_2 = []\n",
    "    for i in range(50000):\n",
    "\n",
    "        # FOWARD PROP\n",
    "\n",
    "        if NONLINEARITY == \"RELU\":\n",
    "            hidden_layer = relu(np.dot(X, W1) + b1)\n",
    "            hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)\n",
    "            scores = np.dot(hidden_layer2, W3) + b3\n",
    "\n",
    "        elif NONLINEARITY == \"SIGM\":\n",
    "            hidden_layer = sigmoid(np.dot(X, W1) + b1)\n",
    "            hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)\n",
    "            scores = np.dot(hidden_layer2, W3) + b3\n",
    "\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # [N x K]\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "        data_loss = np.sum(corect_logprobs) / num_examples\n",
    "        reg_loss = (\n",
    "            0.5 * reg * np.sum(W1 * W1) + 0.5 * reg * np.sum(W2 * W2) + 0.5 * reg * np.sum(W3 * W3)\n",
    "        )\n",
    "        loss = data_loss + reg_loss\n",
    "        if i % 1000 == 0:\n",
    "            print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples), y] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # BACKPROP HERE\n",
    "        dW3 = (hidden_layer2.T).dot(dscores)\n",
    "        db3 = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        if NONLINEARITY == \"RELU\":\n",
    "\n",
    "            # backprop ReLU nonlinearity here\n",
    "            dhidden2 = np.dot(dscores, W3.T)\n",
    "            dhidden2[hidden_layer2 <= 0] = 0\n",
    "            dW2 = np.dot(hidden_layer.T, dhidden2)\n",
    "            plot_array_2.append(np.sum(np.abs(dW2)) / np.sum(np.abs(dW2.shape)))\n",
    "            db2 = np.sum(dhidden2, axis=0)\n",
    "            dhidden = np.dot(dhidden2, W2.T)\n",
    "            dhidden[hidden_layer <= 0] = 0\n",
    "\n",
    "        elif NONLINEARITY == \"SIGM\":\n",
    "\n",
    "            # backprop sigmoid nonlinearity here\n",
    "            dhidden2 = dscores.dot(W3.T) * sigmoid_grad(hidden_layer2)\n",
    "            dW2 = (hidden_layer.T).dot(dhidden2)\n",
    "            plot_array_2.append(np.sum(np.abs(dW2)) / np.sum(np.abs(dW2.shape)))\n",
    "            db2 = np.sum(dhidden2, axis=0)\n",
    "            dhidden = dhidden2.dot(W2.T) * sigmoid_grad(hidden_layer)\n",
    "\n",
    "        dW1 = np.dot(X.T, dhidden)\n",
    "        plot_array_1.append(np.sum(np.abs(dW1)) / np.sum(np.abs(dW1.shape)))\n",
    "        db1 = np.sum(dhidden, axis=0)\n",
    "\n",
    "        # add regularization\n",
    "        dW3 += reg * W3\n",
    "        dW2 += reg * W2\n",
    "        dW1 += reg * W1\n",
    "\n",
    "        # option to return loss, grads -- uncomment next comment\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = dW1\n",
    "        grads[\"W2\"] = dW2\n",
    "        grads[\"W3\"] = dW3\n",
    "        grads[\"b1\"] = db1\n",
    "        grads[\"b2\"] = db2\n",
    "        grads[\"b3\"] = db3\n",
    "        # return loss, grads\n",
    "\n",
    "        # update\n",
    "        W1 += -step_size * dW1\n",
    "        b1 += -step_size * db1\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        W3 += -step_size * dW3\n",
    "        b3 += -step_size * db3\n",
    "    # evaluate training set accuracy\n",
    "    if NONLINEARITY == \"RELU\":\n",
    "        hidden_layer = relu(np.dot(X, W1) + b1)\n",
    "        hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)\n",
    "    elif NONLINEARITY == \"SIGM\":\n",
    "        hidden_layer = sigmoid(np.dot(X, W1) + b1)\n",
    "        hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)\n",
    "    scores = np.dot(hidden_layer2, W3) + b3\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    print(\"training accuracy: %.2f\" % (np.mean(predicted_class == y)))\n",
    "    # return cost, grads\n",
    "    return plot_array_1, plot_array_2, W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train net with sigmoid nonlinearity first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize toy model, train sigmoid net\n",
    "\n",
    "N = 100  # number of points per class\n",
    "D = 2  # dimensionality\n",
    "K = 3  # number of classes\n",
    "h = 50\n",
    "h2 = 50\n",
    "num_train_examples = X.shape[0]\n",
    "\n",
    "model = {}\n",
    "model[\"h\"] = h  # size of hidden layer 1\n",
    "model[\"h2\"] = h2  # size of hidden layer 2\n",
    "model[\"W1\"] = 0.1 * np.random.randn(D, h)\n",
    "model[\"b1\"] = np.zeros((1, h))\n",
    "model[\"W2\"] = 0.1 * np.random.randn(h, h2)\n",
    "model[\"b2\"] = np.zeros((1, h2))\n",
    "model[\"W3\"] = 0.1 * np.random.randn(h2, K)\n",
    "model[\"b3\"] = np.zeros((1, K))\n",
    "\n",
    "(sigm_array_1, sigm_array_2, s_W1, s_W2, s_W3, s_b1, s_b2, s_b3) = three_layer_net(\n",
    "    \"SIGM\", X, y, model, step_size=1e-1, reg=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now train net with ReLU nonlinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-initialize model, train relu net\n",
    "\n",
    "model = {}\n",
    "model[\"h\"] = h  # size of hidden layer 1\n",
    "model[\"h2\"] = h2  # size of hidden layer 2\n",
    "model[\"W1\"] = 0.1 * np.random.randn(D, h)\n",
    "model[\"b1\"] = np.zeros((1, h))\n",
    "model[\"W2\"] = 0.1 * np.random.randn(h, h2)\n",
    "model[\"b2\"] = np.zeros((1, h2))\n",
    "model[\"W3\"] = 0.1 * np.random.randn(h2, K)\n",
    "model[\"b3\"] = np.zeros((1, K))\n",
    "\n",
    "(relu_array_1, relu_array_2, r_W1, r_W2, r_W3, r_b1, r_b2, r_b3) = three_layer_net(\n",
    "    \"RELU\", X, y, model, step_size=1e-1, reg=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing Gradient Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the sum of the magnitude of gradients for the weights between hidden layers as a cheap heuristic to measure speed of learning (you can also use the magnitude of gradients for each neuron in the hidden layer here). Intuitevely, when the magnitude of the gradients of the weight vectors or of each neuron are large, the net is learning faster. (NOTE: For our net, each hidden layer has the same number of neurons. If you want to play around with this, make sure to adjust the heuristic to account for the number of neurons in the layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({\"figure.figsize\": (12, 8), \"font.size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(sigm_array_1))\n",
    "plt.plot(np.array(sigm_array_2))\n",
    "plt.title(\"Sum of magnitudes of gradients -- SIGM weights\")\n",
    "plt.legend((\"sigm first layer\", \"sigm second layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(relu_array_1))\n",
    "plt.plot(np.array(relu_array_2))\n",
    "plt.title(\"Sum of magnitudes of gradients -- ReLU weights\")\n",
    "plt.legend((\"relu first layer\", \"relu second layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaying the two plots to compare\n",
    "plt.plot(np.array(relu_array_1))\n",
    "plt.plot(np.array(relu_array_2))\n",
    "plt.plot(np.array(sigm_array_1))\n",
    "plt.plot(np.array(sigm_array_2))\n",
    "plt.title(\"Sum of magnitudes of gradients -- hidden layer neurons\")\n",
    "plt.legend((\"relu first layer\", \"relu second layer\", \"sigm first layer\", \"sigm second layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feel free to play around with this notebook to gain intuition. Things you might want to try:\n",
    "\n",
    "- Adding additional layers to the nets and seeing how early layers continue to train slowly for the sigmoid net \n",
    "- Experiment with hyperparameter tuning for the nets -- changing regularization and gradient descent step size\n",
    "- Experiment with different nonlinearities -- Leaky ReLU, Maxout. How quickly do different layers learn now?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how well each classifier does in terms of distinguishing the toy data classes. As expected, since the ReLU net trains faster, for a set number of epochs it performs better compared to the sigmoid net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the classifiers- SIGMOID\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = (\n",
    "    np.dot(\n",
    "        sigmoid(np.dot(sigmoid(np.dot(np.c_[xx.ravel(), yy.ravel()], s_W1) + s_b1), s_W2) + s_b2),\n",
    "        s_W3,\n",
    "    )\n",
    "    + s_b3\n",
    ")\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the classifiers-- RELU\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = (\n",
    "    np.dot(\n",
    "        relu(np.dot(relu(np.dot(np.c_[xx.ravel(), yy.ravel()], r_W1) + r_b1), r_W2) + r_b2), r_W3\n",
    "    )\n",
    "    + r_b3\n",
    ")\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
