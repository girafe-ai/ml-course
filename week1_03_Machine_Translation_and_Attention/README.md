Neural Machine Translation as seq2sec:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/master/week1_03_Machine_Translation_and_Attention/practice_seq2seq_for_nmt.ipynb)

Solved version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/master/week1_03_Machine_Translation_and_Attention/practice_seq2seq_for_nmt_solved.ipynb)

Attention basics and Tensorboard example:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/master/week1_03_Machine_Translation_and_Attention/extra_practice_Attention_basics_and_tensorboard.ipynb)

Further readings:

- Great blog post by Jay Alammar:
  https://jalammar.github.io/illustrated-transformer/
- Notebook on positional encoding:
  [link](https://github.com/ml-mipt/ml-mipt/blob/advanced/week04_Transformer/week04_positional_encoding_carriers.ipynb)
- Great Annotated Transformer article with code and comments by Harvard NLP
  group: https://nlp.seas.harvard.edu/2018/04/03/attention.html
