Slides:
[link](https://github.com/girafe-ai/ml-mipt/blob/21f_advanced/week1_03_machine_translation_and_attention/ml-mipt_f21_lect103_Machine_Tranlation.pdf)

CNN for salary prediction

* Self-practice version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/21f_advanced/week1_03_machine_translation_and_attention/practice1_03_seq2seq_nmt_and_tensorboard.ipynb)

* Solved version:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/21f_advanced/week1_03_machine_translation_and_attention/practice1_03_seq2seq_nmt_and_tensorboard__completed.ipynb)


Further readings:

1. Great explanation of attention and seq2seq translation by Lena Voita: https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn
2. Blog post by Jay Alammar on the seq2seq and attention: http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/