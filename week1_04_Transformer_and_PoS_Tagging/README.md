PoS Tagging with BiLSTM:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/advanced_s21/week1_04_Transformer_and_PoS_Tagging/week1_04_BiLSTM_for_PoS_Tagging.ipynb)

Understanding the positional encoding:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/advanced_s21/week1_04_Transformer_and_PoS_Tagging/week1_04_positional_encoding_carriers.ipynb)

Full Transformer architecture and training pipeline by Harvard NLP:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb)

**Further readings**:

- [en] The Illustrated Transformer
  [blog post](https://jalammar.github.io/illustrated-transformer/)

- [en] Harvard NLP
  [full implementation in PyTorch](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

- [en] OpenAI blog post
  [Better Language Models and Their Implications (GPT-2)](https://openai.com/blog/better-language-models/)

- [en] Paper describing positional encoding
  ["Convolutional Sequence to Sequence Learning"](https://arxiv.org/pdf/1705.03122)

- [en] Paper presenting [Layer Normalization](https://arxiv.org/abs/1607.06450)
