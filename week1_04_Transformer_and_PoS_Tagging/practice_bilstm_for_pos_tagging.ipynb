{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBl105NwehuA"
   },
   "source": [
    "# Practice: BiLSTM for PoS Tagging\n",
    "_This notebook is based on [open-source implementation](https://github.com/bentrevett/pytorch-pos-tagging) of PoS Tagging in PyTorch._\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this series we'll be building a machine learning model that produces an output for every element in an input sequence, using PyTorch and torchtext. Specifically, we will be inputting a sequence of text and the model will output a part-of-speech (PoS) tag for each token in the input text. This can also be used for named entity recognition (NER), where the output for each token will be what type of entity, if any, the token is.\n",
    "\n",
    "In this notebook, we'll be implementing a multi-layer bi-directional LSTM (BiLSTM) to predict PoS tags using the Universal Dependencies English Web Treebank (UDPOS) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nraZXRyjIlhw"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's use the torchtext to load the data. Each item of the UDPOS dataset consists of 3 lists: a tokenized sentence and two different sets of tags, [universal dependency (UD) tags](https://universaldependencies.org/u/pos/) and [Penn Treebank (PTB) tags](https://www.sketchengine.eu/penn-treebank-tagset/). We'll only train our model on the UD tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaYrowz1mRiy"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import UDPOS\n",
    "\n",
    "\n",
    "train_data = list(UDPOS(split=\"train\"))\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print()\n",
    "\n",
    "text, ud_tags, ptb_tags = train_data[0]\n",
    "print(\"-------------------------------\")\n",
    "print(\"Word\\t\\tUD Tag\\tPTB tag\")\n",
    "print(\"-------------------------------\")\n",
    "for word, ud_tag, ptb_tag in zip(text, ud_tags, ptb_tags):\n",
    "    print(f\"{word:<8}\\t{ud_tag}\\t{ptb_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn-3UmXdXXUw"
   },
   "source": [
    "Just like before, we need to build vocabularies for both words and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAk6S047mRS0"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "\n",
    "word_counts = Counter()\n",
    "tag_counts = Counter()\n",
    "for text, tags, _ in train_data:\n",
    "    word_counts.update(word.lower() for word in text)\n",
    "    tag_counts.update(tags)\n",
    "\n",
    "word_vocab = Vocab(word_counts, min_freq=2)\n",
    "tag_vocab = Vocab(tag_counts)\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "print(\"Tag\\t\\tCount\\tPercentage\")\n",
    "print(\"----------------------------------\")\n",
    "total = sum(tag_counts.values())\n",
    "for tag, count in tag_counts.most_common():\n",
    "    print(f\"{tag:<8}\\t{count}\\t{100 * count / total:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3jm73U0NDIC"
   },
   "source": [
    "Our word vocabulary will have to handle unknown tokens (note how we set `min_freq` to 2 for it) in order to simulate the real world conditions. On the other hand, the tags vocabulary doesn't have such a problem, as we deal with strictly finite set of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsWfXovnNumB"
   },
   "outputs": [],
   "source": [
    "word_vocab.insert_token(\"<unk>\", index=0)\n",
    "word_vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKxPe_8KNwGZ"
   },
   "source": [
    "Also, both vocabularies would need a padding tokens to handle the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrdSmIhwm8ik"
   },
   "outputs": [],
   "source": [
    "word_vocab.append_token(\"<pad>\")\n",
    "tag_vocab.append_token(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wH0DHOmOCOl"
   },
   "source": [
    "Now that we have our vocabularies, we are ready to start building our model! However, before we get there, let's use one more thing. The torchtext library provides a range of pretrained word2vec models for us to use. Let's use the [GloVe](https://nlp.stanford.edu/projects/glove/) to initialize our embeddings!\n",
    "\n",
    "We can load the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u55xyc5YXUxP"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "\n",
    "glove = GloVe(name=\"6B\", dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4l7SAkPOyUx"
   },
   "source": [
    "And extract word vectors as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nL_xm3vOx8j"
   },
   "outputs": [],
   "source": [
    "glove[\"word\"].shape, glove[\"word\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWnftE0PO161"
   },
   "source": [
    "Using this dict-like interface, we can extract vectors for all the words, the model saw during training. What happens to the out-of-vocabulary words? Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJyxH-Ugo1g7"
   },
   "outputs": [],
   "source": [
    "glove[\"some-non-existent-word\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKOShMVfPYdE"
   },
   "source": [
    "The torchtext's GloVe yields a zeros vector. This is the default behaviour, which we can modify using the `unk_init` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvzOOBwyo9zP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "glove = GloVe(name=\"6B\", dim=100, unk_init=torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9HAE7gSPpUe"
   },
   "source": [
    "Now the in-vocabulary words still yield the same vectors as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nI1dX0GPtYP"
   },
   "outputs": [],
   "source": [
    "glove[\"word\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54EHlyxaPuSH"
   },
   "source": [
    "But out-of-vocabulary words now yield normally distributed vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8eNht8-pTR3"
   },
   "outputs": [],
   "source": [
    "glove[\"some-non-existent-word\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUP3WVb4P092"
   },
   "source": [
    "What's important here is to note that GloVe doesn't save this *unknown* vectors, it just calls our `unk_init` function on each unknown word each time. This means that if we try to fetch a vector for the same out-of-vocabulary word once more, we would get a completely different vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvsuxqARpUmZ"
   },
   "outputs": [],
   "source": [
    "glove[\"some-non-existent-word\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AeOoSM0QMaa"
   },
   "source": [
    "How is this usefull? Well, we don't want to fetch vectors from GloVe all the time. We actually want our model to learn more task specific embeddings during training, so we still want to create an embeddings layer. However, it might be a good idea to initialize embeddings for the known words with something more meaningful than just random. And this is exactly the place, where GloVe comes into play! Let's generate a matrix with embeddings for all words in our vocabulary (including the out-of-vocabulary words for the GloVe, in which case we will get just random vectors) and use it to init out embeddings layer later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9P6lcQYowp7"
   },
   "outputs": [],
   "source": [
    "word_vectors = glove.get_vecs_by_tokens(word_vocab.lookup_tokens(range(len(word_vocab))))\n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX4H5ULDQ6j7"
   },
   "source": [
    "Now that are finished tinkering with data, the last bit is to create our `DataLoader` for which we once more need to write a custom `collate_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQJKpPcQXmaK"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, tags_list = [], []\n",
    "    for text, tags, _ in batch:\n",
    "        # YOUR CODE HERE\n",
    "        # Convert text and tags into lists of token indices and cast them\n",
    "        # into torch tensors. Store tensors in text_list and tag_list.\n",
    "        pass\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Pad sequences with pad_sequence function.\n",
    "    # texts_padded = pad_sequence(...)\n",
    "    # tags_padded = pad_sequence(...)\n",
    "\n",
    "    return texts_padded, tags_padded\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "text, tags = next(iter(train_dataloader))\n",
    "text.shape, tags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs8RH2Z6RvfE"
   },
   "source": [
    "Let's also create a `DataLoader` for a validation dataset to evaluate our model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqK_DQ74wYv_"
   },
   "outputs": [],
   "source": [
    "val_data = list(UDPOS(split=\"valid\"))\n",
    "val_dataloader = DataLoader(val_data, batch_size, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyNaPpyKehuR"
   },
   "source": [
    "## Building the Model\n",
    "\n",
    "Next up, we define our model - a multi-layer bi-directional LSTM. The image below shows a simplified version of the model with only one LSTM layer and omitting the LSTM's cell state for clarity.\n",
    "\n",
    "![](https://github.com/girafe-ai/ml-mipt/blob/21f_advanced/week1_04_transformer_n_pos_tagging/assets/pos-bidirectional-lstm.png?raw=1)\n",
    "\n",
    "The model takes in a sequence of tokens, $X = \\{x_1, x_2,...,x_T\\}$, passes them through an embedding layer, $e$, to get the token embeddings, $e(X) = \\{e(x_1), e(x_2), ..., e(x_T)\\}$.\n",
    "\n",
    "These embeddings are processed - one per time-step - by the forward and backward LSTMs. The forward LSTM processes the sequence from left-to-right, whilst the backward LSTM processes the sequence right-to-left, i.e. the first input to the forward LSTM is $x_1$ and the first input to the backward LSTM is $x_T$. \n",
    "\n",
    "The LSTMs also take in the the hidden, $h$, and cell, $c$, states from the previous time-step\n",
    "\n",
    "$$h^{\\rightarrow}_t = \\text{LSTM}^{\\rightarrow}(e(x^{\\rightarrow}_t), h^{\\rightarrow}_{t-1}, c^{\\rightarrow}_{t-1})$$\n",
    "$$h^{\\leftarrow}_t=\\text{LSTM}^{\\leftarrow}(e(x^{\\leftarrow}_t), h^{\\leftarrow}_{t-1}, c^{\\leftarrow}_{t-1})$$\n",
    "\n",
    "After the whole sequence has been processed, the hidden and cell states are then passed to the next layer of the LSTM.\n",
    "\n",
    "The initial hidden and cell states, $h_0$ and $c_0$, for each direction and layer are initialized to a tensor full of zeros.\n",
    "\n",
    "We then concatenate both the forward and backward hidden states from the final layer of the LSTM, $H = \\{h_1, h_2, ... h_T\\}$, where $h_1 = [h^{\\rightarrow}_1;h^{\\leftarrow}_T]$, $h_2 = [h^{\\rightarrow}_2;h^{\\leftarrow}_{T-1}]$, etc. and pass them through a linear layer, $f$, which is used to make the prediction of which tag applies to this token, $\\hat{y}_t = f(h_t)$.\n",
    "\n",
    "When training the model, we will compare our predicted tags, $\\hat{Y}$ against the actual tags, $Y$, to calculate a loss, the gradients w.r.t. that loss, and then update our parameters.\n",
    "\n",
    "We implement the model detailed above in the `BiLSTMPOSTagger` class.\n",
    "\n",
    "`nn.Embedding` is an embedding layer and the input dimension should be the size of the input (text) vocabulary. We tell it what the index of the padding token is so it does not update the padding token's embedding entry.\n",
    "\n",
    "`nn.LSTM` is the LSTM. We apply dropout as regularization between the layers, if we are using more than one.\n",
    "\n",
    "`nn.Linear` defines the linear layer to make predictions using the LSTM outputs. We double the size of the input if we are using a bi-directional LSTM. The output dimensions should be the size of the tag vocabulary.\n",
    "\n",
    "We also define a dropout layer with `nn.Dropout`, which we use in the `forward` method to apply dropout to the embeddings and the outputs of the final layer of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GL4irQ2MehuR"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, word_vectors, hidden_dim, n_tags, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # This is how we can init our embeddings with pretrained word vectors.\n",
    "        # The freeze=False parameter tells torch that we still want to update\n",
    "        # the embeddings during training.\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            word_vectors, freeze=False, padding_idx=pad_idx\n",
    "        )\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Define LSTM, linear and dropout layers.\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text has a shape of [seq_len, batch_size]\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Compute an embedding and apply dropout.\n",
    "        # embedded = ...\n",
    "        # embedded should have a shape of [seq_len, batch_size, emb_dim]\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Compute the RNN output values.\n",
    "        # outputs = ...\n",
    "        # Note that RNN will return two things:\n",
    "        # 1) outputs: holds the backward and forward hidden states in the final layer\n",
    "        # 2) hidden: holds the backward and forward hidden state at the final time-step\n",
    "        # outputs should have a shape of [seq_len, batch_size, 2 * hid_dim]\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Use the fc layer to make a prediction of what the tag should be.\n",
    "        # Don't forget to apply dropout\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21qZTMjMehuR"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "Next, we instantiate the model. We need to ensure the embedding dimensions matches that of the GloVe embeddings we loaded earlier.\n",
    "\n",
    "The rest of the hyperparmeters have been chosen as sensible defaults, though there may be a combination that performs better on this model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMklHBCZehuS"
   },
   "outputs": [],
   "source": [
    "model = BiLSTMPOSTagger(\n",
    "    word_vectors,\n",
    "    hidden_dim=128,\n",
    "    n_tags=len(tag_vocab),\n",
    "    n_layers=2,\n",
    "    dropout=0.25,\n",
    "    pad_idx=word_vocab[\"<pad>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL6jYTEDehuS"
   },
   "source": [
    "We initialize the weights from a simple Normal distribution. Again, there may be a better initialization scheme for this model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulVTSarBehuS"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "\n",
    "model.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpk8J2cuehuS"
   },
   "source": [
    "Next, a small function to tell us how many parameters are in our model. Useful for comparing different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HZqDv7pehuS"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkFVcMP-ehuT"
   },
   "source": [
    "Now we define our optimizer. We use Adam with the default learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuki8eUQehuU"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "senCh0bhehuU"
   },
   "source": [
    "Next, we define our loss function, cross-entropy loss.\n",
    "\n",
    "Even though we have no `<unk>` tokens within our tag vocab, we still have `<pad>` tokens. This is because all sentences within a batch need to be the same size. However, we don't want to calculate the loss when the target is a `<pad>` token as we aren't training our model to recognize padding tokens.\n",
    "\n",
    "We handle this by setting the `ignore_index` in our loss function to the index of the padding token in our tag vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AohETU56ehuU"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tag_vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKOeDzKAehuU"
   },
   "source": [
    "We then place our model on our GPU, if we have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UduGW2mqehuV"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtpPMFR3ehuV"
   },
   "source": [
    "We will be using the loss value between our predicted and actual tags to train the network, but ideally we'd like a more interpretable way to see how well our model is doing - accuracy.\n",
    "\n",
    "The issue is that we don't want to calculate accuracy over the `<pad>` tokens as we aren't interested in predicting them.\n",
    "\n",
    "The function below only calculates accuracy over non-padded tokens. We then compare the predictions of such elements with the labels to get a count of how many predictions were correct. We then divide this by the number of non-pad elements to get our accuracy value over the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rd7Kol-ehuW"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(pred, target, pad_idx=tag_vocab[\"<pad>\"]):\n",
    "    pred = pred.argmax(dim=1)\n",
    "    correct = (pred == target) & (target != pad_idx)\n",
    "    return correct.sum() / torch.count_nonzero(target != pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRIJVmxST8_A"
   },
   "source": [
    "Once again we will use a tensorboard to track our training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVrf9yc2DXzf"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQbqNS0xehuW"
   },
   "source": [
    "Next is the training loop.\n",
    "\n",
    "We first set the model to `train` mode to turn on dropout/batch-norm/etc. (if used). Then we iterate over our iterator, which returns a batch of examples. \n",
    "\n",
    "For each batch: \n",
    "- we zero the gradients over the parameters from the last gradient calculation\n",
    "- insert the batch of text into the model to get predictions\n",
    "- as PyTorch loss functions cannot handle 3-dimensional predictions we reshape our predictions\n",
    "- calculate the loss and accuracy between the predicted tags and actual tags\n",
    "- call `backward` to calculate the gradients of the parameters w.r.t. the loss\n",
    "- take an optimizer `step` to update the parameters\n",
    "- add to the running total of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUwtU5pqDbc2"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "n_epochs = 15\n",
    "global_step = 0  # for writer\n",
    "for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    for text, tags in tqdm(train_dataloader, desc=\"Train\", leave=False):\n",
    "        # YOUR CODE HERE\n",
    "        # Use model to get prediction and compute loss using criterion.\n",
    "        # After you've computed loss, zero gradients, run backprop and\n",
    "        # update model with optimizer.\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(pred, tags).item()\n",
    "        writer.add_scalar(\"Training/loss\", loss.item(), global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    writer.add_scalar(\"Evaluation/train_loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Evaluation/train_acc\", train_acc, epoch)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text, tags in tqdm(val_dataloader, desc=\"Val\", leave=False):\n",
    "            # YOUR CODE HERE\n",
    "            # Once again compute model prediction and loss, but don't\n",
    "            # try and update model parameters with it.\n",
    "            # Just use it for model evaluation.\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += accuracy(pred, tags).item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc /= len(val_dataloader)\n",
    "    writer.add_scalar(\"Evaluation/val_loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Evaluation/val_acc\", val_acc, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFekLN_6ehuX"
   },
   "source": [
    "## Inference\n",
    "\n",
    "We should see validation accuracy around 90%, which looks pretty good. Let's see our model tag some actual sentences!\n",
    "\n",
    "We define a `tag_sentence` function that will:\n",
    "- put the model into evaluation mode\n",
    "- tokenize the sentence if it is not a list\n",
    "- lowercase the tokens\n",
    "- numericalize the tokens using the vocabulary\n",
    "- convert the numericalized tokens into a tensor and add a batch dimension\n",
    "- feed the tensor into the model\n",
    "- get the predictions over the sentence\n",
    "- convert the predictions into readable tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1x8bsx_ehuX"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def tag_sentence(sent, model):\n",
    "    if isinstance(sent, str):\n",
    "        tokens = tokenizer.tokenize(sent.lower())\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sent]\n",
    "\n",
    "    encoded = [word_vocab[token] for token in tokens]\n",
    "    encoded = torch.tensor(encoded, device=device).unsqueeze(1)\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(encoded).squeeze(1)\n",
    "    pred = pred.argmax(dim=1)\n",
    "    tag_itos = tag_vocab.get_itos()\n",
    "    pred_tags = [tag_itos[t.item()] for t in pred]\n",
    "    return tokens, pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAei5AvrehuX"
   },
   "source": [
    "We'll get an already tokenized example from the validation set and test our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6DfrieMehuY"
   },
   "outputs": [],
   "source": [
    "example_idx = 5\n",
    "example_sent, example_tags, _ = val_data[example_idx]\n",
    "tokens, pred = tag_sentence(example_sent, model)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "for token, pred_tag, actual_tag in zip(tokens, pred, example_tags):\n",
    "    correct = \"✔\" if pred_tag == actual_tag else \"✘\"\n",
    "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROYhmLepehuY"
   },
   "source": [
    "We can then check how well it did!\n",
    "\n",
    "Let's now make up our own sentence and see how well the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0ocSTluehuZ"
   },
   "outputs": [],
   "source": [
    "sent = \"The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.\"\n",
    "tokens, tags = tag_sentence(sent, model)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"Pred. Tag\\tToken\")\n",
    "print(\"-------------------------\")\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"{tag}\\t\\t{token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmxpmjC9ehuZ"
   },
   "source": [
    "We've now seen how to implement PoS tagging with PyTorch and torchtext! \n",
    "\n",
    "The BiLSTM isn't a state-of-the-art model, in terms of performance, but is a strong baseline for PoS tasks and is a good tool to have in your arsenal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE2RDGUvehuZ"
   },
   "source": [
    "## Going deeper\n",
    "What if we could combine word-level and char-level approaches?\n",
    "\n",
    "![title](https://i.postimg.cc/tT9hsBfj/ive-put-an-rnn-in-your-rnn-so-you-can-train-an-rnn-on-every-step-of-your-rnn-training-loop.jpg)\n",
    "\n",
    "\n",
    "Actually, we can. Let's use LSTM to generate embedding for every word on char-level.\n",
    "![title](https://guillaumegenthial.github.io/assets/char_representation.png)\n",
    "*Image source: https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html*\n",
    "\n",
    "![title](https://guillaumegenthial.github.io/assets/bi-lstm.png)\n",
    "*Image source: https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7YRNqZYehuc"
   },
   "source": [
    "To do that we need to make few adjustments to the code we've written above.\n",
    "\n",
    "First of all, we would need a new vocabulary for the chars, which may sound redundant, but our dataset in fact contains quite a lot of non-latin characters, such as brackets and dashes and everything. We even need an `<unk>` token, because, apparently, validation dataset has the `/` character, whilst the training set doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FXp67nmeJZn"
   },
   "outputs": [],
   "source": [
    "char_counts = Counter()\n",
    "for text, _, _ in train_data:\n",
    "    for word in text:\n",
    "        char_counts.update(c for c in word)\n",
    "\n",
    "char_vocab = Vocab(char_counts)\n",
    "char_vocab.insert_token(\"<unk>\", 0)\n",
    "char_vocab.set_default_index(0)\n",
    "\n",
    "char_vocab.append_token(\"<bos>\")\n",
    "char_vocab.append_token(\"<eos>\")\n",
    "char_vocab.append_token(\"<pad>\")\n",
    "print(f\"Unique tokens in char vocabulary: {len(char_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAbxEtUj8wNA"
   },
   "source": [
    "Note that we added the `<bos>` and `<eos>` tokens into the vocabulary. That has to do with a fact that we want our model to be able to distinguish the situation when it is in a middle of a word from the situation where it is the last character and we need to prepare the summarization.\n",
    "\n",
    "Once we're done collecting the vocabulary, we need to modify our `collate_fn`. We will use the old `collate_batch` to preprocess texts and tags for us and do the chars separately. The biggest challenge with chars is the fact that we need to deal with difference in words' lenghts as well as with difference in sentences' lengths and we need to deal with both simultaneously. The `pad_sequence` just won't cut it. For this I haven't found better solution than to pre-compute the resulting tensor size and create it before-hand. After that we do the second pass through our batch and fill our padded tensor with values. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNwlRypKeJX3"
   },
   "outputs": [],
   "source": [
    "def collate_batch_with_chars(batch):\n",
    "    texts_padded, tags_padded = collate_batch(batch)\n",
    "    max_text_len = texts_padded.shape[0]\n",
    "    max_word_len = 0\n",
    "    for text, _, _ in batch:\n",
    "        for word in text:\n",
    "            if len(word) > max_word_len:\n",
    "                max_word_len = len(word)\n",
    "\n",
    "    max_word_len += 2  # for <bos> and <eos>\n",
    "    chars_padded = torch.full(\n",
    "        (max_word_len, max_text_len, len(batch)), fill_value=char_vocab[\"<pad>\"]\n",
    "    )\n",
    "    for k, (text, _, _) in enumerate(batch):\n",
    "        for j, word in enumerate(text):\n",
    "            chars_padded[: len(word) + 2, j, k] = torch.tensor(\n",
    "                [char_vocab[\"<bos>\"]] + [char_vocab[c] for c in word] + [char_vocab[\"<eos>\"]]\n",
    "            )\n",
    "\n",
    "    return texts_padded, tags_padded, chars_padded\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size, shuffle=True, collate_fn=collate_batch_with_chars\n",
    ")\n",
    "val_dataloader = DataLoader(val_data, batch_size, shuffle=True, collate_fn=collate_batch_with_chars)\n",
    "\n",
    "text, tags, chars = next(iter(train_dataloader))\n",
    "text.shape, tags.shape, chars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFP0ULA2-K75"
   },
   "source": [
    "Now that we can load our data, we need to define our model. The model also resembles the `BiLSTMPOSTagger` a lot. However, there're differences, mainly instead of just one `Embedding` (and `LSTM`) layer we now have two, one for chars and one for words. To keep the competition fair, we init the second one with the same GloVe vectors. The `forward` method also gets a little bit more complicated: we now need to process the chars and concatenate the resulting hidden state to the word embeddings. But that's about it. Nothing too hard. Here it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bT5QrsBgeJVX"
   },
   "outputs": [],
   "source": [
    "class BiLSTMPOSTaggerWithChars(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_chars,\n",
    "        char_emb_dim,\n",
    "        char_hid_dim,\n",
    "        word_vectors,\n",
    "        word_hid_dim,\n",
    "        n_tags,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.char_embedding = nn.Embedding(n_chars, char_emb_dim, padding_idx=char_vocab[\"<pad>\"])\n",
    "        self.char_lstm = nn.LSTM(char_emb_dim, char_hid_dim, bidirectional=True)\n",
    "\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(\n",
    "            word_vectors, freeze=False, padding_idx=word_vocab[\"<pad>\"]\n",
    "        )\n",
    "        self.word_lstm = nn.LSTM(\n",
    "            word_vectors.shape[1] + 2 * char_hid_dim,\n",
    "            word_hid_dim,\n",
    "            n_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(2 * word_hid_dim, n_tags)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, chars):\n",
    "        # chars has a shape of [word_len, text_len, batch_size]\n",
    "        # text has a shape of [text_len, batch_size]\n",
    "        word_len, text_len, batch_size = chars.shape\n",
    "\n",
    "        # Integrate the text length into batch_size to compute\n",
    "        # all words with LSTM simultaneously.\n",
    "        chars = chars.view(word_len, -1)\n",
    "\n",
    "        chars_embedded = self.char_embedding(chars)\n",
    "        chars_embedded = self.dropout(chars_embedded)\n",
    "\n",
    "        # chars_embedded now has a shape of [word_len, text_len * batch_size, char_emb_dim]\n",
    "\n",
    "        # We take only hidden state, which is the last state for the\n",
    "        # LSTM pass in both directions.\n",
    "        _, (chars_hid, _) = self.char_lstm(chars_embedded)\n",
    "\n",
    "        # chars_hid has a shape of [text_len * batch_size, 2 * char_hid_dim].\n",
    "        # Let's \"unwrap\" our texts back:\n",
    "        chars_hid = chars_hid.view(text_len, batch_size, -1)\n",
    "\n",
    "        # Now we compute an embedding for the whole word, concatenate the\n",
    "        # character-based embedding and apply dropout.\n",
    "        word_embedded = self.word_embedding(text)\n",
    "        word_embedded = torch.cat([word_embedded, chars_hid], dim=2)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "\n",
    "        # Now that we have our word_embedded tensor, we are ready to compute\n",
    "        # the LSTM outputs for it in order to predict POS tags from them.\n",
    "        outputs, _ = self.word_lstm(word_embedded)\n",
    "\n",
    "        # Now we apply the dropout and compute our final prediction.\n",
    "        outputs = self.dropout(outputs)\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86bITSaaAOJy"
   },
   "source": [
    "Let's create our model, apply weights init and compute the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_hcfsRVysS2"
   },
   "outputs": [],
   "source": [
    "model = BiLSTMPOSTaggerWithChars(\n",
    "    len(char_vocab),\n",
    "    char_emb_dim=32,\n",
    "    char_hid_dim=32,\n",
    "    word_vectors=word_vectors,\n",
    "    word_hid_dim=128,\n",
    "    n_tags=len(tag_vocab),\n",
    "    n_layers=2,\n",
    "    dropout=0.25,\n",
    ").to(device)\n",
    "model.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McqGlY-veJP1"
   },
   "outputs": [],
   "source": [
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywj5sTUCAdGk"
   },
   "source": [
    "Model has grown a little, but not too much as we set the all the char-related dims to be small.\n",
    "\n",
    "Finally, let's create the optimizer and train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3Rs_dpoeJNe"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTvdwPo4AviV"
   },
   "source": [
    "Because we did a good work of hiding the implementation details in our model, the training procedure doesn't differ much from the one we used to train the previous model. The only difference really is the third tensor we receive from dataloaders and the second parameter (third if you count `self`) in the `model.__call__` method. That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C9_lEO-eJLO"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "n_epochs = 15\n",
    "global_step = 0  # for writer\n",
    "for epoch in trange(n_epochs, desc=\"Epochs\"):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    for text, tags, chars in tqdm(train_dataloader, desc=\"Train\", leave=False):\n",
    "        text, tags, chars = text.to(device), tags.to(device), chars.to(device)\n",
    "        pred = model(text, chars)\n",
    "        pred = pred.view(-1, pred.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = criterion(pred, tags)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(pred, tags).item()\n",
    "        writer.add_scalar(\"Training/loss\", loss.item(), global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    writer.add_scalar(\"Evaluation/train_loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Evaluation/train_acc\", train_acc, epoch)\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text, tags, chars in tqdm(val_dataloader, desc=\"Val\", leave=False):\n",
    "            text, tags, chars = text.to(device), tags.to(device), chars.to(device)\n",
    "            pred = model(text, chars)\n",
    "            pred = pred.view(-1, pred.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            val_loss += criterion(pred, tags).item()\n",
    "            val_acc += accuracy(pred, tags).item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc /= len(val_dataloader)\n",
    "    writer.add_scalar(\"Evaluation/val_loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Evaluation/val_acc\", val_acc, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBt6rx1fBPof"
   },
   "source": [
    "And we can observe the training progress using the same tensorboard. The plots you'll see will probably almost precisely repeat the previous run as the change we introduced is minimal and isn't a deal breaker in any sence, however, it is still cool that we can do something complicated like that with so little code and effort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASmp1E_Cehue"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "practice_bilstm_for_pos_tagging_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
