{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practice_question_answering_and_tts.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DphyQXreodzp"
      },
      "source": [
        "# Practice: Question Answering with a Fine-Tuned BERT (and TTS example)\n",
        "\n",
        "This notebook is based on great [post and corresponding notebook](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/) *by Chris McCormick*. It contains some minor changes and additions (especially parts 3 and 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_mAnIPKaXyw"
      },
      "source": [
        "What does it mean for BERT to achieve \"human-level performance on Question Answering\"? Is BERT the greatest search engine ever, able to find the answer to any question we pose it?\n",
        "\n",
        "In **Part 1** of this notebook, we will discuss what it really means to apply BERT to QA, and illustrate the details.\n",
        "\n",
        "**Part 2** contains example code--we'll be downloading a model that's *already been fine-tuned* for question answering, and try it out on our own text! \n",
        "\n",
        "In **Part 3** we will apply the same approach to Russian language using the model pre-trained on SberQuAD dataset.\n",
        "\n",
        "And in **Part 4** and **Part 5** we will generate question and answer as audio in english and russian languages.\n",
        "\n",
        "**Links**\n",
        "\n",
        "* The [video walkthrough](https://youtu.be/l8ZYCvgGu0o) on this topic. \n",
        "* The [original blog post](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/) version.\n",
        "* The [original Colab Notebook](https://colab.research.google.com/drive/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5uC3kgC7rRK"
      },
      "source": [
        "!pip install -U transformers deeppavlov unidecode omegaconf\n",
        "!python -m deeppavlov install squad_ru_rubert\n",
        "\n",
        "# Pre-downloading the BERT for Russian language. Same result can be achieved with\n",
        "# `!python -m deeppavlov download squad_ru_rubert`\n",
        "# But it works significantly slower.\n",
        "!wget -nc https://www.dropbox.com/s/7za1o6vaffbdlcg/rubert_cased_L-12_H-768_A-12_v1.tar.gz\n",
        "!mkdir -p /root/.deeppavlov/downloads/bert_models/\n",
        "!tar -xzvf rubert_cased_L-12_H-768_A-12_v1.tar.gz -C /root/.deeppavlov/downloads/bert_models\n",
        "\n",
        "!wget -nc https://www.dropbox.com/s/ns8280pd9t9n9dc/squad_model_ru_rubert.tar.gz\n",
        "!mkdir -p /root/.deeppavlov/models/\n",
        "!tar -xzvf squad_model_ru_rubert.tar.gz -C /root/.deeppavlov/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfzJwB_17rRM"
      },
      "source": [
        "import torch\n",
        "\n",
        "assert torch.cuda.is_available(), 'Tacotron2 by NVIDIA infers only on GPU, so the Part 4 will not work on CPU-only machine'\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2', **{'map_location': device})\n",
        "tacotron2.to(device)\n",
        "tacotron2.eval()\n",
        "\n",
        "waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow.to(device)\n",
        "waveglow.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2bUvKUffHNY"
      },
      "source": [
        "## Part 1: Applying BERT to Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su7fixBdiUex"
      },
      "source": [
        "### The SQuAD v1.1 Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT5ESKDxfnLf"
      },
      "source": [
        "When someone mentions \"Question Answering\" as an application of BERT, what they are really referring to is applying BERT to the Stanford Question Answering Dataset (SQuAD).\n",
        "\n",
        "The task posed by the SQuAD benchmark is a little different than you might think. Given a question, and *a passage of text containing the answer* (often refered to as context), BERT needs to highlight the \"span\" of text corresponding to the correct answer. \n",
        "\n",
        "The SQuAD homepage has a fantastic tool for exploring the questions and reference text for this dataset, and even shows the predictions made by top-performing models.\n",
        "\n",
        "For example, here are some [interesting examples](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html?model=r-net+%20(ensemble)%20(Microsoft%20Research%20Asia)&version=1.1) on the topic of Super Bowl 50.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xN5f1bxf6K_"
      },
      "source": [
        "### BERT Input Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctum5SK6f9uP"
      },
      "source": [
        "To feed a QA task into BERT, we pack both the question and the reference text into the input.\n",
        "\n",
        "![Input format for QA](https://raw.githubusercontent.com/neychev/made_nlp_course/master/week10_speech_distillation_and_perspectives/img/input_formatting_image.png)\n",
        "*Image credits: [Chris McCormick](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/)*\n",
        "\n",
        "The two pieces of text are separated by the special `[SEP]` token. \n",
        "\n",
        "> _Side note:_ Original BERT also uses \"Segment Embeddings\" to differentiate the question from the reference text. These are simply two embeddings (for segments \"A\" and \"B\") that BERT learned, and which it adds to the token embeddings before feeding them into the input layer. However today we will be using DistilBERT model, which relies solely on the special tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs31dcrPg5Tg"
      },
      "source": [
        "### Start & End Token Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOdUa9Wg-Uv"
      },
      "source": [
        "BERT needs to highlight a \"span\" of text containing the answer--this is represented as simply predicting which token marks the start of the answer, and which token marks the end.\n",
        "\n",
        "![Start token classification](https://raw.githubusercontent.com/neychev/made_nlp_course/master/week10_speech_distillation_and_perspectives/img/start_token_classification_image.png)\n",
        "*Image credits: [Chris McCormick](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/)*\n",
        "\n",
        "For every token in the text, we feed its final embedding into the start token classifier. The start token classifier only has a single set of weights (represented by the blue \"start\" rectangle in the above illustration) which it applies to every word.\n",
        "\n",
        "After taking the dot product between the output embeddings and the 'start' weights, we apply the softmax activation to produce a probability distribution over all of the words. Whichever word has the highest probability of being the start token is the one that we pick.\n",
        "\n",
        "We repeat this process for the end token--we have a separate weight vector this.\n",
        "\n",
        "![End token classification](https://raw.githubusercontent.com/neychev/made_nlp_course/master/week10_speech_distillation_and_perspectives/img/end_token_classification_image.png)\n",
        "*Image credits: [Chris McCormick](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "457VPa20fZzY"
      },
      "source": [
        "## Part 2: Example Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wpCLgVCkki5"
      },
      "source": [
        "In the example code below, we'll be downloading a model that's *already been fine-tuned* for question answering, and try it out on our own text.\n",
        "\n",
        "If you do want to fine-tune on your own dataset, it is possible to fine-tune BERT for question answering yourself. See [run_squad.py](https://github.com/huggingface/transformers/blob/master/examples/run_squad.py) in the `transformers` library. However, you may find that the \"fine-tuned-on-squad\" model already does a good job, even if your text is from a different domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVq-TuylYRDW"
      },
      "source": [
        "### 1. Load Fine-Tuned BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9nhy3PzGQ44"
      },
      "source": [
        "This example uses the `transformers` [library](https://github.com/huggingface/transformers/) by huggingface. We've already installed it in the top of this notebook.\n",
        "\n",
        "For Question Answering we use the `DistilBertForQuestionAnswering` class from the `transformers` library.\n",
        "\n",
        "This class supports fine-tuning, but for this example we will keep things simpler and load a BERT model that has already been fine-tuned for the SQuAD benchmark.\n",
        "\n",
        "The `transformers` library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation [here](https://huggingface.co/transformers/pretrained_models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apS1yS6CdRyX"
      },
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8imoOxoqGZ0h"
      },
      "source": [
        "> _Side note:_ Apparently the vocabulary of this model is identicaly to the one in bert-base-uncased. You can load the tokenizer from `bert-base-uncased` and that works just as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I__1ubvcZYow"
      },
      "source": [
        "### 2. Ask a Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8MQ7b-GJIcM"
      },
      "source": [
        "Now we're ready to feed in an example!\n",
        "\n",
        "A QA example consists of a question and a passage of text containing the answer to that question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWzZP4EN-Zxg"
      },
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "context = (\n",
        "    \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, \"\n",
        "    \"for a total of 340M parameters! Altogether it is 1.34GB, so expect it to \"\n",
        "    \"take a couple minutes to download to your Colab instance.\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llLvxhScKLZn"
      },
      "source": [
        "We'll need to run the BERT tokenizer against both the `question` and the `context`. To feed these into BERT, we actually concatenate them together and place the special `[SEP]` token in between.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoX33CfKGsr"
      },
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, context)\n",
        "print(f'The input has a total of {len(input_ids)} tokens.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNRVuaKSNFG8"
      },
      "source": [
        "Just to see exactly what the tokenizer is doing, let's print out the tokens with their IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iow838yPNDTv"
      },
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# Display tokens and ids as table.\n",
        "# For each token and its id...\n",
        "for token, token_id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if token_id == tokenizer.sep_token_id:\n",
        "        print()\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, token_id))\n",
        "\n",
        "    if token_id == tokenizer.sep_token_id:\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNwhEw0kQPBN"
      },
      "source": [
        "We're ready to feed our example into the model!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK0obn5x-1EI"
      },
      "source": [
        "import torch\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "start_scores = outputs.start_logits\n",
        "end_scores = outputs.end_logits\n",
        "start_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a30sBTcqQv6X"
      },
      "source": [
        ">*Side Note: Where's the padding?*\n",
        ">\n",
        "> The original [example code](https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#transformers.BertForQuestionAnswering) does not perform any padding. I suspect that this is because we are only feeding in a *single example*. If we instead fed in a batch of examples, then we would need to pad or truncate all of the samples in the batch to a single length, and supply an attention mask to tell BERT to ignore the padding tokens. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBdS_QkIbDzh"
      },
      "source": [
        "Now we can highlight the answer just by looking at the most probable start and end words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeUQ44hAJmn9"
      },
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start : answer_end + 1])\n",
        "\n",
        "print(f'Answer: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twMUWmr2brRw"
      },
      "source": [
        "It got it right! Awesome :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cERYCGKMbOXX"
      },
      "source": [
        "> *Side Note: It's a little naive to pick the highest scores for start and end--what if it predicts an end word that's before the start word?! The correct implementation is to pick the highest total score for which end >= start.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6j2znkwXYsn"
      },
      "source": [
        "With a little more effort, we can reconstruct any words that got broken down into subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBrAsWMJrw7i"
      },
      "source": [
        "answer = tokenizer.convert_tokens_to_string(tokens[answer_start : answer_end + 1])\n",
        "print(f'Answer: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hh6nkIdXq-O"
      },
      "source": [
        "### 3. Visualizing Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hG2YCHYXtg-"
      },
      "source": [
        "Let's see what the scores were for all of the words. The following cells generate bar plots showing the start and end scores for every word in the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkKFa73eJkPE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "plt.rcParams['figure.figsize'] = (16, 8)\n",
        "plt.rcParams['font.size'] = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7kazkb2iEuQ"
      },
      "source": [
        "Retrieve all of the start and end scores, and use all of the tokens as x-axis labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C56AtMg2UBxN"
      },
      "source": [
        "# Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\n",
        "start_scores = start_scores.numpy().flatten()\n",
        "end_scores = end_scores.numpy().flatten()\n",
        "\n",
        "# We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
        "# to be unique, so we'll add the token index to the end of each one.\n",
        "token_labels = []\n",
        "for (i, token) in enumerate(tokens):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIaW7RyTiLeu"
      },
      "source": [
        "Create a bar plot showing the score for every input word being the \"start\" word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6OAV1dL3-UB"
      },
      "source": [
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=start_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('Start Word Scores');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwrF7y6iS1l"
      },
      "source": [
        "Create a second bar plot showing the score for every input word being the \"end\" word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tXEqIp-Tzou"
      },
      "source": [
        "# Create a barplot showing the end word score for all of the tokens.\n",
        "ax = sns.barplot(x=token_labels, y=end_scores, ci=None)\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "ax.grid(True)\n",
        "\n",
        "plt.title('End Word Scores');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awgi7Z_a9KSq"
      },
      "source": [
        "**Alternate View**\n",
        "\n",
        "I also tried visualizing both the start and end scores on a single bar plot, but I think it may actually be more confusing then seeing them separately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4VUk6R05uXS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Store the tokens and scores in a DataFrame. \n",
        "# Each token will have two rows, one for its start score and one for its end\n",
        "# score. The \"marker\" column will differentiate them. A little wacky, I know.\n",
        "scores = []\n",
        "for (i, token_label) in enumerate(token_labels):\n",
        "\n",
        "    # Add the token's start score as one row.\n",
        "    scores.append({'token_label': token_label, \n",
        "                   'score': start_scores[i],\n",
        "                   'marker': 'start'})\n",
        "    \n",
        "    # Add  the token's end score as another row.\n",
        "    scores.append({'token_label': token_label, \n",
        "                   'score': end_scores[i],\n",
        "                   'marker': 'end'})\n",
        "    \n",
        "df = pd.DataFrame(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xyo-I97Ntt"
      },
      "source": [
        "# Draw a grouped barplot to show start and end scores for each word.\n",
        "# The \"hue\" parameter is where we tell it which datapoints belong to which\n",
        "# of the two series.\n",
        "plot = sns.catplot(\n",
        "    x=\"token_label\", y=\"score\", hue=\"marker\",\n",
        "    data=df, kind=\"bar\", height=6, aspect=4\n",
        ")\n",
        "\n",
        "# Turn the xlabels vertical.\n",
        "plot.set_xticklabels(plot.ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "# Turn on the vertical grid to help align words to scores.\n",
        "plot.ax.grid(True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UyBYNmeegGf"
      },
      "source": [
        "### 4. More Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWtcRpPef-Ce"
      },
      "source": [
        "Turn the QA process into a function so we can easily try out other examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH8NbBlsfxZ_"
      },
      "source": [
        "def answer_question(question, context):\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    inputs = tokenizer(question, context, return_tensors='pt')\n",
        "    input_ids = inputs.input_ids.numpy().flatten()\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example question through the model.\n",
        "    outputs = model(**inputs)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    token_ids = input_ids[answer_start : answer_end + 1]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    answer = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVlKTK-njWrX"
      },
      "source": [
        "As our reference text, we've taken the Abstract of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqfBGgc8AKk2"
      },
      "source": [
        "bert_abstract = (\n",
        "    'We introduce a new language representation model called BERT, which stands for '\n",
        "    'Bidirectional Encoder Representations from Transformers. Unlike recent language '\n",
        "    'representation models (Peters et al., 2018a; Radford et al., 2018), BERT is '\n",
        "    'designed to pretrain deep bidirectional representations from unlabeled text by '\n",
        "    'jointly conditioning on both left and right context in all layers. As a result, '\n",
        "    'the pre-trained BERT model can be finetuned with just one additional output '\n",
        "    'layer to create state-of-the-art models for a wide range of tasks, such as '\n",
        "    'question answering and language inference, without substantial taskspecific '\n",
        "    'architecture modifications. BERT is conceptually simple and empirically '\n",
        "    'powerful. It obtains new state-of-the-art results on eleven natural language '\n",
        "    'processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute '\n",
        "    'improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 '\n",
        "    'question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD '\n",
        "    'v2.0 Test F1 to 83.1 (5.1 point absolute improvement).'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay_mwbBJAP87"
      },
      "source": [
        "Let's ask BERT what its name stands for (the answer is in the first sentence of the abstract)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4VPq6FdjxyX"
      },
      "source": [
        "question = \"What does the 'B' in BERT stand for?\"\n",
        "answer = answer_question(question, bert_abstract)\n",
        "print(f'Answer: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6HcijzxkTO9"
      },
      "source": [
        "Let's ask BERT about example applications of itself :)\n",
        "\n",
        "The answer to the question comes from this passage from the abstract: \n",
        "\n",
        "> \"...BERT model can be finetuned with just one additional output\n",
        "layer to create state-of-the-art models for **a wide range of tasks, such as\n",
        "question answering and language inference,** without substantial taskspecific\n",
        "architecture modifications.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVNVGN5-gI06"
      },
      "source": [
        "question = \"What are some example applications of BERT?\"\n",
        "answer = answer_question(question, bert_abstract)\n",
        "print(f'Answer: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXAJ2wkV7rRl"
      },
      "source": [
        "## Part 3. RuBERT for question answering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcnPsGzEbGL6"
      },
      "source": [
        "Here we will use the model pre-trained on the SberQuAD dataset from the [SDSJ-2017 challenge problem B](https://github.com/sberbank-ai/data-science-journey-2017/tree/master/problem_B)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JslS5CG7rRl"
      },
      "source": [
        "from deeppavlov import build_model, configs\n",
        "\n",
        "model_ru = build_model(configs.squad.squad_ru_rubert, download=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHVayqJ37rRl"
      },
      "source": [
        "The following text is copied from [habr post on Crew Dragon flight](https://habr.com/ru/news/t/504642/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5pDyTRL7rRl"
      },
      "source": [
        "context = (\n",
        "    'Первая многоразовая ступень ракеты-носителя Falcon 9 успешно отделилась через две с половиной '\n",
        "    'минуты после старта и автоматически приземлилась на плавучую платформу Of Course I Still '\n",
        "    'Love You у берегов Флориды. Через 12 минут после запуска космический корабль Crew Dragon '\n",
        "    'вышел на расчетную орбиту и отделился от второй ступени ракеты.'\n",
        "    '\\n\\n'\n",
        "    'Сближение корабля Crew Dragon с Международной космической станцией запланировано на 31 мая. '\n",
        "    'К стыковочному адаптеру на узловом модуле «Гармония» американского сегмента МКС Crew Dragon '\n",
        "    'должен причалить в ручном или, при необходимости, в автоматическом режиме. Эта процедура '\n",
        "    'запланирована на 10:29 по времени Восточного побережья США (17:29 по московскому времени).'\n",
        "    '\\n\\n'\n",
        "    'В испытательном полете DM2 астронавт Херли является командиром космического корабля (spacecraft '\n",
        "    'commander), а его напарник Бенкен — командир по операциям стыковки и расстыковки (joint '\n",
        "    'operations commander). Фактически это означает, что именно Херли управляет Crew Dragon в '\n",
        "    'полете к МКС, к которой они должны пристыковаться в течение суток после старта. Херли и Бенкен '\n",
        "    'также будут выполнять необходимые для сертификации НАСА проверки систем корабля в полете.'\n",
        "    '\\n\\n'\n",
        "    'Во время полета Херли и Бенкен провели небольшую экскурсию по Crew Dragon.'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tVX9PJ_GPE-"
      },
      "source": [
        "And here is how to use deeppavlov's model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BDo1IjGFPG"
      },
      "source": [
        "question = 'Когда отделилась первая ступень?'\n",
        "model_ru([context], [question])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyRAYAc_GxAL"
      },
      "source": [
        "The model returns list with answer, answer starting position in context and the answer logit.\n",
        "\n",
        "This yields the following `answer_question` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXi5hm_AEFB4"
      },
      "source": [
        "def answer_question_ru(question, context):\n",
        "    output = model_ru([context], [question])\n",
        "    return output[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wfCi3FvHBuL"
      },
      "source": [
        "Let's ask a bunch of other questions to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98PIvy4g7rRm"
      },
      "source": [
        "question = 'На какую дату запланирована стыковка?'\n",
        "answer = answer_question_ru(question, context)\n",
        "print(f'Ответ: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1BK3PAm7rRn"
      },
      "source": [
        "question = 'Кто участвует в полете?'\n",
        "answer = answer_question_ru(question, context)\n",
        "print(f'Ответ: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugo2Wyd57rRn"
      },
      "source": [
        "question = 'Кто участвует в полете кроме астронавта Херли?'\n",
        "answer = answer_question_ru(question, context)\n",
        "print(f'Ответ: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B4vytlCYTvs"
      },
      "source": [
        "question = 'Какие астронавты участвовали в полете?'\n",
        "answer = answer_question_ru(question, context)\n",
        "\n",
        "# Notice how model finds the appropriate answer dispite slightly different context.\n",
        "print(f'Ответ: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-AwQsIU7rRn"
      },
      "source": [
        "question = 'Какая ступень приземлилась на плавучую платформу Of Course I Still Love You?'\n",
        "answer = answer_question_ru(question, context)\n",
        "print(f'Ответ: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU4ZxB6o7rRp"
      },
      "source": [
        "## Part 4. Question answering with speech using Tacotron 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVstcIVxadEr"
      },
      "source": [
        "### Text to speech using Tacotron 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoj8MMIJbTji"
      },
      "source": [
        "Tacotron 2 is a network proposed in 2017 in [Natural TTS Synthesis By Conditioning\n",
        "Wavenet On Mel Spectrogram Predictions](https://arxiv.org/pdf/1712.05884.pdf) paper. This network takes an input text and maps it into the mel-frequency spectrogram. This spectrogram is then passed through a modified WaveNet (generative model for audio, original paper can be found [here](https://arxiv.org/pdf/1609.03499.pdf)) to generate the actual speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btil2I1zejk3"
      },
      "source": [
        "Let's look more closely at a mel spectrogram (for more info on its nature please refer to the [Tacotron 2 paper](https://arxiv.org/pdf/1712.05884.pdf))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDAPjR_Lx-uZ"
      },
      "source": [
        "assert tacotron2 is not None and waveglow is not None, 'Tacotron2 by NVIDIA infers only on GPU, so the Part 4 will not work on CPU-only machine'\n",
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
        "\n",
        "text = 'Some test text.'\n",
        "sequences, lengths = utils.prepare_input_sequence([text])\n",
        "with torch.no_grad():\n",
        "    mel, _, _ = tacotron2.infer(sequences, lengths)\n",
        "\n",
        "sns.reset_orig()\n",
        "plt.imshow(mel[0].cpu().numpy())\n",
        "plt.title('mel-frequency spectrogram');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9odRMQS3fISF"
      },
      "source": [
        "After obtaining this spectrogram, we can generate the audio with `waveglow` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdFfCjmsUUxQ"
      },
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sampling_rate = 22050\n",
        "\n",
        "with torch.no_grad():\n",
        "    audio = waveglow.infer(mel)\n",
        "\n",
        "audio_numpy = audio[0].cpu().numpy()\n",
        "Audio(audio_numpy, rate=sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0mnDWpdhpdi"
      },
      "source": [
        "We've generated a `.wav` format audio. We can save it using the `scipy.io.wavfile.write`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTMNu9Krh2cW"
      },
      "source": [
        "from scipy.io.wavfile import write\n",
        "\n",
        "write('audio.wav', sampling_rate, audio_numpy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pjEZy4TfT3w"
      },
      "source": [
        "This yields the following `text_to_speech` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEvDynkPVscj"
      },
      "source": [
        "def text_to_speech(text):\n",
        "    # preprocessing\n",
        "    sequences, lengths = utils.prepare_input_sequence([text])\n",
        "\n",
        "    # run the models\n",
        "    with torch.no_grad():\n",
        "        mel, _, _ = tacotron2.infer(sequences, lengths)\n",
        "        audio = waveglow.infer(mel)\n",
        "\n",
        "    audio_numpy = audio[0].cpu().numpy()\n",
        "    return audio_numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwqwZoxsWNmq"
      },
      "source": [
        "text = 'Another test text.'\n",
        "audio_numpy = text_to_speech(text)\n",
        "Audio(audio_numpy, rate=sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYittYwlfZfU"
      },
      "source": [
        "### Tying text to speech with question answering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83WLbVnv7rRq"
      },
      "source": [
        "Let's take a look at [Mail.ru group blog post on Computer Vision on habr.com](https://habr.com/ru/company/mailru/blog/467905/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2eMzsMY7rRq"
      },
      "source": [
        "context = (\n",
        "    'One of Mail.ru Cloud’s objectives is to provide the handiest means for accessing '\n",
        "    'and searching your own photo and video archives. For this purpose, we at Mail.ru '\n",
        "    'Computer Vision Team have created and implemented systems for smart image '\n",
        "    'processing: search by object, by scene, by face, etc. Another spectacular '\n",
        "    'technology is landmark recognition. Today, I am going to tell you how we made '\n",
        "    'this a reality using Deep Learning.'\n",
        "    '\\n\\n'\n",
        "    'Imagine the situation: you return from your vacation with a load of photos. Talking '\n",
        "    'to your friends, you are asked to show a picture of a place worth seeing, like '\n",
        "    'palace, castle, pyramid, temple, lake, waterfall, mountain, and so on. You rush to '\n",
        "    'scroll your gallery folder trying to find one that is really good. Most likely, it '\n",
        "    'is lost amongst hundreds of images, and you say you will show it later.'\n",
        "    '\\n\\n'\n",
        "    'We solve this problem by grouping user photos in albums. This will let you find '\n",
        "    'pictures you need just in few clicks. Now we have albums compiled by face, by '\n",
        "    'object and by scene, and also by landmark.'\n",
        "    '\\n\\n'\n",
        "    'Photos with landmarks are essential because they often capture highlights of our '\n",
        "    'lives (journeys, for example). These can be pictures with some architecture or '\n",
        "    'wilderness in the background. This is why we seek to locate such images and make '\n",
        "    'them readily available to users.'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tkwZk-B7rRq"
      },
      "source": [
        "question = 'Why photos with landmarks are essential?'\n",
        "answer = answer_question(question, context)\n",
        "print(f'Answer: \"{answer}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chBm8WdIh_Bc"
      },
      "source": [
        "Let's cat question and answer into one phrase and convert it to audio!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9NhT0Np7rRr"
      },
      "source": [
        "text = f'{question}\\n{answer}'\n",
        "audio_numpy = text_to_speech(text)\n",
        "Audio(audio_numpy, rate=sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFDgT4OuijIp"
      },
      "source": [
        "And another one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iqcYLwgy1nT"
      },
      "source": [
        "question = \"Which places except mountain are worth seeing?\"\n",
        "answer = answer_question(question, context)\n",
        "print(f'Answer: \"{answer}\"')\n",
        "\n",
        "text = f'{question}\\n{answer}'\n",
        "audio_numpy = text_to_speech(text)\n",
        "Audio(audio_numpy, rate=sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIUIia_G7rRs"
      },
      "source": [
        "# Take your time, experiment with questions and the generated audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv5jEpkw7rRs"
      },
      "source": [
        "# 5. Russian langugage speech generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0uhvcBhj2k6"
      },
      "source": [
        "Of course, text to speech is not specific to english language. Here is how you can do it with russian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD79JX0g7rRs"
      },
      "source": [
        "from omegaconf import OmegaConf\n",
        "\n",
        "torch.hub.download_url_to_file(\n",
        "    'https://raw.githubusercontent.com/snakers4/silero-models/master/models.yml',\n",
        "    'latest_silero_models.yml',\n",
        "    progress=False\n",
        ")\n",
        "models = OmegaConf.load('latest_silero_models.yml')\n",
        "\n",
        "# see latest avaiable models\n",
        "available_languages = list(models['tts_models'].keys())\n",
        "print(f'Available languages {available_languages}')\n",
        "\n",
        "for lang in available_languages:\n",
        "    speakers = list(models['tts_models'][lang].keys())\n",
        "    print(f'Available speakers for {lang}: {speakers}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVaR1xG8k94K"
      },
      "source": [
        "Let's choose our language and speaker and try using them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKCT72on7rRt"
      },
      "source": [
        "language = 'ru'\n",
        "speaker = 'kseniya_16khz'\n",
        "device = torch.device('cpu')\n",
        "model, symbols, sample_rate, example_text, apply_tts = torch.hub.load(\n",
        "    'snakers4/silero-models', 'silero_tts',\n",
        "    language=language, speaker=speaker\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "audio = apply_tts(\n",
        "    texts=[example_text],\n",
        "    model=model,\n",
        "    sample_rate=sample_rate,\n",
        "    symbols=symbols,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(example_text)\n",
        "Audio(audio[0], rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0O3eCX87rRt"
      },
      "source": [
        "audio = apply_tts(\n",
        "    texts=[\"Дерзайте знать! Спасибо за внимание!\"],\n",
        "    model=model,\n",
        "    sample_rate=sample_rate,\n",
        "    symbols=symbols,\n",
        "    device=device\n",
        ")\n",
        "Audio(audio[0], rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2_1sCYvALxG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}