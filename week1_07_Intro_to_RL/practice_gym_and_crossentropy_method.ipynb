{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "seminar_gym_interface.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuXufW6hAlo-"
   },
   "source": [
    "# Practice: gym interface and crossentropy method\n",
    "\n",
    "_Reference:_ This notebook is based on Practical RL [week01](https://github.com/yandexdataschool/Practical_RL/tree/master/week01_intro)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Aw_mjq4C9i-N"
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ[\"DISPLAY\"] = \":1\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftOrVNgg9i-O"
   },
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mg6tdsNtA9hl"
   },
   "source": [
    "We're gonna spend several next weeks learning algorithms that solve decision processes. We are then in need of some interesting decision problems to test our algorithms.\n",
    "\n",
    "That's where OpenAI Gym comes into play. It's a Python library that wraps many classical decision problems including robot control, videogames and board games.\n",
    "\n",
    "So here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qgUa6Yz89i-Q"
   },
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "plt.imshow(env.render(\"rgb_array\"));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRTK8yeU9i-R"
   },
   "source": [
    "### Gym interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMWiCMVIVPTI"
   },
   "source": [
    "The three main methods of an environment are\n",
    "* `reset()`: reset environment to the initial state, and _return it_\n",
    "* `render()`: show current environment state (a more colorful version)\n",
    "* `step(a)`: commit action `a` and return `(new_state, reward, is_done, info)`\n",
    " * `new_state`: the new state right after committing the action `a`\n",
    " * `reward`: a number representing your reward for committing action `a`\n",
    " * `is_done`: True if the MDP has just finished, False if still in progress\n",
    " * `info`: some auxiliary stuff about what just happened. For now, ignore it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AhGD7yXr9i-R"
   },
   "source": [
    "state = env.reset()\n",
    "print(\"initial state:\", state)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9WYWMRvrl5r"
   },
   "source": [
    "In MountainCar, observation is just two numbers: car position and velocity.\n",
    "\n",
    "Let's take action 2, which stands for \"go right\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pSs_w9Lu9i-S"
   },
   "source": [
    "print(\"taking action 2 (right)\")\n",
    "new_state, reward, is_done, _ = env.step(2)\n",
    "\n",
    "print(\"new state:\", new_state)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1CLfCf6rpx8"
   },
   "source": [
    "As you can see, the car has moved to the right slightly (around 0.0005)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS56PCMa9i-S"
   },
   "source": [
    "### Play with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrL9_D56VXSI"
   },
   "source": [
    "Below is the code that drives the car to the right. However, if you simply use the default policy, the car will not reach the flag at the far right due to gravity.\n",
    "\n",
    "__Your task__ is to fix it. Find a strategy that reaches the flag. \n",
    "\n",
    "You are not required to build any sophisticated algorithms for now, and you definitely don't need to know any reinforcement learning for this. Feel free to hard-code :)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_CaEYOrf9i-T"
   },
   "source": [
    "actions = {\"left\": 0, \"stop\": 1, \"right\": 2}\n",
    "\n",
    "\n",
    "def policy(state, time_step):\n",
    "    # Write the code for your policy here. You can use the current state\n",
    "    # (a tuple of position and velocity), the current time step, or both,\n",
    "    # if you want.\n",
    "    position, velocity = state\n",
    "\n",
    "    # This is an example policy. You can try running it, but it will not work.\n",
    "    # Your goal is to fix that. You don't need anything sophisticated here,\n",
    "    # and you can hard-code any policy that seems to work.\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: think how you would make a swing go farther and faster.\n",
    "    return actions[\"right\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k02kqprA9i-T"
   },
   "source": [
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "time_limit = 250\n",
    "for time_step in range(time_limit):\n",
    "    # Choose action based on your policy.\n",
    "    action = policy(state, time_step)\n",
    "\n",
    "    # Pass the action to the environment.\n",
    "    state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # We don't do anything with reward here because MountainCar is a very\n",
    "    # simple environment, and reward is a constant -1 (meaning that your\n",
    "    # goal is to end the episode as quickly as possible).\n",
    "\n",
    "    # Draw game image on display.\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render(\"rgb_array\"))\n",
    "    plt.show()\n",
    "\n",
    "    if done:\n",
    "        print(\"Well done!\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Time limit exceeded. Try again.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LYwulaIBZeH"
   },
   "source": [
    "## Crossentropy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7eHzR_9Bm2K"
   },
   "source": [
    "Now that we know how does the `gym` work, let's try and solve a more complicated problem using the crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "boirRXNq--s-"
   },
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU66aOiPdVVY"
   },
   "source": [
    "As `Taxi-v3` is a much more sophisticated environment, it presents us with more possible states and actions at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O9l4J5bpB04k"
   },
   "source": [
    "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "print(f\"n_states={n_states}, n_actions={n_actions}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxLN1ZxmdxcI"
   },
   "source": [
    "That's definitely a lot. Way too much to hard-code as we did with previous problem. Let's use the crossentropy method on this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FExaEKW-Vf2j"
   },
   "source": [
    "### Create stochastic policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3B63b1fVl6C"
   },
   "source": [
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s, a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_TzXJVHVlY3"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def initialize_policy(n_states, n_actions):\n",
    "    # YOUR CODE HERE\n",
    "    # Create an array to store action probabilities\n",
    "\n",
    "    return policy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a99xkrpd0pbX"
   },
   "source": [
    "policy = initialize_policy(n_states, n_actions)\n",
    "assert isinstance(policy, np.ndarray)\n",
    "assert np.allclose(policy, 1 / n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THBVPqvOW2OA"
   },
   "source": [
    "### Play the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOgE_1xMXQ61"
   },
   "source": [
    "Let's play the game just like before, however this time we will also record states, actions and rewards to use them in training loop."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6QBRMMZeW38F"
   },
   "source": [
    "def generate_session(env, policy, time_limit=10 ** 4):\n",
    "    state = env.reset()\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.0\n",
    "    for _ in range(time_limit):\n",
    "        # YOUR CODE HERE\n",
    "        # Choose action based on policy and take it.\n",
    "        # Hint: you can use np.random.choice for sampling action\n",
    "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "\n",
    "        # Record information we just got from the environment.\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PFF98gn8Xnme"
   },
   "source": [
    "states, actions, reward = generate_session(env, policy)\n",
    "assert isinstance(states, list) and isinstance(actions, list)\n",
    "assert len(states) == len(actions)\n",
    "assert isinstance(reward, float)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3KlOA04YCpj"
   },
   "source": [
    "Let's see the initial reward distribution for our \"naive\" policy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HP5VOP15X59V"
   },
   "source": [
    "sample_rewards = [generate_session(env, policy, time_limit=1000)[2] for _ in range(200)]\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color=\"green\")\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color=\"red\")\n",
    "plt.legend();"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2Hbt1n1YK9G"
   },
   "source": [
    "### Crossentropy method step"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p5WSk58tYUpp"
   },
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile.\n",
    "\n",
    "    Compute minimum reward for session to be elite and choose elite states\n",
    "    and actions based on this threshold.\n",
    "\n",
    "    Note that states_batch and actions_batch are both 2d lists, i.e. lists\n",
    "    containing lists of states and actions from each session in batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: use np.precentile to compute the threshold\n",
    "\n",
    "    return elite_states, elite_actions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S_779hM5eozQ"
   },
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],  # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],  # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],  # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],  # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_30 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) and np.all(\n",
    "    test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "), \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1]) and np.all(\n",
    "    test_result_30[1] == [3, 2, 0, 1, 3, 3]\n",
    "), \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and np.all(\n",
    "    test_result_90[1] == [3, 3]\n",
    "), \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and np.all(\n",
    "    test_result_100[1] == [3, 3]\n",
    "), \"Please make sure you use >=, not >. Also double-check how you compute percentile.\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PMQ00-p6UN3v"
   },
   "source": [
    "def get_new_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given a list of elite states/actions from select_elites, return a new\n",
    "    policy where each action probability is proportional to\n",
    "\n",
    "        policy[s, a] ~ #[occurrences of s and a in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize the policy to get valid probabilities.\n",
    "    For states that you never visited, use a uniform distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Set probabilities for actions given elite states & actions.\n",
    "    # Don't forget to set 1/n_actions for all actions in unvisited states.\n",
    "\n",
    "    return new_policy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "As4H-gOXUg5R"
   },
   "source": [
    "elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\n",
    "elite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "new_policy = get_new_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(\n",
    "    new_policy\n",
    ").all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(new_policy >= 0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(\n",
    "    new_policy.sum(axis=-1), 1\n",
    "), \"Your new policy should be a valid probability distribution over actions\"\n",
    "\n",
    "reference_answer = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.5, 0.0, 0.0, 0.5, 0.0],\n",
    "        [0.0, 0.33333333, 0.66666667, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    ]\n",
    ")\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4SYwLlztaLv"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3E8Jf0ttdUw"
   },
   "source": [
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T8ds_27BtEKV"
   },
   "source": [
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress.\n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    mean_rewards = [mean_reward for mean_reward, threshold in log]\n",
    "    reward_thresholds = [threshold for mean_reward, threshold in log]\n",
    "    plt.plot(mean_rewards, label=\"Mean rewards\")\n",
    "    plt.plot(reward_thresholds, label=\"Reward thresholds\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines(\n",
    "        [np.percentile(rewards_batch, percentile)],\n",
    "        ymin=[0],\n",
    "        ymax=[100],\n",
    "        label=\"percentile\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"mean reward = {mean_reward:.3f}, threshold={threshold:.3f}\")\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-uh8JpOp0yXL"
   },
   "source": [
    "# reset policy\n",
    "policy = initialize_policy(n_states, n_actions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XWwgfYHB00dE"
   },
   "source": [
    "n_sessions = 250  # sample this many sessions\n",
    "percentile = 50  # take this percent of session with highest rewards\n",
    "learning_rate = 0.5  # how quickly the policy is updated, on a scale from 0 to 1\n",
    "\n",
    "log = []\n",
    "\n",
    "for _ in range(100):\n",
    "    # YOUR CODE HERE\n",
    "    # Generate a list of n_sessions new sessions, select elites and compute\n",
    "    # new policy based on them. After that update the existing policy wrt\n",
    "    # learning rate.\n",
    "    # Note: use functions we just defined.\n",
    "\n",
    "    # display results on chart\n",
    "    show_progress(rewards_batch, log, percentile)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlIrztK22HqI"
   },
   "source": [
    "### Analysing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hehsmk982J70"
   },
   "source": [
    "You may have noticed that the taxi problem quickly converges from very little values to a near-optimal score and then descends back. This is caused (at least in part) by the innate randomness of the environment. Namely, the starting points of passenger/driver change from episode to episode.\n",
    "\n",
    "In such case if crossentropy policy failed to learn how to win from one distinct starting point, it will simply discard it because no sessions from that starting point will make it into the \"elites\".\n",
    "\n",
    "To mitigate that problem, you can either reduce the threshold for elite sessions (duct tape way) or change the way you evaluate strategy (theoretically correct way). For each starting state, you can sample an action randomly, and then evaluate this action by running several games starting from it and averaging the total reward. Choosing elite sessions with this kind of sampling (where each session's reward is counted as the average of the rewards of all sessions with the same starting state and action) should improve the performance of your policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTClEj9c3JrE"
   },
   "source": [
    "## Deeging deeper: approximate crossentropy with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZFlyLJf3TAA"
   },
   "source": [
    "In this section we'll extend your CEM implementation with neural networks! You will train a multi-layer neural network to solve simple continuous state space games.\n",
    "\n",
    "![img](https://watanimg.elwatannews.com/old_news_images/large/249765_Large_20140709045740_11.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eS6CtVHR1mRR"
   },
   "source": [
    "# .env is to remove auto-assigned time limit wrapper\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "print(\"state vector dim =\", state_dim)\n",
    "print(\"n_actions =\", n_actions)\n",
    "plt.imshow(env.render(\"rgb_array\"));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiAvTBy8r9qd"
   },
   "source": [
    "Here, just like in a `MountainCar-v0`, we will be controlling a cart, which we can move right or left. However our goal here is different. In this environment we want to keep pole attached to the top of our cart from falling as long as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udRr7zCW4F9B"
   },
   "source": [
    "### Neural Network Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XXRmz-B4I7L"
   },
   "source": [
    "For this assignment we'll utilize the simplified neural network implementation from [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Here's what you'll need:\n",
    "* `agent.partial_fit(states, actions)` - make a single training pass over the data to increase the probability of provided `actions` in provided `states`\n",
    "* `agent.predict_proba(states)` - predict probabilities of all actions, a matrix of shape `[len(states), n_actions] `"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e3-zE4Yp3uV8"
   },
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "agent = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 20),\n",
    "    activation=\"tanh\",\n",
    ")\n",
    "\n",
    "# initialize agent to the dimension of state space and number of actions\n",
    "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO3tM6wFtZr_"
   },
   "source": [
    "Despite the apparent differences, you will find the training procedure for such agent to be very similar to the one we used in the previous part. We won't even need to rewrite most of our helper functions at all! However, one thing that has changed is the way we get actions' probabilities. So let's adapt our `generate_session` function to this new agent-based policy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "glqypTJL6PmE"
   },
   "source": [
    "def generate_session(env, agent, time_limit=10 ** 4):\n",
    "    state = env.reset()\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.0\n",
    "    for _ in range(time_limit):\n",
    "        # YOUR CODE HERE\n",
    "        # Use agent to predict a vector of action probabilities for current\n",
    "        # state and use the probabilities you predicted to pick an action.\n",
    "        # Sample actions, don't just take the most likely one!\n",
    "        # Hint: you can use np.random.choice for sampling once more\n",
    "\n",
    "        # Record information we just got from the environment.\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K6-YrqLP6L-1"
   },
   "source": [
    "states, actions, reward = generate_session(env, agent, time_limit=5)\n",
    "print(\"states:\", np.stack(states))\n",
    "print(\"actions:\", actions)\n",
    "print(\"reward:\", reward)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9cj1aiN7cH-"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iFpPGkXL7nCa"
   },
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "\n",
    "log = []\n",
    "\n",
    "for _ in range(100):\n",
    "    # YOUR CODE HERE\n",
    "    # Generate new sessions, select elites and update our agent.\n",
    "    # Note: use functions we just defined.\n",
    "\n",
    "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
    "\n",
    "    if np.mean(rewards_batch) > 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3QsLwz38h9h"
   },
   "source": [
    "### Analysing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k5Me6LS8lEt"
   },
   "source": [
    "Let's record a video of our agent playing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XvsGIMiA8Ka1"
   },
   "source": [
    "import gym.wrappers\n",
    "\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor, agent) for _ in range(100)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VPoe8GIu8rQx"
   },
   "source": [
    "from base64 import b64encode\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "video_paths = sorted([file for file in Path(\"videos\").iterdir() if file.suffix == \".mp4\"])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open(\"rb\") as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g2xYT2X8CbJ"
   },
   "source": [
    "## Bonus area I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBkwLd_58CbJ"
   },
   "source": [
    "### Tabular crossentropy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_iIe6DV8CbJ"
   },
   "source": [
    "You may have noticed that the taxi problem quickly converges from -100 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKozzVzv8CbJ"
   },
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXQ6iosS8CbJ"
   },
   "source": [
    "- __1.1__ (1 pts) Find out how the algorithm performance changes if you use a different `percentile` and/or `n_sessions`.\n",
    "- __1.2__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "\n",
    "It's okay to modify the existing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CoIv-iA8CbJ"
   },
   "source": [
    "```<Describe what you did here.  Preferably with plot/report to support it.>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjrlkfiO8CbJ"
   },
   "source": [
    "## Bonus area II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azP4BNni8CbJ"
   },
   "source": [
    "### Deep crossentropy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb6D8lnY8CbJ"
   },
   "source": [
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to try something harder.\n",
    "\n",
    "> Note: if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9lXNqxr8CbJ"
   },
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuaMOyAv8CbJ"
   },
   "source": [
    "* __2.1__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2.\n",
    "  * For MountainCar, get average reward of __at least -150__\n",
    "  * For LunarLander, get average reward of __at least +50__\n",
    "\n",
    "See the tips section below, it's kinda important.\n",
    "__Note:__ If your agent is below the target score, you'll still get most of the points depending on the result, so don't be afraid to submit it.\n",
    "  \n",
    "  \n",
    "* __2.2__ (bonus: 4++ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  * __Please list what you did in anytask submission form__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2zYbN168CbK"
   },
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnHEAoOq8CbK"
   },
   "source": [
    "* Gym page: [MountainCar](https://gym.openai.com/envs/MountainCar-v0), [LunarLander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "\n",
    "You may find the following snippet useful:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fd7hMAUj8CbK"
   },
   "source": [
    "def visualize_mountain_car(env, agent):\n",
    "    xs = np.linspace(env.min_position, env.max_position, 100)\n",
    "    vs = np.linspace(-env.max_speed, env.max_speed, 100)\n",
    "    grid = np.dstack(np.meshgrid(xs, vs)).transpose(1, 0, 2)\n",
    "    grid_flat = grid.reshape(len(xs) * len(vs), 2)\n",
    "    probs = agent.predict_proba(grid_flat).reshape(len(xs), len(vs), 3)\n",
    "    return probs\n",
    "\n",
    "\n",
    "plt.imshow(visualize_mountain_car(env, agent))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSGUg0FT8CbK"
   },
   "source": [
    "### More bonus tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGbohUzw8CbK"
   },
   "source": [
    "* __2.3 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation. If you attempted this task, please mention it in anytask submission._)\n",
    "\n",
    "* __2.4 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * Start with [\"Pendulum-v0\"](https://github.com/openai/gym/wiki/Pendulum-v0).\n",
    "  * Since your agent only predicts the \"expected\" action, you will have to add noise to ensure exploration.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) \n",
    "  * 4 points for solving. Slightly less for getting some results below solution threshold. Note that discrete and continuous environments may have slightly different rules aside from action spaces."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jhPikS1q1kAT"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}
