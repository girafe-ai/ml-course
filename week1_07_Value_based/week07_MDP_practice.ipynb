{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week07: Value based\n",
    "### Markov decision process\n",
    "_Reference: based on Practical RL_ [week02](https://github.com/yandexdataschool/Practical_RL/tree/master/week02_value_based)\n",
    "\n",
    "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
    "\n",
    "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
    "\n",
    "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/blob/36a0b58261acde756abd55306fbe63df226bf62b/hw2/HW2.ipynb) _by Berkeley_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's define a simple MDP from this picture:\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/800px-Markov_Decision_Process.svg.png' width=300px>\n",
    "_img by MistWiz (Own work) [Public domain], via Wikimedia Commons_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from mdp import MDP, FrozenLakeEnv, has_graphviz\n",
    "from mdp_get_action_value import get_action_value\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:32.642838Z",
     "start_time": "2018-04-02T13:44:32.545142Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you Colab, uncomment this please\n",
    "# !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring19/week02_value_based/mdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:32.642838Z",
     "start_time": "2018-04-02T13:44:32.545142Z"
    }
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "# fmt: on\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use MDP just as any other gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:34.203384Z",
     "start_time": "2018-04-02T13:44:34.199297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it also has other methods that you'll need for Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:34.956122Z",
     "start_time": "2018-04-02T13:44:34.949856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Visualizing MDPs\n",
    "\n",
    "You can also visualize any MDP with the drawing fuction donated by [neer201](https://github.com/neer201).\n",
    "\n",
    "You have to install graphviz for system and for python. For __ubuntu__ just run:\n",
    "\n",
    "1. `sudo apt-get install graphviz`\n",
    "2. `pip install graphviz`\n",
    "3. restart the notebook\n",
    "\n",
    "For __macOs__ use `brew` instead of `sudo apt-get`. \n",
    "\n",
    "__Note:__ Installing graphviz on some OS (esp. Windows) may be tricky. However, you can ignore this part alltogether and use the standart vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:37.797182Z",
     "start_time": "2018-04-02T13:44:37.794073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphviz available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Graphviz available:\", has_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:38.715883Z",
     "start_time": "2018-04-02T13:44:38.648684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: MDP Pages: 1 -->\n",
       "<svg width=\"720pt\" height=\"196pt\"\n",
       " viewBox=\"0.00 0.00 720.00 195.51\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(.7008 .7008) rotate(0) translate(4 275)\">\n",
       "<title>MDP</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-275 1023.463,-275 1023.463,4 -4,4\"/>\n",
       "<!-- s0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>s0</title>\n",
       "<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"40\" cy=\"-102\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"40\" cy=\"-102\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-95.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">s0</text>\n",
       "</g>\n",
       "<!-- s0&#45;a0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>s0&#45;a0</title>\n",
       "<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"193.5772\" cy=\"-146\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.5772\" y=\"-141\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a0</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>s0&#45;&gt;s0&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M79.2844,-110.7026C99.8974,-115.5498 125.4811,-122.0095 148,-129 151.327,-130.0328 154.7652,-131.1741 158.1924,-132.3622\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"157.1257,-135.698 167.7202,-135.7831 159.4912,-129.1097 157.1257,-135.698\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>s0&#45;a1</title>\n",
       "<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"193.5772\" cy=\"-219\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.5772\" y=\"-214\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a1</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>s0&#45;&gt;s0&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M66.6878,-131.9984C76.104,-141.7862 87.0765,-152.3514 98,-161 117.3546,-176.324 141.0693,-190.7377 160.0344,-201.3632\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"158.4757,-204.5005 168.923,-206.2624 161.8547,-198.37 158.4757,-204.5005\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>s0&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M166.0433,-141.6398C146.7591,-138.2473 120.5165,-132.9481 98,-126 94.1846,-124.8227 90.2733,-123.4907 86.3715,-122.0724\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"87.3439,-118.6975 76.753,-118.4084 84.852,-125.239 87.3439,-118.6975\"/>\n",
       "<text text-anchor=\"middle\" x=\"123\" y=\"-144.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.5</text>\n",
       "</g>\n",
       "<!-- s2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>s2</title>\n",
       "<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"433.1543\" cy=\"-158\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"433.1543\" cy=\"-158\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"433.1543\" y=\"-151.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">s2</text>\n",
       "</g>\n",
       "<!-- s0&#45;a0&#45;&gt;s2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>s0&#45;a0&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M221.5286,-147.4C261.0547,-149.3798 333.9202,-153.0295 382.9604,-155.4859\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"382.8803,-158.9862 393.0429,-155.9909 383.2305,-151.9949 382.8803,-158.9862\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.1543\" y=\"-160.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.5</text>\n",
       "</g>\n",
       "<!-- s2&#45;a0 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>s2&#45;a0</title>\n",
       "<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"666.7315\" cy=\"-129\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"666.7315\" y=\"-124\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a0</text>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;s2&#45;a0 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>s2&#45;&gt;s2&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M473.5103,-157.8232C511.8415,-156.9917 570.9151,-153.9822 621.1543,-144 624.3408,-143.3669 627.6142,-142.5785 630.8741,-141.6969\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"631.9229,-145.0366 640.5126,-138.8344 629.93,-138.3263 631.9229,-145.0366\"/>\n",
       "</g>\n",
       "<!-- s2&#45;a1 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>s2&#45;a1</title>\n",
       "<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"666.7315\" cy=\"-56\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"666.7315\" y=\"-51\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a1</text>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;s2&#45;a1 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>s2&#45;&gt;s2&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M459.0758,-127.434C468.3559,-118.1046 479.4406,-108.5837 491.1543,-102 534.782,-77.4792 591.8554,-65.6786 628.8989,-60.2529\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"629.8226,-63.6588 639.2512,-58.8264 628.867,-56.7243 629.8226,-63.6588\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>s0&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M220.8397,-213.4876C256.5286,-206.0905 320.9236,-192.1221 375.1543,-177 378.5225,-176.0608 381.9861,-175.048 385.4645,-173.9962\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"386.7297,-177.2682 395.2399,-170.9575 384.6517,-170.5837 386.7297,-177.2682\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.1543\" y=\"-213.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 1</text>\n",
       "</g>\n",
       "<!-- s1 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>s1</title>\n",
       "<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"829.3087\" cy=\"-106\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"829.3087\" cy=\"-106\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"829.3087\" y=\"-99.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">s1</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>s1&#45;a0</title>\n",
       "<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"991.8858\" cy=\"-70\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"991.8858\" y=\"-65\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a0</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1&#45;a0 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>s1&#45;&gt;s1&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M868.6641,-97.2854C894.7641,-91.506 928.8065,-83.9679 954.4945,-78.2797\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"955.5871,-81.6226 964.5939,-76.0433 954.0737,-74.7882 955.5871,-81.6226\"/>\n",
       "</g>\n",
       "<!-- s1&#45;a1 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>s1&#45;a1</title>\n",
       "<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"991.8858\" cy=\"-152\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"991.8858\" y=\"-147\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">a1</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1&#45;a1 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>s1&#45;&gt;s1&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M868.7555,-113.3782C891.6544,-118.0876 920.9044,-124.8376 946.3087,-133 949.8641,-134.1424 953.5293,-135.4461 957.1639,-136.8233\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"956.0315,-140.1394 966.6171,-140.5796 958.6164,-133.6342 956.0315,-140.1394\"/>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>s1&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M969.5089,-53.411C939.3125,-32.711 883.0867,0 829.3087,0 193.5772,0 193.5772,0 193.5772,0 145.8315,0 101.5836,-34.6324 72.9684,-63.537\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"70.0864,-61.4847 65.6803,-71.1198 75.1333,-66.3354 70.0864,-61.4847\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.1543\" y=\"-5.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.7 &#160;reward =5</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s2 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>s1&#45;a0&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M976.3243,-93.2984C972.1185,-100.1282 967.7762,-107.7204 964.3087,-115 953.9716,-136.7008 965.931,-152.1169 946.3087,-166 873.0415,-217.8376 597.2942,-183.133 483.1269,-165.993\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"483.3302,-162.4838 472.918,-164.4422 482.2789,-169.4044 483.3302,-162.4838\"/>\n",
       "<text text-anchor=\"middle\" x=\"741.8087\" y=\"-198.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>s1&#45;a0&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M965.7872,-61.0803C944.1683,-55.1324 912.9482,-49.9157 887.3087,-59 879.6639,-61.7086 872.2908,-65.9014 865.5135,-70.691\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"863.0781,-68.1479 857.2719,-77.0101 867.3373,-73.703 863.0781,-68.1479\"/>\n",
       "<text text-anchor=\"middle\" x=\"916.8087\" y=\"-64.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.1</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>s1&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M975.9063,-175.015C968.1402,-184.234 957.96,-193.8712 946.3087,-199 761.1617,-280.4996 683.2412,-262.4365 491.1543,-199 484.6246,-196.8436 478.1874,-193.6811 472.1124,-190.0532\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"473.7965,-186.9738 463.5022,-184.4679 469.987,-192.8464 473.7965,-186.9738\"/>\n",
       "<text text-anchor=\"middle\" x=\"741.8087\" y=\"-258.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.05</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>s1&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M964.4489,-147.6602C943.147,-143.9487 913.0104,-137.9461 887.3087,-130 883.4939,-128.8206 879.583,-127.4872 875.6816,-126.068\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"876.6544,-122.6932 866.0635,-122.4023 874.1614,-129.2342 876.6544,-122.6932\"/>\n",
       "<text text-anchor=\"middle\" x=\"916.8087\" y=\"-149.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.95</text>\n",
       "</g>\n",
       "<!-- s2&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>s2&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M643.5438,-113.3955C636.633,-109.559 628.8443,-105.9847 621.1543,-104 429.892,-54.638 191.2809,-79.6056 89.5999,-94.0846\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"89.0082,-90.6337 79.6156,-95.5358 90.0151,-97.561 89.0082,-90.6337\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.1543\" y=\"-84.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.4</text>\n",
       "</g>\n",
       "<!-- s2&#45;a0&#45;&gt;s1 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>s2&#45;a0&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M694.3105,-125.0984C717.6285,-121.7995 751.619,-116.9909 779.6586,-113.0241\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"780.2778,-116.4714 789.6889,-111.6051 779.2972,-109.5404 780.2778,-116.4714\"/>\n",
       "<text text-anchor=\"middle\" x=\"741.8087\" y=\"-128.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.6</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s0 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>s2&#45;a1&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M639.1151,-53.6811C570.962,-48.4197 389.3931,-37.211 239.1543,-53 175.3691,-59.7034 159.9312,-66.3263 98,-83 94.6236,-83.909 91.1538,-84.8994 87.671,-85.9351\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"86.4168,-82.659 77.8873,-88.9438 88.4744,-89.3497 86.4168,-82.659\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.1543\" y=\"-58.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.3 &#160;reward =&#45;1</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>s2&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M648.0919,-76.9874C640.4469,-84.3829 631.0663,-92.0732 621.1543,-97 568.4674,-123.188 545.9506,-100.5736 491.1543,-122 485.639,-124.1566 480.0669,-126.8429 474.6705,-129.7699\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"472.7205,-126.8538 465.7954,-134.8722 476.2093,-132.9224 472.7205,-126.8538\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.1543\" y=\"-127.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.4</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>s2&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M693.2142,-64.1446C716.9579,-71.4469 752.3787,-82.3405 781.1324,-91.1836\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"780.4078,-94.6224 790.9948,-94.2167 782.4655,-87.9317 780.4078,-94.6224\"/>\n",
       "<text text-anchor=\"middle\" x=\"741.8087\" y=\"-92.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.3</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fe45c069fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if has_graphviz:\n",
    "    from mdp import (\n",
    "        plot_graph,\n",
    "        plot_graph_optimal_strategy_and_state_values,\n",
    "        plot_graph_with_state_values,\n",
    "    )\n",
    "\n",
    "    display(plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.101416Z",
     "start_time": "2018-04-02T13:43:17.095468Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile mdp_get_action_value.py\n",
    "\n",
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    q_value = None\n",
    "\n",
    "    return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.102247Z",
     "start_time": "2018-04-02T13:43:05.502Z"
    }
   },
   "outputs": [],
   "source": [
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.103358Z",
     "start_time": "2018-04-02T13:43:05.506Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    new_state_value = None\n",
    "\n",
    "    return new_state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.104340Z",
     "start_time": "2018-04-02T13:43:05.510Z"
    }
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 0.69)\n",
    "assert test_Vs == test_Vs_copy, \"please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:09.793405Z",
     "start_time": "2018-04-02T13:44:09.770623Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9  # discount for MDP\n",
    "num_iter = 100  # maximum iterations, excluding initialization\n",
    "min_difference = 0.001  # stop VI if new values are this close to old values (or closer)\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # YOUR CODE HERE: compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    new_state_values = {}\n",
    "\n",
    "    assert isinstance(new_state_values, dict)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()), end='\\n\\n')\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.106395Z",
     "start_time": "2018-04-02T13:43:05.522Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 8.032) < 0.01\n",
    "assert abs(state_values['s1'] - 11.169) < 0.01\n",
    "assert abs(state_values['s2'] - 8.921) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.107338Z",
     "start_time": "2018-04-02T13:43:05.525Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    selected_action = None\n",
    "\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.108149Z",
     "start_time": "2018-04-02T13:43:05.530Z"
    }
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:05.017823Z",
     "start_time": "2018-04-02T13:44:04.962755Z"
    }
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    try:\n",
    "        display(plot_graph_optimal_strategy_and_state_values(mdp, state_values))\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Run the cell that starts with \\\"%%writefile mdp_get_action_value.py\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.110002Z",
     "start_time": "2018-04-02T13:43:05.538Z"
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "state = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    state, reward, done, _ = mdp.step(get_optimal_action(mdp, state_values, state, gamma))\n",
    "    rewards.append(reward)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert 0.85 < np.mean(rewards) < 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.110991Z",
     "start_time": "2018-04-02T13:43:05.541Z"
    }
   },
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.111919Z",
     "start_time": "2018-04-02T13:43:05.545Z"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # YOUR CODE HERE: compute new state values using the functions you defined above.\n",
    "        # It must be a dict {state : new_V(state)}\n",
    "        new_state_values = {}\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "\n",
    "        print(\n",
    "            \"iter {:4.0f}   |   diff: {:6.5f}   |   V(start): {:.3f} \".format(\n",
    "                i, diff, new_state_values[mdp._initial_state]\n",
    "            )\n",
    "        )\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.112871Z",
     "start_time": "2018-04-02T13:43:05.548Z"
    }
   },
   "outputs": [],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.114062Z",
     "start_time": "2018-04-02T13:43:05.552Z"
    }
   },
   "outputs": [],
   "source": [
    "state = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    action = get_optimal_action(mdp, state_values, state, gamma)\n",
    "    print(action, end='\\n\\n')\n",
    "    state, reward, done, _ = mdp.step(action)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize!\n",
    "\n",
    "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.115092Z",
     "start_time": "2018-04-02T13:43:05.556Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h) - 0.5)\n",
    "    ax.set_yticks(np.arange(w) - 0.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            # fmt: off\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12, verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            # fmt: on\n",
    "            a = Pi[y, x]\n",
    "            if a is None:\n",
    "                continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u * 0.3, -v * 0.3, color='m', head_width=0.1, head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.116164Z",
     "start_time": "2018-04-02T13:43:05.560Z"
    }
   },
   "outputs": [],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.117143Z",
     "start_time": "2018-04-02T13:43:05.563Z"
    }
   },
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_rewards(mdp):\n",
    "    \"\"\" computes total rewards for given mdp \"\"\"\n",
    "    state_values = value_iteration(mdp)\n",
    "    total_rewards = []\n",
    "    for game_i in range(1000):\n",
    "        state = mdp.reset()\n",
    "        rewards = []\n",
    "        for t in range(100):\n",
    "            state, reward, done, _ = mdp.step(get_optimal_action(mdp, state_values, state, gamma))\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        total_rewards.append(np.sum(rewards))\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.118218Z",
     "start_time": "2018-04-02T13:43:05.568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "total_rewards = compute_total_rewards(mdp)\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert 1.0 <= np.mean(total_rewards) <= 1.0\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.119075Z",
     "start_time": "2018-04-02T13:43:05.571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "total_rewards = compute_total_rewards(mdp)\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert 0.8 <= np.mean(total_rewards) <= 0.95\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.120316Z",
     "start_time": "2018-04-02T13:43:05.574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "total_rewards = compute_total_rewards(mdp)\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert 0.6 <= np.mean(total_rewards) <= 0.7\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.121544Z",
     "start_time": "2018-04-02T13:43:05.578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "total_rewards = compute_total_rewards(mdp)\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert 0.6 <= np.mean(total_rewards) <= 0.8\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1 - find an MDP for which value iteration takes long to converge \n",
    "\n",
    "When we ran value iteration on the small frozen lake problem, the last iteration where an action changed was iteration 6--i.e., value iteration computed the optimal policy at iteration 6. Are there any guarantees regarding how many iterations it'll take value iteration to compute the optimal policy? There are no such guarantees without additional assumptions--we can construct the MDP in such a way that the greedy policy will change after arbitrarily many iterations.\n",
    "\n",
    "Your task: define an MDP with at most 3 states and 2 actions, such that when you run value iteration, the optimal action changes at iteration >= 50. Use discount=0.95. (However, note that the discount doesn't matter here--you can construct an appropriate MDP with any discount.)\n",
    "\n",
    "Note: value function must change at least once after iteration >=50, not necessarily change on every iteration till >=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.122424Z",
     "start_time": "2018-04-02T13:43:05.582Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "transition_probs = {}\n",
    "\n",
    "# YOUR CODE HERE\n",
    "rewards = {}\n",
    "\n",
    "# Feel free to change the initial_state\n",
    "mdp = MDP(transition_probs, rewards, initial_state=random.choice(tuple(transition_probs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.123825Z",
     "start_time": "2018-04-02T13:43:05.586Z"
    }
   },
   "outputs": [],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "policy = np.array(\n",
    "    [get_optimal_action(mdp, state_values, state, gamma) for state in sorted(mdp.get_all_states())]\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "\n",
    "    new_policy = np.array(\n",
    "        [\n",
    "            get_optimal_action(mdp, state_values, state, gamma)\n",
    "            for state in sorted(mdp.get_all_states())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    n_changes = (policy != new_policy).sum()\n",
    "    print(\"N actions changed = %i \\n\" % n_changes)\n",
    "    policy = new_policy\n",
    "\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2 - Policy Iteration\n",
    "\n",
    "Let's implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// random or fixed action`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Unlike VI, policy iteration has to maintain a policy - chosen actions from all states - and estimate $V^{\\pi_{n}}$ based on this policy. It only changes policy once values converged.\n",
    "\n",
    "\n",
    "Below are a few helpers that you may or may not use in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.125320Z",
     "start_time": "2018-04-02T13:43:05.590Z"
    }
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "# fmt: on\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "\n",
    "Unlike VI, this time you must find the exact solution, not just a single iteration.\n",
    "\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You'll have to solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.126518Z",
     "start_time": "2018-04-02T13:43:05.593Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
    "    :param policy: a dict of currently chosen actions {s : a}\n",
    "    :returns: a dict {state : V^pi(state) for all states}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    state_values = None\n",
    "\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.127349Z",
     "start_time": "2018-04-02T13:43:05.597Z"
    }
   },
   "outputs": [],
   "source": [
    "test_policy = {s: np.random.choice(mdp.get_possible_actions(s)) for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "\n",
    "assert type(new_vpi) is dict, \"compute_vpi must return a dict {state : V^pi(state) for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've got new state values, it's time to update our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.128415Z",
     "start_time": "2018-04-02T13:43:05.601Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Computes new policy as argmax of state values\n",
    "    :param vpi: a dict {state : V^pi(state) for all states}\n",
    "    :returns: a dict {state : optimal action for all states}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    new_policy = None\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.129416Z",
     "start_time": "2018-04-02T13:43:05.604Z"
    }
   },
   "outputs": [],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert (\n",
    "    type(new_policy) is dict\n",
    "), \"compute_new_policy must return a dict {state : optimal action for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main loop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.130183Z",
     "start_time": "2018-04-02T13:43:05.608Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\"\n",
    "    Run the policy iteration loop for num_iter iterations or till difference between V(s) is below min_difference.\n",
    "    If policy is not given, initialize it at random.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    state_values = None\n",
    "    policy = None\n",
    "\n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your PI Results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.130926Z",
     "start_time": "2018-04-02T13:43:05.612Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Compare PI and VI on the MDP from bonus 1, then on small & large FrozenLake"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
