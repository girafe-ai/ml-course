{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwUEmpTvyXZV"
   },
   "source": [
    "# Practice: Q-learning\n",
    "\n",
    "_Reference:_ This notebook is based on Practical RL [week03](https://github.com/yandexdataschool/Practical_RL/tree/master/week03_model_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4_8lIHAyXZY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ[\"DISPLAY\"] = \":1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeYgwaKKZGgd"
   },
   "source": [
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRmBDXR2y37a"
   },
   "source": [
    "This notebook will guide you through implementation of vanilla Q-learning algorithm. This part is based on the [Berkeley's RL course](https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html).\n",
    "\n",
    "You need to implement the main methods of the agent, namely, action sampling and q-values update procedure.\n",
    "\n",
    "In order to simplify the implementation, agent also has separate function to estimate the value function of the state given the q-function estimates. The value-function in this case can be estimated as the maximum of q-function values in current state:\n",
    "\n",
    "$$\n",
    "V(s) = \\max_a Q(s, a)\n",
    "$$\n",
    "\n",
    "Once we can evaluate states, we can try to update our q-values accordingly, i.e. suppose we were in a state $s$ and took action $a$, which changed the state to $s'$ and yielded the reward $r$. In this case we can deduce that our q-function should be equal to:\n",
    "\n",
    "$$\n",
    "Q(s, a) = r + \\gamma V(s')\n",
    "$$\n",
    "\n",
    "Here $\\gamma$ is the discounting factor.\n",
    "\n",
    "However, we don't want to change our q-value so radically on each step, because this would mean that we will be discarding all our previous updates to this value. Instead we will call this new value $Q'(s, a)$ and will update our q-function in a following maner:\n",
    "\n",
    "$$\n",
    "Q_{n+1}(s, a) = (1 - \\alpha) \\cdot Q_n(s, a) + \\alpha \\cdot Q_n'(s, a)\n",
    "$$\n",
    "\n",
    "Action sampling is done using the simple $\\varepsilon$-greedy strategy, which means that we take random action with probability $\\varepsilon$ or the best action (wrt the q-function values) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P95haD6wyXZZ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, gamma, n_actions):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.possible_actions = range(n_actions)\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    # -------------------------START OF YOUR CODE-------------------------#\n",
    "    #                          !!!Important!!!                           #\n",
    "    #              Please avoid using self._qvalues directly.            #\n",
    "    #  There's a special self.get_qvalue/set_qvalue functions for that.  #\n",
    "    # -------------------------START OF YOUR CODE-------------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        # Agent parameters.\n",
    "        possible_actions = self.possible_actions\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Calculate the approximation of value function V(s).\n",
    "        # value = ...\n",
    "\n",
    "        return value\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # Agent parameters.\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Calculate the updated value of Q(s, a).\n",
    "        # qvalue = ...\n",
    "\n",
    "        self.set_qvalue(state, action, qvalue)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        # Agent parameters.\n",
    "        possible_actions = self.possible_actions\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Choose the best action wrt the qvalues.\n",
    "        # best_action = ...\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Agent parameters.\n",
    "        possible_actions = self.possible_actions\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Choose action in an epsilon-greedy maner.\n",
    "        # action = ...\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6lkqpS-yXZb"
   },
   "source": [
    "## Try it on taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chnLpPKAZENJ"
   },
   "source": [
    "Here we use the qlearning agent on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6REbLYeDyXZb"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "n_actions = env.action_space.n\n",
    "agent = QLearningAgent(alpha=0.5, epsilon=0.25, gamma=0.99, n_actions=n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud0VvArnyXZb"
   },
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, time_limit=10 ** 4):\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "    for _ in range(time_limit):\n",
    "        # YOUR CODE HERE\n",
    "        # Pick action given current state, take the selected action and update agent.\n",
    "\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TX5UO6B5HnZY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Helper functions to plot rewards during training.\n",
    "\n",
    "\n",
    "def moving_average(data, span=100):\n",
    "    return pd.Series(data).ewm(span=span).mean().values\n",
    "\n",
    "\n",
    "def plot_rewards(rewards, epsilon):\n",
    "    rewards_ewma = moving_average(rewards)\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(rewards_ewma, label='rewards ewma@100')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title(f\"eps = {epsilon:e}, rewards ewma@100 = {rewards_ewma[-1]:.1f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIXSPSsnyXZc"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    agent.epsilon *= 0.99\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plot_rewards(rewards, agent.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj0kO9NyyXZd"
   },
   "source": [
    "## Binarized state spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZUgRkNegoc5"
   },
   "source": [
    "Now that we know how to use our agent in case of discrete observation space, let's try to use agent to train efficiently on `CartPole-v0`. This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n",
    "\n",
    "The simplest way is to use `round(x, ndigits)` to round real number to a given amount of digits.\n",
    "\n",
    "The tricky part is to get the `ndigits` right for each state to train effectively.\n",
    "\n",
    "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBp_wHDXyXZd"
   },
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return gym.make(\"CartPole-v0\").env  # .env unwraps the TimeLimit wrapper\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"first state:{env.reset()}\")\n",
    "plt.imshow(env.render(\"rgb_array\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU1sIGMSyXZd"
   },
   "source": [
    "### Play a few games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG-wxq7QhEfS"
   },
   "source": [
    "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7tC9VbOEhmI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_cartpole_observation_distribution(states):\n",
    "    states = np.array(states)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 9), sharey=True)\n",
    "    axes = axes.flatten()\n",
    "    for i, title in enumerate(\n",
    "        ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Velocity At Tip']\n",
    "    ):\n",
    "        ax = axes[i]\n",
    "        ax.hist(states[:, i], bins=20)\n",
    "        ax.set_title(title)\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        ax.set_xlim(min(xmin, -xmax), max(-xmin, xmax))\n",
    "        ax.grid()\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiTRSS6byXZe"
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for _ in range(1000):\n",
    "    states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, _, done, _ = env.step(env.action_space.sample())\n",
    "        states.append(state)\n",
    "\n",
    "visualize_cartpole_observation_distribution(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5yL-LJHyXZe"
   },
   "source": [
    "### Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFHn7eOoyXZe"
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "    def observation(self, state):\n",
    "        # YOUR CODE HERE\n",
    "        # Binarize state, e.g. round it to some amount digits (hint: you can do that with round(x, ndigits))\n",
    "        # You might need to pick a different n_digits for each dimension based on histograms.\n",
    "\n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l26s0px8yXZe"
   },
   "outputs": [],
   "source": [
    "env = Binarizer(make_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb9JspcfyXZf"
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for _ in range(1000):\n",
    "    states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, _, done, _ = env.step(env.action_space.sample())\n",
    "        states.append(state)\n",
    "\n",
    "visualize_cartpole_observation_distribution(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnGfei25yXZf"
   },
   "source": [
    "### Learn binarized policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQGhbh2hkjUn"
   },
   "source": [
    "Now let's train a policy that uses binarized state space.\n",
    "\n",
    "__Tips:__ \n",
    "* Note that increasing the number of digits for one dimension of the observations increases your state space by a factor of $10$.\n",
    "* If your binarization is too fine-grained, your agent will take much longer than 10000 steps to converge. You can either increase the number of iterations and reduce epsilon decay or change binarization. In practice we found that this kind of mistake is rather frequent.\n",
    "* If your binarization is too coarse, your agent may fail to find the optimal policy. In practice we found that on this particular environment this kind of mistake is rare.\n",
    "* **Start with a coarse binarization** and make it more fine-grained if that seems necessary.\n",
    "* Having $10^3$â€“$10^4$ distinct states is recommended (len(agent._qvalues)), but not required.\n",
    "* If things don't work without annealing $\\varepsilon$, consider adding that, but make sure that it doesn't go to zero too quickly.\n",
    "\n",
    "A reasonable agent should attain an average reward of at least 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KWSvLWayXZf"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=0.5, epsilon=0.25, gamma=0.99, n_actions=n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrUyIy5dyXZf"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(10000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    # YOUR CODE HERE (optional)\n",
    "    # Adjust epsilon.\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plot_rewards(rewards, agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5KKOknoBn1E"
   },
   "outputs": [],
   "source": [
    "print('Your agent has learned {} Q-values.'.format(len(agent._qvalues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1UDnv6CFtsi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "practice_qlearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
