{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIhUmAAkR24b"
   },
   "source": [
    "# Practice: REINFORCE in PyTorch\n",
    "__This notebook is based on [Practical_RL week06](https://github.com/yandexdataschool/Practical_RL/tree/master/week06_policy_based) materials__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4ugm59uR24d"
   },
   "source": [
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiaF9QLRR24e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ[\"DISPLAY\"] = \":1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EYF5lpLR24e"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PR2YFXrR24f"
   },
   "source": [
    "## Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_tWdvOMR24f"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bqkxAmgR24f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Build a simple neural network that predicts policy logits.\n",
    "# Hint: keep it simple. CartPole isn't worth deep architectures.\n",
    "# model = nn.Sequential(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHzZxBk4R24f"
   },
   "source": [
    "## Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCzve02MS2rS"
   },
   "source": [
    "Let's define function which will take states as input and return probabilities for actions.\n",
    "\n",
    "> Note: input and output of this function are not a torch tensors, they're numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZW2P5oZR24g"
   },
   "outputs": [],
   "source": [
    "def predict_probs(state):\n",
    "    # YOUR CODE HERE\n",
    "    # Predict action probabilities given current state.\n",
    "    # Note: state is a numpy array of shape [state_dim]\n",
    "    # and return value should be a numpy array of shape [n_actions]\n",
    "    # probs = ...\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYFGLVhXR24g"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    test_state = env.reset()\n",
    "    test_probs = predict_probs(test_state)\n",
    "    assert isinstance(\n",
    "        test_probs, np.ndarray\n",
    "    ), f\"you must return np array and not {type(test_probs)}\"\n",
    "    assert tuple(test_probs.shape) == (n_actions,), f\"wrong output shape: {test_probs.shape}\"\n",
    "    assert np.isclose(test_probs.sum(), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN6_Ia3pR24g"
   },
   "source": [
    "## Play the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hcHPzssW7cQ"
   },
   "source": [
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax-53tyaR24g"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, time_limit=1000):\n",
    "    states, actions, rewards = [], [], []\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(time_limit):\n",
    "        action_probs = predict_probs(state)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Sample action with given probabilities.\n",
    "        # action = ...\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # record session history to train later.\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Convert to numpy for faster to torch transformations.\n",
    "    return np.array(states), np.array(actions), np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YH9owxzfR24h"
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7HDoCmCR24h"
   },
   "source": [
    "## Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9VpDiXeXGZE"
   },
   "source": [
    "Now that we have our session's rewards, let's compute cumulative rewards for actions:\n",
    "\n",
    "$$ G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\ldots = r_t + \\gamma G_{t+1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uoiw4y83R24h"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    # YOUR CODE HERE\n",
    "    # Take a list of rewards for the whole session and compute cumulative rewards.\n",
    "    # Note: you must return an array of cumulative rewards with as many elemnts as\n",
    "    # in the initial one.\n",
    "    # Hint: the simplest way would be to iterate from last reward to first and\n",
    "    # compute G_t = r_t + gamma * G_{t+1} recurrently.\n",
    "    # cumulative_rewards = ...\n",
    "\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMJECrKpR24h"
   },
   "outputs": [],
   "source": [
    "assert isinstance(get_cumulative_rewards(rewards), np.ndarray)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0],\n",
    ")\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0],\n",
    ")\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgtYuM9ZR24i"
   },
   "source": [
    "## Loss function and updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX0-DFW_btxO"
   },
   "source": [
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "Which means that we can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu2POiuwR24i"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    states = torch.tensor(states, dtype=torch.float32)  # shape: [batch_size, state_dim]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)  # shape: [batch_size]\n",
    "    # cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    # cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)  # shape: [batch_size]\n",
    "    cumulative_returns = torch.tensor(\n",
    "        get_cumulative_rewards(rewards, gamma), dtype=torch.float32\n",
    "    )  # shape: [batch_size]\n",
    "\n",
    "    # Predict probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    log_probs = torch.log_softmax(logits, dim=1)\n",
    "\n",
    "    # Select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = log_probs[range(states.shape[0]), actions]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Compute loss here.\n",
    "    # Don't forget entropy regularization with `entropy_coef`.\n",
    "    # loss = ...\n",
    "\n",
    "    # Gradient descent step.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return session rewards to print them later.\n",
    "    return rewards.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JM455vnR24i"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Isuz7Wz7R24j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]\n",
    "    mean_reward = np.mean(rewards)\n",
    "    print(f\"mean reward: {mean_reward:.1f}\")\n",
    "    if mean_reward > 500:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mD639TSR24j"
   },
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwNEr5VfR24j"
   },
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDjLfLV3R24j"
   },
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from base64 import b64encode\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XPawOnFR24j"
   },
   "source": [
    "## Bonus area: solving Acrobot-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cyaxV6Lg2q4"
   },
   "source": [
    "Try to solve more complex environment using Policy gradient method.\n",
    "*Hint: you will need add some imporovements to the original REINFORCE (e.g. Advantage Actor Critic or anything else).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olsGdlZNR24j"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_dim = env.reset().shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stsfWcS8R24k"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "binpord_practice_reinforce_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
