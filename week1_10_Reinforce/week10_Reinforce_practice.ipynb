{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week10\n",
    "### REINFORCE in PyTorch\n",
    "__This notebook is based on [Practical_RL week06](https://github.com/yandexdataschool/Practical_RL/tree/master/week06_policy_based) materials__\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import gym.wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in google colab uncomment this\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.system('apt-get install -y xvfb')\n",
    "# os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
    "# os.system('apt-get install -y python-opengl ffmpeg')\n",
    "# os.system('pip install pyglet==1.2.4')\n",
    "\n",
    "# os.system('python -m pip install -U pygame --user')\n",
    "\n",
    "# print('setup complete')\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa831940fa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAATE0lEQVR4nO3dXaxdZ53f8e8vfs3ESXAmxxmPX4gLHtoEzTj0yEUKqlKgxE1fHC6ojFTki0jmwkigjmidGakDF1ZnqgHaC14USjQWZQhGkMRCaYvJQCltGuOEJNhxPPEQkxzs2I49URIYHL/8e3GWlY19js/2ecnxc873I23ttf7rWXv/n8jn55XHa5+dqkKS1I4rprsBSdKlMbglqTEGtyQ1xuCWpMYY3JLUGINbkhozZcGdZF2S/UkOJNkyVe8jSbNNpuI+7iRzgL8G/ikwBPwY+HBVPT3pbyZJs8xUXXGvBQ5U1c+q6nXgPmD9FL2XJM0qc6fodZcBL/TsDwH/aLTB119/fd14441T1IoktefgwYO89NJLGenYVAX3SG/2G2sySTYBmwBWrlzJ7t27p6gVSWrP4ODgqMemaqlkCFjRs78cONQ7oKruqarBqhocGBiYojYkaeaZquD+MbA6yaok84ENwI4pei9JmlWmZKmkqk4n+RjwP4E5wL1VtXcq3kuSZpupWuOmqh4CHpqq15ek2cpPTkpSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JasyEvrosyUHgVeAMcLqqBpNcB3wDuBE4CPzrqvrbibUpSTpnMq64/0lVramqwW5/C/BwVa0GHu72JUmTZCqWStYD27rtbcCdU/AekjRrTTS4C/hukseSbOpqN1TVYYDueckE30OS1GNCa9zArVV1KMkSYGeSZ/o9sQv6TQArV66cYBuSNHtM6Iq7qg51z0eB+4G1wJEkSwG656OjnHtPVQ1W1eDAwMBE2pCkWWXcwZ3kqiRXn9sGPgDsAXYAG7thG4EHJ9qkJOkNE1kquQG4P8m51/nLqvofSX4MbE9yF/A88KGJtylJOmfcwV1VPwP+YIT6ceB9E2lKkjQ6PzkpSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNWbM4E5yb5KjSfb01K5LsjPJs93z4p5jdyc5kGR/ktunqnFJmq36ueL+C2DdebUtwMNVtRp4uNsnyU3ABuDm7pwvJJkzad1KksYO7qr6IXDivPJ6YFu3vQ24s6d+X1WdrKrngAPA2slpVZIE41/jvqGqDgN0z0u6+jLghZ5xQ13tAkk2JdmdZPexY8fG2YYkzT6T/Y+TGaFWIw2sqnuqarCqBgcGBia5DUmaucYb3EeSLAXono929SFgRc+45cCh8bcnSTrfeIN7B7Cx294IPNhT35BkQZJVwGpg18RalCT1mjvWgCRfB24Drk8yBPwJ8KfA9iR3Ac8DHwKoqr1JtgNPA6eBzVV1Zop6l6RZaczgrqoPj3LofaOM3wpsnUhTkqTR+clJSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNGTO4k9yb5GiSPT21TyX5RZInuscdPcfuTnIgyf4kt09V45I0W/Vzxf0XwLoR6p+rqjXd4yGAJDcBG4Cbu3O+kGTOZDUrSeojuKvqh8CJPl9vPXBfVZ2squeAA8DaCfQnSTrPRNa4P5bkqW4pZXFXWwa80DNmqKtdIMmmJLuT7D527NgE2pCk2WW8wf1F4G3AGuAw8JmunhHG1kgvUFX3VNVgVQ0ODAyMsw1Jmn3GFdxVdaSqzlTVWeDLvLEcMgSs6Bm6HDg0sRYlSb3GFdxJlvbsfhA4d8fJDmBDkgVJVgGrgV0Ta1GS1GvuWAOSfB24Dbg+yRDwJ8BtSdYwvAxyEPgoQFXtTbIdeBo4DWyuqjNT0rkkzVJjBndVfXiE8lcuMn4rsHUiTUmSRucnJyWpMQa3JDXG4JakxhjcktQYg1uSGmNwS51fv/wiJ195abrbkMY05u2A0mzw+msnePah/0KumMOCa974FQxXLVnF0n/4L0lG+m0O0vQwuCXg7OnXef2XfwtVnHzljV96NnDTbdPXlDQKl0qkMXi1rcuNwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0ZM7iTrEjy/ST7kuxN8vGufl2SnUme7Z4X95xzd5IDSfYnuX0qJyBJs00/V9yngT+sqn8AvBvYnOQmYAvwcFWtBh7u9umObQBuBtYBX0gyZyqal6TZaMzgrqrDVfV4t/0qsA9YBqwHtnXDtgF3dtvrgfuq6mRVPQccANZOct+SNGtd0hp3khuBW4BHgRuq6jAMhzuwpBu2DHih57Shrnb+a21KsjvJ7mPHjp1/WHpT1dmzUCMc8BdM6TLUd3AnWQR8C/hEVb1ysaEj1C74kaiqe6pqsKoGBwYGRjhFevMceeq7nP/H9LeufyvXLPv709OQdBF9BXeSeQyH9teq6ttd+UiSpd3xpcDRrj4ErOg5fTlwaHLalabG6ZO/uqCWOXO5Yu78aehGurh+7ioJ8BVgX1V9tufQDmBjt70ReLCnviHJgiSrgNXArslrWZJmt36+AedW4CPAT5M80dX+CPhTYHuSu4DngQ8BVNXeJNuBpxm+I2VzVZ2Z7MYlabYaM7ir6keMvG4N8L5RztkKbJ1AX5KkUfjJSUlqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbg16509fYqzp1+/oD534VXT0I00NoNbs96vjj/Pq7/Yd0H9ht//wDR0I43N4JZG+uYbIPHHQ5cn/2RKUmMMbklqjMEtSY0xuCWpMQa3JDWmny8LXpHk+0n2Jdmb5ONd/VNJfpHkie5xR885dyc5kGR/ktuncgKSNNv082XBp4E/rKrHk1wNPJZkZ3fsc1X1572Dk9wEbABuBn4X+F6S3/MLgyVpcox5xV1Vh6vq8W77VWAfsOwip6wH7quqk1X1HHAAWDsZzUqSLnGNO8mNwC3Ao13pY0meSnJvksVdbRnwQs9pQ1w86CVJl6Dv4E6yCPgW8ImqegX4IvA2YA1wGPjMuaEjnH7BZ9OSbEqyO8nuY8eOXWrfkjRr9RXcSeYxHNpfq6pvA1TVkao6U1VngS/zxnLIELCi5/TlwKHzX7Oq7qmqwaoaHBgYmMgcJGlW6eeukgBfAfZV1Wd76kt7hn0Q2NNt7wA2JFmQZBWwGtg1eS1L0uzWz10ltwIfAX6a5Imu9kfAh5OsYXgZ5CDwUYCq2ptkO/A0w3ekbPaOEkmaPGMGd1X9iJHXrR+6yDlbga0T6Et60/z65cMX1OZeeQ1zFvzWNHQjjc1PTmrWe2n//72gdtWSVVy5eOkIo6XpZ3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTH9/FpXqSn79+9ny5YtfY/f+O63sGLx/N+o7dq1i09+aecoZ7xh4cKFfOlLX+Laa6+95D6l8TK4NeOcOHGCBx54oO/xd7z9TpZc89Zur5h/xa85fPgwDzzw3THPXbRoESdPnhxfo9I4Gdya1VbecC2/nH8rf3XsPQCEs9x8zY/Yte//THNn0uhc49asdsP1yzl15bs4U/M4U/M4XQt46uX38P09r053a9KoDG7Naq+c/m1+deY316fP1FzO1khf+iRdHvr5suCFSXYleTLJ3iSf7urXJdmZ5NnueXHPOXcnOZBkf5Lbp3IC0kQsnvciV889/hu1BVf8HXPi16Tq8tXPFfdJ4L1V9QfAGmBdkncDW4CHq2o18HC3T5KbgA3AzcA64AtJ5kxB79KEnT71GnNe+1+89NJB6vUjXD33OO9a/D2unONSiS5f/XxZcAGvdbvzukcB64Hbuvo24AfAv+/q91XVSeC5JAeAtcAjo73HqVOnePHFF8c3A+k8J06c6Hvs7v2HePzP/iNFWPP23+Gtv3Mt/7uK46/8XV/nVxVHjx7l7Nmz421XGtGpU6dGPdbXXSXdFfNjwNuBz1fVo0luqKrDAFV1OMmSbvgy4P/1nD7U1UZ1/PhxvvrVr/bTijSmn//855c0/mwVUPzk2UP85NlDl3TuqVOn+OY3v8miRYsu6TxpLMePHx/1WF/BXVVngDVJ3gLcn+SdFxk+0r/q1AWDkk3AJoCVK1fyyU9+sp9WpDE98sgjfP7zn39T3mv+/Pls3ryZJUuWjD1YugTf+MY3Rj12SXeVVNXLDC+JrAOOJFkK0D0f7YYNASt6TlsOXHAZU1X3VNVgVQ0ODAxcShuSNKv1c1fJQHelTZIrgfcDzwA7gI3dsI3Ag932DmBDkgVJVgGrgV2T3LckzVr9LJUsBbZ169xXANur6jtJHgG2J7kLeB74EEBV7U2yHXgaOA1s7pZaJEmToJ+7Sp4Cbhmhfhx43yjnbAW2Trg7SdIF/OSkJDXG4JakxvjbATXjXHfdddx5551vynstXLiQBQsWvCnvJZ1jcGvGecc73sH9998/3W1IU8alEklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUmH6+LHhhkl1JnkyyN8mnu/qnkvwiyRPd446ec+5OciDJ/iS3T+UEJGm26ef3cZ8E3ltVryWZB/woyX/vjn2uqv68d3CSm4ANwM3A7wLfS/J7fmGwJE2OMa+4a9hr3e687lEXOWU9cF9Vnayq54ADwNoJdypJAvpc404yJ8kTwFFgZ1U92h36WJKnktybZHFXWwa80HP6UFeTJE2CvoK7qs5U1RpgObA2yTuBLwJvA9YAh4HPdMMz0kucX0iyKcnuJLuPHTs2jtYlaXa6pLtKqupl4AfAuqo60gX6WeDLvLEcMgSs6DltOXBohNe6p6oGq2pwYGBgPL1L0qzUz10lA0ne0m1fCbwfeCbJ0p5hHwT2dNs7gA1JFiRZBawGdk1q15I0i/VzV8lSYFuSOQwH/faq+k6SryZZw/AyyEHgowBVtTfJduBp4DSw2TtKJGnyjBncVfUUcMsI9Y9c5JytwNaJtSZJGomfnJSkxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY1JVU13DyQ5BvwSeGm6e5kC1+O8WjNT5+a82vLWqhoY6cBlEdwASXZX1eB09zHZnFd7ZurcnNfM4VKJJDXG4JakxlxOwX3PdDcwRZxXe2bq3JzXDHHZrHFLkvpzOV1xS5L6MO3BnWRdkv1JDiTZMt39XKok9yY5mmRPT+26JDuTPNs9L+45dnc31/1Jbp+erseWZEWS7yfZl2Rvko939abnlmRhkl1Jnuzm9emu3vS8zkkyJ8lPknyn258p8zqY5KdJnkiyu6vNiLmNS1VN2wOYA/wN8PeA+cCTwE3T2dM45vCPgXcBe3pq/wnY0m1vAf6s276pm+MCYFU39znTPYdR5rUUeFe3fTXw113/Tc8NCLCo254HPAq8u/V59czv3wJ/CXxnpvxZ7Po9CFx/Xm1GzG08j+m+4l4LHKiqn1XV68B9wPpp7umSVNUPgRPnldcD27rtbcCdPfX7qupkVT0HHGD4v8Flp6oOV9Xj3farwD5gGY3PrYa91u3O6x5F4/MCSLIc+OfAf+0pNz+vi5jJc7uo6Q7uZcALPftDXa11N1TVYRgOQGBJV29yvkluBG5h+Oq0+bl1ywlPAEeBnVU1I+YF/Gfg3wFne2ozYV4w/Jfrd5M8lmRTV5spc7tkc6f5/TNCbSbf5tLcfJMsAr4FfKKqXklGmsLw0BFql+XcquoMsCbJW4D7k7zzIsObmFeSfwEcrarHktzWzykj1C67efW4taoOJVkC7EzyzEXGtja3SzbdV9xDwIqe/eXAoWnqZTIdSbIUoHs+2tWbmm+SeQyH9teq6ttdeUbMDaCqXgZ+AKyj/XndCvyrJAcZXnJ8b5L/RvvzAqCqDnXPR4H7GV76mBFzG4/pDu4fA6uTrEoyH9gA7JjmnibDDmBjt70ReLCnviHJgiSrgNXArmnob0wZvrT+CrCvqj7bc6jpuSUZ6K60SXIl8H7gGRqfV1XdXVXLq+pGhn+O/qqq/g2NzwsgyVVJrj63DXwA2MMMmNu4Tfe/jgJ3MHzHwt8Afzzd/Yyj/68Dh4FTDP9Nfxfw28DDwLPd83U94/+4m+t+4J9Nd/8Xmdd7GP7fy6eAJ7rHHa3PDfh94CfdvPYA/6GrNz2v8+Z4G2/cVdL8vBi+6+zJ7rH3XE7MhLmN9+EnJyWpMdO9VCJJukQGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1Jjfn//TGcTnSMbfEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "example_state = env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits.\n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "\n",
    "sample = env.reset()\n",
    "state_dim = sample.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # YOUR CODE HERE: define a neural network that predicts policy logits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE: convert states, compute logits, use softmax to get probability\n",
    "    probs = None\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(\n",
    "    test_probas\n",
    ")\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0],\n",
    "    env.action_space.n,\n",
    "), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"\n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([state]))[0]  # noqa: F841\n",
    "\n",
    "        # YOUR CODE HERE: sample action with given probabilities\n",
    "        action = None\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma=0.99):  # rewards at each step and discount for reward\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session\n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE: create an array of cumulative rewards\n",
    "    G = None\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0],\n",
    ")\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0],\n",
    ")\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(\n",
    "        isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]\n",
    "    ), \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(  # noqa: F841\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1\n",
    "    )\n",
    "\n",
    "    # YOUR CODE HERE: Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
    "    entropy = None  # noqa: F841\n",
    "    loss = None  # noqa: F841\n",
    "\n",
    "    # YOUR CODE HERE: gradient descent step\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session()) for _ in range(100)]  # generate new sessions\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "video_names = list(filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        \"./videos/\" + video_names[-1]\n",
    "    )\n",
    ")  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus area: solving Acrobot-v1\n",
    "Try to solve more complex environment using Policy gradient method.\n",
    "*Hint: you will need add some imporovements to the original REINFORCE (e.g. Advantage Actor Critic or anything else).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_dim = env.reset().shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(state_dim, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
