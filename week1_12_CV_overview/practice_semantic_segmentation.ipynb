{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1702f0-ee5a-4cf8-a7c1-f107e7881fd6",
   "metadata": {
    "id": "3a1702f0-ee5a-4cf8-a7c1-f107e7881fd6"
   },
   "source": [
    "# Practice: Semantic segmentation\n",
    "_Reference: based on [Albumentations tutorial](https://albumentations.ai/docs/examples/pytorch_semantic_segmentation/)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d3280-4c5e-418b-a3f2-3876029fd803",
   "metadata": {
    "id": "2d3d3280-4c5e-418b-a3f2-3876029fd803"
   },
   "source": [
    "Today we will attempt to solve the semantic segmentation task. We will use the [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The task will be to classify each pixel of an input image either as `pet` or `background`. We will use [U-Net](https://arxiv.org/abs/1505.04597) inspired architecture. We will take pretrained [EfficientNetV2](https://arxiv.org/abs/2104.00298) model as a backbone and train our task specific decoder. We will use the [PyTorch Image Models](https://github.com/rwightman/pytorch-image-models) library (aka [timm](https://pypi.org/project/timm/)) to load pretrained model and [Albumentations](https://albumentations.ai/) library for image augmentations. We will also use [Weights & Biases](https://wandb.ai/site) to log our metrics and losses during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840b25b-0266-4431-a534-707754f19fd4",
   "metadata": {
    "id": "8840b25b-0266-4431-a534-707754f19fd4"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbadd99-243b-4571-8699-da2932ba56fe",
   "metadata": {
    "id": "ccbadd99-243b-4571-8699-da2932ba56fe"
   },
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b967b-7697-4139-8ea7-8d947491b6ab",
   "metadata": {
    "id": "bb5b967b-7697-4139-8ea7-8d947491b6ab"
   },
   "outputs": [],
   "source": [
    "!pip install -U albumentations timm wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a694e83-8b96-4164-8c64-4b9ac7fc7cbc",
   "metadata": {
    "id": "3a694e83-8b96-4164-8c64-4b9ac7fc7cbc"
   },
   "source": [
    "### Login to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d502108-375a-4987-b72b-97ed8bfff107",
   "metadata": {
    "id": "7d502108-375a-4987-b72b-97ed8bfff107"
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8711705-1265-4700-a0b4-76e9a2a26e3c",
   "metadata": {
    "id": "b8711705-1265-4700-a0b4-76e9a2a26e3c"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400fa09-adc1-4f1f-b902-334d0b4474b2",
   "metadata": {
    "id": "4400fa09-adc1-4f1f-b902-334d0b4474b2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb4910-5784-4a80-b61a-f36eabcb7543",
   "metadata": {
    "id": "1bcb4910-5784-4a80-b61a-f36eabcb7543"
   },
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65cafb8-d04d-4a34-b0c1-bb0401e7a9ff",
   "metadata": {
    "id": "e65cafb8-d04d-4a34-b0c1-bb0401e7a9ff"
   },
   "outputs": [],
   "source": [
    "# We will store all data in './data' directory.\n",
    "!mkdir -p data\n",
    "\n",
    "# Images\n",
    "!wget -nc https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz -P data\n",
    "!tar xzf data/images.tar.gz -C data\n",
    "\n",
    "# Masks\n",
    "!wget -nc https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz -P data\n",
    "!tar xzf data/annotations.tar.gz -C data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ac1ec-2c1d-4e48-a24e-69db289559fe",
   "metadata": {
    "id": "0d8ac1ec-2c1d-4e48-a24e-69db289559fe"
   },
   "source": [
    "### Split files from the dataset into the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2a104-a0e0-4519-8363-d08d5a2e0081",
   "metadata": {
    "id": "d5e2a104-a0e0-4519-8363-d08d5a2e0081"
   },
   "source": [
    "Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 6000 images for training, 1374 images for validation, and 10 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7be93-0629-433b-86d0-041178362722",
   "metadata": {
    "id": "8ee7be93-0629-433b-86d0-041178362722"
   },
   "outputs": [],
   "source": [
    "root_directory = \"data\"\n",
    "images_directory = os.path.join(root_directory, \"images\")\n",
    "masks_directory = os.path.join(root_directory, \"annotations\", \"trimaps\")\n",
    "\n",
    "images_filenames = list(sorted(os.listdir(images_directory)))\n",
    "correct_images_filenames = [\n",
    "    filename\n",
    "    for filename in images_filenames\n",
    "    if cv2.imread(os.path.join(images_directory, filename)) is not None\n",
    "]\n",
    "\n",
    "random_state = 42\n",
    "random_generator = np.random.default_rng(random_state)\n",
    "random_generator.shuffle(correct_images_filenames)\n",
    "\n",
    "train_images_filenames = correct_images_filenames[:6000]\n",
    "val_images_filenames = correct_images_filenames[6000:-10]\n",
    "test_images_filenames = images_filenames[-10:]\n",
    "\n",
    "print(\n",
    "    len(train_images_filenames),\n",
    "    len(val_images_filenames),\n",
    "    len(test_images_filenames),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a71ae-ce0c-4639-882e-905c310259be",
   "metadata": {
    "id": "159a71ae-ce0c-4639-882e-905c310259be"
   },
   "source": [
    "### Preprocess masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c57cd-1e9c-4c3a-a34d-e7f22c884e7d",
   "metadata": {
    "id": "d73c57cd-1e9c-4c3a-a34d-e7f22c884e7d"
   },
   "source": [
    "The dataset contains pixel-level trimap segmentation. For each image, there is an associated PNG file with a mask. The size of a mask equals to the size of the related image. Each pixel in a mask image can take one of three values: `1`, `2`, or `3`. `1` means that this pixel of an image belongs to the class `pet`, `2` - to the class `background`, `3` - to the class `border`. Since this example demonstrates a task of binary segmentation (that is assigning one of two classes to each pixel), we will preprocess the mask, so it will contain only two uniques values: `0.0` if a pixel is a background and `1.0` if a pixel is a pet or a border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213eff0-28a5-4006-b4c9-fac441346674",
   "metadata": {
    "id": "2213eff0-28a5-4006-b4c9-fac441346674"
   },
   "outputs": [],
   "source": [
    "def preprocess_mask(mask):\n",
    "    background = mask == 2\n",
    "    return np.where(background, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d7142-ad76-44ca-bedb-8c89a703ba45",
   "metadata": {
    "id": "293d7142-ad76-44ca-bedb-8c89a703ba45"
   },
   "source": [
    "### Visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68ee40-1452-48a4-a438-5266799a438a",
   "metadata": {
    "id": "2a68ee40-1452-48a4-a438-5266799a438a"
   },
   "outputs": [],
   "source": [
    "visualization_filenames = random_generator.choice(\n",
    "    train_images_filenames,\n",
    "    size=9,\n",
    "    replace=False,\n",
    ")\n",
    "_, axes = plt.subplots(nrows=3, ncols=3, figsize=(24, 24))\n",
    "for ax, image_filename in zip(axes.flatten(), visualization_filenames):\n",
    "    image = cv2.imread(os.path.join(images_directory, image_filename))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    mask = cv2.imread(\n",
    "        os.path.join(masks_directory, image_filename.replace(\".jpg\", \".png\")),\n",
    "        cv2.IMREAD_UNCHANGED,\n",
    "    )\n",
    "    mask = preprocess_mask(mask)\n",
    "\n",
    "    ax.imshow(image, interpolation=\"none\")\n",
    "    ax.imshow(mask, cmap=\"gray\", alpha=0.5, interpolation=\"none\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b808e9d-4b58-4c55-820b-524c86c085d4",
   "metadata": {
    "id": "1b808e9d-4b58-4c55-820b-524c86c085d4"
   },
   "source": [
    "### Image sizes for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fb2ee-ee51-4d30-8b59-aff5a1d8b00c",
   "metadata": {
    "id": "fa1fb2ee-ee51-4d30-8b59-aff5a1d8b00c"
   },
   "source": [
    "As you can see, images have different heights, widths and aspect ratios. That fact brings two challenges to a deep learning pipeline:\n",
    "\n",
    "* PyTorch requires all images in a batch to have the same height and width.\n",
    "* If a neural network is not fully convolutional, you have to use the same width and height for all images during training and inference.\n",
    "* Generally, fully convolutional architectures can work with images of any size. However, some architectures, such as U-Net, require that an image's size must be divisible by a downsampling factor of a network (usually 32).\n",
    "\n",
    "There are three common ways to deal with those challenges:\n",
    "\n",
    "1. Resize all images and masks to a fixed size during training. After a model predicts a mask with that fixed size during inference, resize the mask to the original image size. This approach is simple, but it has a few drawbacks:\n",
    "    * The predicted mask is smaller than the image, and the mask may lose some context and important details of the original image.\n",
    "    * This approach may be problematic if images in your dataset have different aspect ratios. For example, suppose you are resizing an image with the size 1024x512 pixels (so an image with an aspect ratio of 2:1) to 256x256 pixels (1:1 aspect ratio). In that case, this transformation will distort the image and may also affect the quality of predictions.\n",
    "1. If you use a fully convolutional neural network, you can train a model with image crops, but use original images for inference. This option usually provides the best tradeoff between quality, speed of training, and hardware requirements.\n",
    "1. Do not alter the sizes of images and use source images both for training and inference. With this approach, you won't lose any information. However, original images could be quite large, so they may require a lot of GPU memory. Also, this approach requires more training time to obtain good results.\n",
    "\n",
    "In this notebook we will use something in between the first two approaches. We will train our model on random resized crops of size 320x320 pixels and inference it on resized images. This is just a duct tape approach for faster validation, you are free to change `A.RandomResizedCrop` to a `A.RandomCrop` and validate on original images to improve the quality of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220b19d-4562-4ffe-b6ad-5dfb65c410b5",
   "metadata": {
    "id": "5220b19d-4562-4ffe-b6ad-5dfb65c410b5"
   },
   "source": [
    "### Define dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af20e5-fb42-4b2e-bc7d-16586f6ca287",
   "metadata": {
    "id": "77af20e5-fb42-4b2e-bc7d-16586f6ca287"
   },
   "source": [
    "Next, we define a PyTorch dataset. If you are new to PyTorch datasets, please refer to [data loading tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "`__init__` will receive an optional `transform` argument. It is a transformation function of the Albumentations augmentation pipeline. Then in `__getitem__`, the Dataset class will use that function to augment an image and a mask and return their augmented versions.\n",
    "\n",
    "> Please note, that we will use Albumentations for transforms as opposed to the default [torchvision transforms](https://pytorch.org/vision/stable/transforms.html) package. We do that, because Albumentations provide a convenient way to transform targets (such as segmentation masks or keypoints) alongside the input image. Albumentations also provide you with more augmentation options for your CV task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c893f07-20b0-494d-9799-93304c8c2d07",
   "metadata": {
    "id": "2c893f07-20b0-494d-9799-93304c8c2d07"
   },
   "outputs": [],
   "source": [
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_filenames,\n",
    "        images_directory,\n",
    "        masks_directory,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_filename = self.images_filenames[index]\n",
    "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(\n",
    "            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")),\n",
    "            cv2.IMREAD_UNCHANGED,\n",
    "        )\n",
    "        mask = preprocess_mask(mask)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b95a8-0213-43c6-bf56-9341e280ea28",
   "metadata": {
    "id": "220b95a8-0213-43c6-bf56-9341e280ea28"
   },
   "source": [
    "Next, we create augmentation pipelines for the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718127d-9291-4935-bdb1-db951298d41e",
   "metadata": {
    "id": "d718127d-9291-4935-bdb1-db951298d41e"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(320, 320),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = OxfordPetDataset(\n",
    "    train_images_filenames, images_directory, masks_directory, transform=train_transform\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(320, 320),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_dataset = OxfordPetDataset(\n",
    "    val_images_filenames,\n",
    "    images_directory,\n",
    "    masks_directory,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a6ad2-115a-434d-95e4-ec823470e79e",
   "metadata": {
    "id": "5e5a6ad2-115a-434d-95e4-ec823470e79e"
   },
   "source": [
    "## Our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8566b-fabd-432a-9d3e-ba5199fae140",
   "metadata": {
    "id": "08b8566b-fabd-432a-9d3e-ba5199fae140"
   },
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2752cf-24c6-4dcc-a971-f1606164a043",
   "metadata": {
    "id": "eb2752cf-24c6-4dcc-a971-f1606164a043"
   },
   "source": [
    "Historically, people used lots of different approaches to semantic segmentation task (even [before the deep learning](https://edward-rees.com/2019/12/12/segmentation.html)). However, these days it seems that most of them have died out and all thats left is just one approach called U-Net. Originally, U-Net was a specific network, proposed by German scientists headed by Olaf Ronneberger in a [2015 paper](https://arxiv.org/abs/1505.04597), which solved semantic segmentation tasks in biological imaging and won 2015 ISBI cell tracking challenge. Their architecture consisted of encoder and decoder which communicated not only on the last layer of encoder and first layer of decoder (via forwarding the hidden state) but also on higher levels via skip-connections. Here is the picture of their network's architecture:\n",
    "\n",
    "![U-Net acrhitecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
    "\n",
    "The exact architecture is not very important, as it did not age well (remember, the original paper was released back in 2015). But the principle of skip-connections from encoder to decoder is very powerful and has been actively used ever since."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e637a-6bca-4694-8602-46db37219525",
   "metadata": {
    "id": "348e637a-6bca-4694-8602-46db37219525"
   },
   "source": [
    "Let's implement our own U-Net. We will take original architecture as a reference and use some tweaks from modern CNN architectures. Two of such tweaks are [batch normalization](https://arxiv.org/abs/1502.03167) and [clever weights initialization](https://arxiv.org/abs/1502.01852). In fact, let's define helper module to use them after every convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc908e-35df-459e-9005-a3cb1168757f",
   "metadata": {
    "id": "e0dc908e-35df-459e-9005-a3cb1168757f"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        output_padding=1,\n",
    "        transpose=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # We will use this module in both encoder and decoder.\n",
    "        # For this reason it needs to support both normal and transposed convolutions.\n",
    "        if transpose:\n",
    "            self.conv = nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                output_padding,\n",
    "                bias=False,\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.norm.weight, 1)\n",
    "        nn.init.constant_(self.norm.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c4cca-516b-4131-80d5-9c1ee7970b84",
   "metadata": {
    "id": "4f1c4cca-516b-4131-80d5-9c1ee7970b84"
   },
   "source": [
    "Now that we got that out of the way, let's implement our U-Net. As I mentioned earlier, U-Net consists of encoder and decoder. Let's implement `UNetEncoder` and `UNetDecoder` classes before we merge them into `UNet`.\n",
    "\n",
    "We will start with encoder part. In its own turn, U-Net's encoder also consists of multiple blocks, each of which includes max pooling and two convolutions. To make our implementation lighter, we will use just two convolutions, one of which will have a stride of 2. Let's implement that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f9435-ae8e-499d-9d20-447d3565e859",
   "metadata": {
    "id": "c58f9435-ae8e-499d-9d20-447d3565e859"
   },
   "outputs": [],
   "source": [
    "class UNetEncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144c4f7-6319-4915-b4dd-7f1d16132a65",
   "metadata": {
    "id": "6144c4f7-6319-4915-b4dd-7f1d16132a65"
   },
   "source": [
    "Now that we have our encoder block implemented, we are ready to define encoder itself. Our encoder's `__init__` will take just one parameter: `hidden_channels`. This is a list of numbers of channels. First number represents the amount of channels in input tensor `x` and for each number in list we want to obtain output tensor with specified amount of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8a7ec-0be4-4356-a38d-396d14ff84a6",
   "metadata": {
    "id": "cac8a7ec-0be4-4356-a38d-396d14ff84a6"
   },
   "outputs": [],
   "source": [
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            UNetEncoderBlock(in_channels, out_channels)\n",
    "            for in_channels, out_channels in zip(\n",
    "                hidden_channels[:-1],\n",
    "                hidden_channels[1:],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Each block subsamples input tensor.\n",
    "        # Skip connection with the biggest size is just input x.\n",
    "        outputs = [x]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a980d-2f83-4540-b482-09e4e9632b33",
   "metadata": {
    "id": "c58a980d-2f83-4540-b482-09e4e9632b33"
   },
   "source": [
    "Now we have finished with encoder and ready to implement the decoder. Just like with encoder, decoder also consists of blocks. In original paper, each block consisted of bilinear upsampling followed by convolution, concatenation with skip connection and 2 additional convolutions. In our implementation we will merge upsampling and first convolution into one transposed convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac7908-11da-444d-a5e1-85c4523ec4b5",
   "metadata": {
    "id": "77ac7908-11da-444d-a5e1-85c4523ec4b5"
   },
   "outputs": [],
   "source": [
    "class UNetDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Decoder block upsamples the input tensor then concatenates it with\n",
    "        # skip connection and runs another convolution.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd512a-e40d-4367-9e83-0afbc8a64bd4",
   "metadata": {
    "id": "e3cd512a-e40d-4367-9e83-0afbc8a64bd4"
   },
   "source": [
    "After we implemented decoder block, we are ready to implement decoder itself. To keep consistency with encoder, our decoder will have the same parameter as encoder: `hidden_channels`, which basically represent the same thing and are obtained from encoder's `hidden_channels` via reversing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33357b-dad1-4e4f-b45d-812111b52bf9",
   "metadata": {
    "id": "3d33357b-dad1-4e4f-b45d-812111b52bf9"
   },
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            UNetDecoderBlock(in_channels, out_channels)\n",
    "            for in_channels, out_channels in zip(\n",
    "                hidden_channels[:-1],\n",
    "                hidden_channels[1:],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        x, *skip_connections = reversed(encoder_outputs)\n",
    "        for block, skip_connection in zip(self.blocks, skip_connections):\n",
    "            x = block(x, skip_connection)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b8006-235f-446c-9476-d13f3a406d45",
   "metadata": {
    "id": "020b8006-235f-446c-9476-d13f3a406d45"
   },
   "source": [
    "Now that we have both encoder and decoder, we are finally ready to implement our U-Net. It will take image with `in_channels` channels (3 channels for RGB in our case) and output tensor with just one channel, which will contain logits for sigmoid. The output of sigmoid will give the probabilities of the `pet` class for every pixel of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28338f0-cf20-4f85-8216-cafb62ab748c",
   "metadata": {
    "id": "c28338f0-cf20-4f85-8216-cafb62ab748c"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=None):\n",
    "        super().__init__()\n",
    "        if hidden_channels is None:\n",
    "            hidden_channels = [64, 128, 256, 512]\n",
    "\n",
    "        # Our model will receive images with in_channels channels, but\n",
    "        # encoder expects input with hidden_channels channels.\n",
    "        # To fix this, we take the original architecture's approach and\n",
    "        # run our input through 2 consequent convolutions.\n",
    "        out_channels = hidden_channels[0]\n",
    "        self.input_convs = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        # Now its time for our encoder and decoder.\n",
    "        self.encoder = UNetEncoder(hidden_channels)\n",
    "        self.decoder = UNetDecoder(hidden_channels[::-1])\n",
    "\n",
    "        # We will predict logits for sigmoid, i.e. 1 number per pixel.\n",
    "        in_channels = hidden_channels[0]\n",
    "        self.to_mask = nn.Conv2d(in_channels, out_channels=1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_convs(x)\n",
    "        encoder_outputs = self.encoder(x)\n",
    "        x = self.decoder(encoder_outputs)\n",
    "        return self.to_mask(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f79eb8-fd0f-455f-a384-a57ee93569cb",
   "metadata": {
    "id": "22f79eb8-fd0f-455f-a384-a57ee93569cb"
   },
   "source": [
    "The last thing to do before training is to define some metric that we can keep track of during training. For these task I chose the IoU metric. In this case we can compute intersection as the number of pixels with class `pet` in both prediction and target masks and union as the number of pixels with class `pet` in at least one of tensors. To correctly average our metric over batches, I implemented the following helper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2ff53-1112-4f3e-9716-7533bf153a3b",
   "metadata": {
    "id": "54a2ff53-1112-4f3e-9716-7533bf153a3b"
   },
   "outputs": [],
   "source": [
    "class IoU:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, prediction, target):\n",
    "        prediction = (prediction >= self.threshold).long()\n",
    "        prediction_pet = prediction == 1\n",
    "        target_pet = target == 1\n",
    "        self.intersection += torch.sum(prediction_pet & target_pet)\n",
    "        self.union += torch.sum(prediction_pet | target_pet)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.intersection / self.union\n",
    "\n",
    "    def reset(self):\n",
    "        self.intersection = 0\n",
    "        self.union = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08c0ce-b204-452b-8c8b-e06dea5abb01",
   "metadata": {
    "id": "ec08c0ce-b204-452b-8c8b-e06dea5abb01"
   },
   "source": [
    "### Training U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce72e6-d30c-4ff1-a558-43fa428bfd21",
   "metadata": {
    "id": "b8ce72e6-d30c-4ff1-a558-43fa428bfd21"
   },
   "source": [
    "Now that we have everything in order, let's go ahead and train our model. We will use `AdamW` optimizer with `OneCycleLR` scheduler and will keep track of our losses and metric values with wandb logging. Here is how our training loop looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22df552-aa7f-4dd2-9447-d8463c99a365",
   "metadata": {
    "id": "a22df552-aa7f-4dd2-9447-d8463c99a365"
   },
   "outputs": [],
   "source": [
    "def train(model, n_epochs, batch_size, learning_rate):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    iou = IoU()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=len(train_dataloader),\n",
    "    )\n",
    "\n",
    "    for epoch in trange(n_epochs, desc=\"epochs\"):\n",
    "        model.train()\n",
    "        for images, target in tqdm(train_dataloader, desc=\"training\", leave=False):\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            prediction = model(images)\n",
    "            loss = criterion(prediction, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            wandb.log({\"train_loss\": loss})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            iou.reset()\n",
    "            val_loss = 0\n",
    "            for images, target in tqdm(val_dataloader, desc=\"validation\", leave=False):\n",
    "                images, target = images.to(device), target.to(device)\n",
    "                prediction = model(images)\n",
    "                val_loss += criterion(prediction, target)\n",
    "                iou.update(prediction, target)\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"iou\": iou.compute(),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675449a-091e-481d-86aa-92515a7b0e95",
   "metadata": {
    "id": "7675449a-091e-481d-86aa-92515a7b0e95"
   },
   "source": [
    "Now we are ready to actually train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef7123-1282-482b-870c-7f048b5784df",
   "metadata": {
    "id": "aeef7123-1282-482b-870c-7f048b5784df"
   },
   "outputs": [],
   "source": [
    "# Create our model.\n",
    "model = UNet().to(device)\n",
    "\n",
    "# With wandb training is done in so-called runs.\n",
    "# Each run is a separate training of a model.\n",
    "# We can start a run using wandb.init(...) function.\n",
    "wandb.init(entity=\"binpord\", project=\"ml-mipt-segmentation-seminar\")\n",
    "\n",
    "# Train our model.\n",
    "train(model, n_epochs=10, batch_size=8, learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4110ee-ce65-4044-bea2-735defc7d613",
   "metadata": {
    "id": "2e4110ee-ce65-4044-bea2-735defc7d613"
   },
   "outputs": [],
   "source": [
    "# Finish wandb run.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9465923-3d98-489e-81d9-b8bfe622250e",
   "metadata": {
    "id": "b9465923-3d98-489e-81d9-b8bfe622250e"
   },
   "source": [
    "Let's visualize our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2953a-955d-4883-9dd2-53efb040aadd",
   "metadata": {
    "id": "3bb2953a-955d-4883-9dd2-53efb040aadd"
   },
   "outputs": [],
   "source": [
    "# Our images are normalized (see val_transform).\n",
    "# In order to visualize them, we need to undo normalization.\n",
    "# This can be done using another normalize with specific parameters.\n",
    "# https://github.com/pytorch/vision/issues/528\n",
    "mean = torch.tensor([0.4915, 0.4823, 0.4468])\n",
    "std = torch.tensor([0.2470, 0.2435, 0.2616])\n",
    "denormalize = A.Normalize(\n",
    "    (-mean / std).tolist(),\n",
    "    (1.0 / std).tolist(),\n",
    "    max_pixel_value=1.0,\n",
    ")\n",
    "\n",
    "visualization_indices = random_generator.choice(\n",
    "    len(val_dataset),\n",
    "    size=9,\n",
    "    replace=False,\n",
    ")\n",
    "images = torch.cat(\n",
    "    [val_dataset[i][0][None, ...] for i in visualization_indices],\n",
    "    dim=0,\n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(images.to(device)).cpu()\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=3, figsize=(24, 24))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image, predicted_mask = images[i], prediction[i]\n",
    "    image = denormalize(image=image.permute(1, 2, 0).numpy())[\"image\"].clip(0, 1)\n",
    "    ax.imshow(image, interpolation=\"none\")\n",
    "    ax.imshow(predicted_mask, cmap=\"gray\", alpha=0.5, interpolation=\"none\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe8d90-5f7d-467e-9220-dabe8ed440e0",
   "metadata": {
    "id": "cabe8d90-5f7d-467e-9220-dabe8ed440e0"
   },
   "source": [
    "### U-Net with pretrained backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4711e-2a04-480d-b2ae-b960e0541bf4",
   "metadata": {
    "id": "bcc4711e-2a04-480d-b2ae-b960e0541bf4"
   },
   "source": [
    "We trained our model and got a decent IoU of almost 78%. Thats great! Can we do better? Sure we can! We used multiple modern tricks to improve our model, but we did not use the most important trick of all - transfer learning. Transfer learning allows us to train more powerful models using less data and less epochs and achieve better results. Let's use that. The default way to obtain pretrained models in PyTorch is the [torchvision](https://pytorch.org/vision/stable/models.html) package. However, torchvision is not updated very frequently and does nothing to simplify U-Net creation. On the other hand, the `timm` package provides a far wider choice of models and many of them can be loaded with `features_only=True` to output intermediate feature maps from model just like we did in our encoder. For this reason we will use `timm`.\n",
    "\n",
    "Let's define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8e2d1-6b35-4d86-84c0-be5e8dbe303c",
   "metadata": {
    "id": "13e8e2d1-6b35-4d86-84c0-be5e8dbe303c"
   },
   "outputs": [],
   "source": [
    "class PretrainedUNet(nn.Module):\n",
    "    def __init__(self, backbone, backbone_channels):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.decoder = UNetDecoder(backbone_channels[::-1])\n",
    "\n",
    "        # For most models, the first output is still half the original size.\n",
    "        # For this reason I use another up-block, but without skip connection.\n",
    "        in_channels = backbone_channels[0]\n",
    "        out_channels = 1\n",
    "        self.to_mask = nn.Sequential(\n",
    "            ConvBlock(in_channels, in_channels // 2, stride=2, transpose=True),\n",
    "            ConvBlock(in_channels // 2, in_channels // 2),\n",
    "            nn.Conv2d(in_channels // 2, out_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # Freeze backbone parameters to train only our decoder.\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_outputs = self.backbone(x)\n",
    "        x = self.decoder(backbone_outputs)\n",
    "        return self.to_mask(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68adf1-0124-4af4-9f15-902e54d97da3",
   "metadata": {
    "id": "3d68adf1-0124-4af4-9f15-902e54d97da3"
   },
   "outputs": [],
   "source": [
    "# Create our model.\n",
    "backbone = timm.create_model(\n",
    "    'efficientnetv2_rw_s',\n",
    "    pretrained=True,\n",
    "    features_only=True,\n",
    ")\n",
    "model = PretrainedUNet(backbone, backbone_channels=[24, 48, 64, 160, 272]).to(device)\n",
    "\n",
    "# With wandb training is done in so-called runs.\n",
    "# Each run is a separate training of a model.\n",
    "# We can start a run using wandb.init(...) function.\n",
    "wandb.init(entity=\"binpord\", project=\"ml-mipt-segmentation-seminar\")\n",
    "\n",
    "# Train our model.\n",
    "train(model, n_epochs=10, batch_size=12, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6764188-5e82-4509-acaa-fcaf05cbd52a",
   "metadata": {
    "id": "c6764188-5e82-4509-acaa-fcaf05cbd52a"
   },
   "outputs": [],
   "source": [
    "# Finish wandb run.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c294d8-ee22-4323-ad96-43391a24bbc0",
   "metadata": {
    "id": "53c294d8-ee22-4323-ad96-43391a24bbc0"
   },
   "source": [
    "It sure beats our vanila implementation! Transfer learning is, probably, the most important thing in moden machine learning and here we can clearly see why.\n",
    "\n",
    "Let's visualize our predictions on the same images we used in our previous visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f461dc4-25d4-4190-b088-61d6af855162",
   "metadata": {
    "id": "2f461dc4-25d4-4190-b088-61d6af855162"
   },
   "outputs": [],
   "source": [
    "images = torch.cat(\n",
    "    [val_dataset[i][0][None, ...] for i in visualization_indices],\n",
    "    dim=0,\n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(images.to(device)).cpu()\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=3, figsize=(24, 24))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image, predicted_mask = images[i], prediction[i]\n",
    "    image = denormalize(image=image.permute(1, 2, 0).numpy())[\"image\"].clip(0, 1)\n",
    "    ax.imshow(image, interpolation=\"none\")\n",
    "    ax.imshow(predicted_mask, cmap=\"gray\", alpha=0.5, interpolation=\"none\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5395ddd-8b37-4e34-824c-b9bfe0d705f4",
   "metadata": {
    "id": "a5395ddd-8b37-4e34-824c-b9bfe0d705f4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "practice_semantic_segmentation_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
